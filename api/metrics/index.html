<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Metrics (pkg) - Semiotic Conjecture</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Metrics (pkg)";
        var mkdocs_page_input_path = "api/metrics.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Semiotic Conjecture
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Semiotic Conjecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../output_schema/">Output schema</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">API</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cli/">CLI</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../config/">Config</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../corpus/">Corpus</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../analysis/">Analysis</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../decodability/">Decodability</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../frontier/">Frontier</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../reporting/">Reporting</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Metrics (pkg)</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics">metrics</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.codeswitch_index">codeswitch_index</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.domain_coverage_score">domain_coverage_score</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.figures_score">figures_score</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.mtld">mtld</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.ner_coverage">ner_coverage</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.ngram_entropy">ngram_entropy</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.pos_entropy">pos_entropy</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.senses_per_lemma">senses_per_lemma</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.sentence_embedding_dispersion">sentence_embedding_dispersion</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.yules_k">yules_k</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.codeswitch">codeswitch</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.codeswitch.codeswitch_index">codeswitch_index</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity">complexity</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.complexity.mtld">mtld</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.complexity.naive_pos_tags">naive_pos_tags</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.complexity.pos_entropy">pos_entropy</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.complexity.pos_tags">pos_tags</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.complexity.senses_per_lemma">senses_per_lemma</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.complexity.tokenize">tokenize</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.complexity.yules_k">yules_k</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.embeddings">embeddings</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.embeddings.sentence_embedding_dispersion">sentence_embedding_dispersion</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.entropy">entropy</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.entropy.entropy_from_counts">entropy_from_counts</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.entropy.ngram_entropy">ngram_entropy</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.entropy.ngrams">ngrams</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.figures">figures</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.figures.figures_score">figures_score</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.intertextuality">intertextuality</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.intertextuality.domain_coverage_score">domain_coverage_score</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#semiconj.metrics.intertextuality.ner_coverage">ner_coverage</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#codeswitch">codeswitch</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.codeswitch">codeswitch</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.codeswitch.codeswitch_index">codeswitch_index</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#complexity">complexity</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity">complexity</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity.mtld">mtld</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity.naive_pos_tags">naive_pos_tags</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity.pos_entropy">pos_entropy</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity.pos_tags">pos_tags</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity.senses_per_lemma">senses_per_lemma</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity.tokenize">tokenize</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.complexity.yules_k">yules_k</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#embeddings">embeddings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.embeddings">embeddings</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.embeddings.sentence_embedding_dispersion">sentence_embedding_dispersion</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#entropy">entropy</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.entropy">entropy</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.entropy.entropy_from_counts">entropy_from_counts</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.entropy.ngram_entropy">ngram_entropy</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.entropy.ngrams">ngrams</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#figures">figures</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.figures">figures</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.figures.figures_score">figures_score</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#intertextuality">intertextuality</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.intertextuality">intertextuality</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.intertextuality.domain_coverage_score">domain_coverage_score</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semiconj.metrics.intertextuality.ner_coverage">ner_coverage</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../surrogates/">Surrogates (pkg)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../semiotic/">Semiotic (pkg)</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Semiotic Conjecture</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">API</li>
      <li class="breadcrumb-item active">Metrics (pkg)</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="metrics-package">Metrics Package<a class="headerlink" href="#metrics-package" title="Permanent link">&para;</a></h1>


<div class="doc doc-object doc-module">



<a id="semiconj.metrics"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.codeswitch_index" class="doc doc-heading">
            <code class="highlight language-python">codeswitch_index(text)</code>

<a href="#semiconj.metrics.codeswitch_index" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Fraction of tokens that appear to be from a non-dominant language.
Heuristic using stopword overlaps across a few languages.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/codeswitch.py</code></summary>
              <pre class="highlight"><code class="language-python">def codeswitch_index(text: str) -&gt; float:
    """Fraction of tokens that appear to be from a non-dominant language.
    Heuristic using stopword overlaps across a few languages.
    """
    tokens = [t.lower() for t in re.findall(r"[\w']+", text)]
    if not tokens:
        return 0.0
    counts = {lang: 0 for lang in STOPWORDS}
    for t in tokens:
        for lang, sw in STOPWORDS.items():
            if t in sw:
                counts[lang] += 1
    dominant = max(counts, key=counts.get)
    dom_words = STOPWORDS[dominant]
    non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant))
    return min(1.0, non_dom / max(1, len(tokens)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.domain_coverage_score" class="doc doc-heading">
            <code class="highlight language-python">domain_coverage_score(text)</code>

<a href="#semiconj.metrics.domain_coverage_score" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Score in [0,1] for how many domain keyword sets are hit.
Multi-domain coverage increases intertextuality.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/intertextuality.py</code></summary>
              <pre class="highlight"><code class="language-python">def domain_coverage_score(text: str) -&gt; float:
    """Score in [0,1] for how many domain keyword sets are hit.
    Multi-domain coverage increases intertextuality.
    """
    words = set(w.lower() for w in re.findall(r"[\w']+", text))
    hits = 0
    for ws in DOMAINS.values():
        if words &amp; ws:
            hits += 1
    return hits / max(1, len(DOMAINS))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.figures_score" class="doc doc-heading">
            <code class="highlight language-python">figures_score(text)</code>

<a href="#semiconj.metrics.figures_score" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Score figurative language intensity in [0,1].</p>
<p>Behavior:
- If a figures Ollama model is configured via runtime config, use it to score the text.
- Otherwise, fall back to the original heuristic (metaphor/irony cue density).
- In all cases, apply <code>cfg.figures_multiplier</code> and clip to [0,1].</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/figures.py</code></summary>
              <pre class="highlight"><code class="language-python">def figures_score(text: str) -&gt; float:
    """Score figurative language intensity in [0,1].

    Behavior:
    - If a figures Ollama model is configured via runtime config, use it to score the text.
    - Otherwise, fall back to the original heuristic (metaphor/irony cue density).
    - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1].
    """
    from logging import getLogger
    logger = getLogger(__name__)
    cfg = get_runtime_config()

    def _heuristic(t: str) -&gt; float:
        if not t.strip():
            logger.debug("figures_score: empty text -&gt; 0.0")
            return 0.0
        L = max(1, len(t.split()))
        m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS)
        i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS)
        raw = (m + i) / L
        return max(0.0, min(1.0, 5.0 * raw))

    def _ollama(t: str) -&gt; float:
        try:
            # Lazy imports to avoid hard dependency when unused
            from ..surrogates.ollama_client import get_shared_client  # type: ignore
            import json
            import re as _re
            if not getattr(cfg, "figures_ollama_model", "").strip():
                raise RuntimeError("No figures_ollama_model configured")
            client = get_shared_client(host=getattr(cfg, "figures_ollama_host", "http://localhost:11434"))
            system = (
                "You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1]."
            )
            rubric = (
                "Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text "
                "on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations."
            )
            prompt = "Text:\n" + t.strip() + "\n\n" + rubric + "\nRespond as JSON: {\"score\": &lt;float between 0 and 1&gt;}"
            raw = client.generate(
                model=getattr(cfg, "figures_ollama_model"),
                prompt=prompt,
                system=system,
                temperature=0.2,
                seed=getattr(cfg, "seed", None),
            )
            score_val = None
            if raw:
                start = raw.find('{')
                end = raw.rfind('}')
                if start != -1 and end != -1 and end &gt; start:
                    try:
                        obj = json.loads(raw[start:end+1])
                        score_val = obj.get("score", None)
                        if score_val is not None:
                            score_val = float(score_val)
                    except Exception:
                        score_val = None
            if score_val is None and raw:
                m = _re.search(r"([01](?:\\.\\d+)?)", raw)
                if m:
                    try:
                        score_val = float(m.group(1))
                    except Exception:
                        score_val = None
            if score_val is None:
                raise ValueError("Could not parse 'score' from Ollama response")
            return max(0.0, min(1.0, float(score_val)))
        except Exception as e:
            logger.warning("figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)", e)
            raise

    # Try Ollama if configured; otherwise heuristic
    base_score: float
    if getattr(cfg, "figures_ollama_model", "").strip():
        try:
            base_score = _ollama(text)
            return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))
        except Exception:
            logger.warning("figures_score: Ollama scoring failed; falling back to heuristic")
            pass  # fall through to heuristic
    base_score = _heuristic(text)
    return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.mtld" class="doc doc-heading">
            <code class="highlight language-python">mtld(tokens, ttr_threshold=0.72, min_segment=10)</code>

<a href="#semiconj.metrics.mtld" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Approximate MTLD (Measure of Textual Lexical Diversity).</p>
<p>Reference: McCarthy, P. M., &amp; Jarvis, S. (2010). MTLD, vocd-D, and HD-D:
A validation study of sophisticated approaches to lexical diversity assessment.</p>
<p>Returns a positive value, roughly stable across lengths; higher implies more diversity.
Note: This function returns a raw MTLD-like score, not normalized to [0,1].</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -&gt; float:
    """Approximate MTLD (Measure of Textual Lexical Diversity).

    Reference: McCarthy, P. M., &amp; Jarvis, S. (2010). MTLD, vocd-D, and HD-D:
    A validation study of sophisticated approaches to lexical diversity assessment.

    Returns a positive value, roughly stable across lengths; higher implies more diversity.
    Note: This function returns a raw MTLD-like score, not normalized to [0,1].
    """
    if not tokens:
        return 0.0
    types = set()
    factor_count = 0
    token_count = 0
    ttr = 1.0
    for tok in tokens:
        token_count += 1
        types.add(tok)
        ttr = len(types) / token_count
        if ttr &lt;= ttr_threshold and token_count &gt;= min_segment:
            factor_count += 1
            types.clear()
            token_count = 0
    if token_count &gt; 0:
        partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold)
        factor_count += partial
    return (len(tokens) / max(1e-9, factor_count))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.ner_coverage" class="doc doc-heading">
            <code class="highlight language-python">ner_coverage(text)</code>

<a href="#semiconj.metrics.ner_coverage" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Named-entity coverage in [0,1].</p>
<p>Behavior:
- If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities.
- Otherwise, fall back to a capitalization-based heuristic proxy.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/intertextuality.py</code></summary>
              <pre class="highlight"><code class="language-python">def ner_coverage(text: str) -&gt; float:
    """Named-entity coverage in [0,1].

    Behavior:
    - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities.
    - Otherwise, fall back to a capitalization-based heuristic proxy.
    """
    try:
        from ..config import get_runtime_config
        cfg = get_runtime_config()
        model = getattr(cfg, "nlp_ollama_model", "").strip()
        if model:
            try:
                from ..surrogates.ollama_client import get_shared_client  # type: ignore
                client = get_shared_client(host=getattr(cfg, "nlp_ollama_host", "http://localhost:11434"))
                res = client.nlp(model=model, text=text)
                ents = res.get("entities", [])
                toks = res.get("tokens", [])
                total = len(toks) if isinstance(toks, list) and toks else None
                if isinstance(ents, list) and ents:
                    if not total:
                        # If tokenizer not provided or empty, estimate using regex length
                        total = len(re.findall(r"[\w']+", text))
                    total = max(1, int(total or 0))
                    cov = len(ents) / total
                    return max(0.0, min(1.0, cov))
            except Exception:
                pass
    except Exception:
        pass
    tokens = re.findall(r"[\w']+", text)
    if not tokens:
        return 0.0
    ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {"i"}]
    return min(1.0, len(ent_like) / max(1, len(tokens)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.ngram_entropy" class="doc doc-heading">
            <code class="highlight language-python">ngram_entropy(tokens, n=1)</code>

<a href="#semiconj.metrics.ngram_entropy" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute normalized entropy over token n-grams.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>tokens</code></b>
                  (<code><span title="typing.List">List</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tokens.</p>
              </div>
            </li>
            <li>
              <b><code>n</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>N-gram size (default 1 for unigrams).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="float">float</span></code>
              –
              <div class="doc-md-description">
                <p>Normalized Shannon entropy of the n-gram distribution in [0,1].</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <pre class="highlight"><code class="language-pycon">&gt;&gt;&gt; round(ngram_entropy(["a", "b", "a", "b"], n=1), 6)
1.0
&gt;&gt;&gt; round(ngram_entropy(["a", "a", "a"], n=1), 6)
0.0</code></pre>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/entropy.py</code></summary>
              <pre class="highlight"><code class="language-python">def ngram_entropy(tokens: List[str], n: int = 1) -&gt; float:
    """Compute normalized entropy over token n-grams.

    Args:
        tokens: List of tokens.
        n: N-gram size (default 1 for unigrams).

    Returns:
        Normalized Shannon entropy of the n-gram distribution in [0,1].

    Examples:
        &gt;&gt;&gt; round(ngram_entropy(["a", "b", "a", "b"], n=1), 6)
        1.0
        &gt;&gt;&gt; round(ngram_entropy(["a", "a", "a"], n=1), 6)
        0.0
    """
    if len(tokens) &lt; n:
        return 0.0
    return entropy_from_counts(Counter(ngrams(tokens, n)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.pos_entropy" class="doc doc-heading">
            <code class="highlight language-python">pos_entropy(tokens)</code>

<a href="#semiconj.metrics.pos_entropy" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Shannon entropy of POS tag distribution normalized to [0,1].</p>
<p>Normalization uses log2(|UPOS|) as the denominator to make scores comparable
across texts, regardless of how many tag types are observed in a short sample.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def pos_entropy(tokens: List[str]) -&gt; float:
    """Shannon entropy of POS tag distribution normalized to [0,1].

    Normalization uses log2(|UPOS|) as the denominator to make scores comparable
    across texts, regardless of how many tag types are observed in a short sample.
    """
    tags = pos_tags(tokens)
    total = len(tags) or 1
    counts = Counter(tags)
    H = 0.0
    for c in counts.values():
        p = c / total
        H -= p * math.log(p + 1e-12, 2)
    max_H = math.log(max(1, len(_UPOS_TAGS)), 2)
    return max(0.0, min(1.0, H / (max_H + 1e-9)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.senses_per_lemma" class="doc doc-heading">
            <code class="highlight language-python">senses_per_lemma(tokens)</code>

<a href="#semiconj.metrics.senses_per_lemma" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Mean WordNet senses per lemma (raw; not normalized).</p>
<p>Strict behavior: requires NLTK WordNet corpus to be installed and available.
Raises ImportError/LookupError if NLTK or its WordNet data is missing.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def senses_per_lemma(tokens: List[str]) -&gt; float:
    """Mean WordNet senses per lemma (raw; not normalized).

    Strict behavior: requires NLTK WordNet corpus to be installed and available.
    Raises ImportError/LookupError if NLTK or its WordNet data is missing.
    """
    try:
        from nltk.corpus import wordnet as wn  # type: ignore
    except Exception as e:
        raise ImportError("NLTK is required for senses_per_lemma. Install nltk and wordnet data.") from e
    lemmas = set(tokens)
    if not lemmas:
        return 0.0
    total = 0
    try:
        for w in lemmas:
            total += len(wn.synsets(w))
    except Exception as e:
        # Typically LookupError when wordnet data is missing
        raise LookupError("NLTK WordNet data not found. Run: python -m nltk.downloader wordnet") from e
    return total / max(1, len(lemmas))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.sentence_embedding_dispersion" class="doc doc-heading">
            <code class="highlight language-python">sentence_embedding_dispersion(sentences)</code>

<a href="#semiconj.metrics.sentence_embedding_dispersion" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute mean pairwise cosine distance among sentence embeddings.</p>
<p>Behavior:
- If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string.
- Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness.
- Returns a value in [0,1]; higher means more dispersion (semantic variety).</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/embeddings.py</code></summary>
              <pre class="highlight"><code class="language-python">def sentence_embedding_dispersion(sentences: List[List[str]]) -&gt; float:
    """Compute mean pairwise cosine distance among sentence embeddings.

    Behavior:
    - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string.
    - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness.
    - Returns a value in [0,1]; higher means more dispersion (semantic variety).
    """
    if not sentences:
        logger.debug("sentence_embedding_dispersion: empty sentences -&gt; 0.0")
        return 0.0

    cfg = get_runtime_config()
    vecs: List[List[float]]

    use_ollama_model = getattr(cfg, "embeddings_ollama_model", "").strip()
    if use_ollama_model:
        try:
            # Any empty token list would complicate dimension handling; fallback in that case
            if any(len(s) == 0 for s in sentences):
                raise ValueError("empty sentence token list")
            # Lazy import to avoid hard dependency when unused
            from ..surrogates.ollama_client import get_shared_client  # type: ignore
            client = get_shared_client(host=getattr(cfg, "embeddings_ollama_host", "http://localhost:11434"))
            vecs = [client.embed(use_ollama_model, " ".join(s)) for s in sentences]
        except Exception as e:
            logger.warning(
                "sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)",
                e,
            )
            vecs = [_avg_vec(s) for s in sentences]
    else:
        vecs = [_avg_vec(s) for s in sentences]

    n = len(vecs)
    if n &lt; 2:
        logger.debug("sentence_embedding_dispersion: &lt;2 sentences -&gt; 0.0")
        return 0.0
    dsum = 0.0
    cnt = 0
    for i in range(n):
        for j in range(i + 1, n):
            sim = _cosine(vecs[i], vecs[j])
            dsum += (1.0 - sim)  # distance
            cnt += 1
    return dsum / max(1, cnt)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.yules_k" class="doc doc-heading">
            <code class="highlight language-python">yules_k(tokens)</code>

<a href="#semiconj.metrics.yules_k" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Yule's K lexical diversity measure.</p>
<p>Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary.
Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1],
keeping a larger-is-better scale and bounding at 1 when K→0.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def yules_k(tokens: List[str]) -&gt; float:
    """Yule's K lexical diversity measure.

    Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary.
    Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1],
    keeping a larger-is-better scale and bounding at 1 when K→0.
    """
    if not tokens:
        return 0.0
    freqs = Counter(tokens)
    N = sum(freqs.values())
    M2 = sum(n * n * v for n, v in Counter(freqs.values()).items())
    K = 1e4 * (M2 - N) / (N * N)
    # invert to larger-better, normalized roughly to [0,1]
    return 1.0 / (1.0 + K)</code></pre>
            </details>
    </div>

</div>


<div class="doc doc-object doc-module">



<h2 id="semiconj.metrics.codeswitch" class="doc doc-heading">
            <code>codeswitch</code>


<a href="#semiconj.metrics.codeswitch" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.codeswitch.codeswitch_index" class="doc doc-heading">
            <code class="highlight language-python">codeswitch_index(text)</code>

<a href="#semiconj.metrics.codeswitch.codeswitch_index" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fraction of tokens that appear to be from a non-dominant language.
Heuristic using stopword overlaps across a few languages.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/codeswitch.py</code></summary>
              <pre class="highlight"><code class="language-python">def codeswitch_index(text: str) -&gt; float:
    """Fraction of tokens that appear to be from a non-dominant language.
    Heuristic using stopword overlaps across a few languages.
    """
    tokens = [t.lower() for t in re.findall(r"[\w']+", text)]
    if not tokens:
        return 0.0
    counts = {lang: 0 for lang in STOPWORDS}
    for t in tokens:
        for lang, sw in STOPWORDS.items():
            if t in sw:
                counts[lang] += 1
    dominant = max(counts, key=counts.get)
    dom_words = STOPWORDS[dominant]
    non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant))
    return min(1.0, non_dom / max(1, len(tokens)))</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="semiconj.metrics.complexity" class="doc doc-heading">
            <code>complexity</code>


<a href="#semiconj.metrics.complexity" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.complexity.mtld" class="doc doc-heading">
            <code class="highlight language-python">mtld(tokens, ttr_threshold=0.72, min_segment=10)</code>

<a href="#semiconj.metrics.complexity.mtld" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Approximate MTLD (Measure of Textual Lexical Diversity).</p>
<p>Reference: McCarthy, P. M., &amp; Jarvis, S. (2010). MTLD, vocd-D, and HD-D:
A validation study of sophisticated approaches to lexical diversity assessment.</p>
<p>Returns a positive value, roughly stable across lengths; higher implies more diversity.
Note: This function returns a raw MTLD-like score, not normalized to [0,1].</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -&gt; float:
    """Approximate MTLD (Measure of Textual Lexical Diversity).

    Reference: McCarthy, P. M., &amp; Jarvis, S. (2010). MTLD, vocd-D, and HD-D:
    A validation study of sophisticated approaches to lexical diversity assessment.

    Returns a positive value, roughly stable across lengths; higher implies more diversity.
    Note: This function returns a raw MTLD-like score, not normalized to [0,1].
    """
    if not tokens:
        return 0.0
    types = set()
    factor_count = 0
    token_count = 0
    ttr = 1.0
    for tok in tokens:
        token_count += 1
        types.add(tok)
        ttr = len(types) / token_count
        if ttr &lt;= ttr_threshold and token_count &gt;= min_segment:
            factor_count += 1
            types.clear()
            token_count = 0
    if token_count &gt; 0:
        partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold)
        factor_count += partial
    return (len(tokens) / max(1e-9, factor_count))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.complexity.naive_pos_tags" class="doc doc-heading">
            <code class="highlight language-python">naive_pos_tags(tokens)</code>

<a href="#semiconj.metrics.complexity.naive_pos_tags" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Very rough POS tags if no tagger available.
This is a heuristic; replace with spaCy/UD tagger if available.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def naive_pos_tags(tokens: List[str]) -&gt; List[str]:
    """Very rough POS tags if no tagger available.
    This is a heuristic; replace with spaCy/UD tagger if available.
    """
    tags = []
    for w in tokens:
        if w.endswith('ly'):
            tags.append('RB')
        elif w.endswith('ing') or w.endswith('ed'):
            tags.append('VB')
        elif w[0:1].isupper():
            tags.append('NNP')
        elif w in {"is", "am", "are", "was", "were", "be", "been", "being", "do", "does", "did", "have", "has", "had"}:
            tags.append('VB')
        elif w in {"the", "a", "an"}:
            tags.append('DT')
        elif w in {"and", "or", "but", "so", "because"}:
            tags.append('CC')
        else:
            tags.append('NN')
    return tags</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.complexity.pos_entropy" class="doc doc-heading">
            <code class="highlight language-python">pos_entropy(tokens)</code>

<a href="#semiconj.metrics.complexity.pos_entropy" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Shannon entropy of POS tag distribution normalized to [0,1].</p>
<p>Normalization uses log2(|UPOS|) as the denominator to make scores comparable
across texts, regardless of how many tag types are observed in a short sample.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def pos_entropy(tokens: List[str]) -&gt; float:
    """Shannon entropy of POS tag distribution normalized to [0,1].

    Normalization uses log2(|UPOS|) as the denominator to make scores comparable
    across texts, regardless of how many tag types are observed in a short sample.
    """
    tags = pos_tags(tokens)
    total = len(tags) or 1
    counts = Counter(tags)
    H = 0.0
    for c in counts.values():
        p = c / total
        H -= p * math.log(p + 1e-12, 2)
    max_H = math.log(max(1, len(_UPOS_TAGS)), 2)
    return max(0.0, min(1.0, H / (max_H + 1e-9)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.complexity.pos_tags" class="doc doc-heading">
            <code class="highlight language-python">pos_tags(tokens)</code>

<a href="#semiconj.metrics.complexity.pos_tags" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return POS tags for the provided tokens.</p>
<p>Behavior:
- If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API.
  The model is prompted to return strict JSON {"pos": [UPOS...]} aligned with the given tokens.
- Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm).
- Otherwise, fall back to the naive heuristic tagger.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def pos_tags(tokens: List[str]) -&gt; List[str]:
    """Return POS tags for the provided tokens.

    Behavior:
    - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API.
      The model is prompted to return strict JSON {"pos": [UPOS...]} aligned with the given tokens.
    - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm).
    - Otherwise, fall back to the naive heuristic tagger.
    """
    from ..config import get_runtime_config
    cfg = get_runtime_config()

    # Try Ollama-based POS tagging first if configured
    model = getattr(cfg, "nlp_ollama_model", "").strip()
    if model and tokens:
        try:
            from ..surrogates.ollama_client import OllamaClient  # type: ignore
            import json
            allowed = sorted(list(_UPOS_TAGS))
            client = OllamaClient(host=getattr(cfg, "nlp_ollama_host", "http://localhost:11434"))
            # Build a strict prompt to tag exactly the provided tokens
            system = (
                "You are a POS tagger. Respond in strict JSON only with a single key 'pos' "
                "containing Universal POS tags (UPOS) for each input token in order."
            )
            # Use json.dumps to pass the exact token list to avoid tokenization drift
            tokens_json = json.dumps(tokens)
            rubric = (
                "Tag each token with one of the UPOS tags: " + ", ".join(allowed) + ". "
                + "Return JSON exactly as: {\"pos\": [\"TAG1\", \"TAG2\", \"...\"]} with length equal to the number of input tokens."
            )
            prompt = (
                "Tokens:\n" + tokens_json + "\n\n" + rubric + "\n"
                "Do not include explanations or additional keys."
            )
            raw = client.generate(model=model, prompt=prompt, system=system, temperature=0.0, seed=getattr(cfg, "seed", None))
            start = raw.find('{')
            end = raw.rfind('}')
            if start != -1 and end != -1 and end &gt; start:
                try:
                    obj = json.loads(raw[start:end+1])
                    pos_list = obj.get("pos", [])
                    if isinstance(pos_list, list) and len(pos_list) == len(tokens):
                        tags: List[str] = []
                        for t in pos_list:
                            tag = str(t).upper()
                            tags.append(tag if tag in _UPOS_TAGS else "X")
                        return tags
                except Exception:
                    pass
        except Exception:
            # Fall through to spaCy/naive
            logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to naive POS tagger.')
            pass

    # Fallbacks
    if cfg.pos_tagger == "spacy":
        return _maybe_spacy_pos(tokens)
    return naive_pos_tags(tokens)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.complexity.senses_per_lemma" class="doc doc-heading">
            <code class="highlight language-python">senses_per_lemma(tokens)</code>

<a href="#semiconj.metrics.complexity.senses_per_lemma" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Mean WordNet senses per lemma (raw; not normalized).</p>
<p>Strict behavior: requires NLTK WordNet corpus to be installed and available.
Raises ImportError/LookupError if NLTK or its WordNet data is missing.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def senses_per_lemma(tokens: List[str]) -&gt; float:
    """Mean WordNet senses per lemma (raw; not normalized).

    Strict behavior: requires NLTK WordNet corpus to be installed and available.
    Raises ImportError/LookupError if NLTK or its WordNet data is missing.
    """
    try:
        from nltk.corpus import wordnet as wn  # type: ignore
    except Exception as e:
        raise ImportError("NLTK is required for senses_per_lemma. Install nltk and wordnet data.") from e
    lemmas = set(tokens)
    if not lemmas:
        return 0.0
    total = 0
    try:
        for w in lemmas:
            total += len(wn.synsets(w))
    except Exception as e:
        # Typically LookupError when wordnet data is missing
        raise LookupError("NLTK WordNet data not found. Run: python -m nltk.downloader wordnet") from e
    return total / max(1, len(lemmas))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.complexity.tokenize" class="doc doc-heading">
            <code class="highlight language-python">tokenize(text)</code>

<a href="#semiconj.metrics.complexity.tokenize" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Tokenize text.</p>
<p>Behavior:
- If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens.
- Otherwise, fall back to a simple regex-based tokenizer.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def tokenize(text: str) -&gt; List[str]:
    """Tokenize text.

    Behavior:
    - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens.
    - Otherwise, fall back to a simple regex-based tokenizer.
    """
    try:
        from ..config import get_runtime_config
        cfg = get_runtime_config()
        model = getattr(cfg, "nlp_ollama_model", "").strip()
        if model:
            try:
                from ..surrogates.ollama_client import get_shared_client  # type: ignore
                client = get_shared_client(host=getattr(cfg, "nlp_ollama_host", "http://localhost:11434"))
                res = client.nlp(model=model, text=text)
                toks = res.get("tokens", [])
                if isinstance(toks, list) and toks:
                    # Normalize to lower-case to preserve previous behavior
                    return [str(t).lower() for t in toks if isinstance(t, str)]
            except Exception:
                # fall back to regex below
                logging.getLogger(__name__).warning(
                    f"Ollama NLP model '{model}' not found. Falling back to regex-based tokenization."
                )
                pass
    except Exception:
        # If config import fails for any reason, use regex
        pass
    return _WORD_RE.findall(text.lower())</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.complexity.yules_k" class="doc doc-heading">
            <code class="highlight language-python">yules_k(tokens)</code>

<a href="#semiconj.metrics.complexity.yules_k" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Yule's K lexical diversity measure.</p>
<p>Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary.
Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1],
keeping a larger-is-better scale and bounding at 1 when K→0.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def yules_k(tokens: List[str]) -&gt; float:
    """Yule's K lexical diversity measure.

    Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary.
    Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1],
    keeping a larger-is-better scale and bounding at 1 when K→0.
    """
    if not tokens:
        return 0.0
    freqs = Counter(tokens)
    N = sum(freqs.values())
    M2 = sum(n * n * v for n, v in Counter(freqs.values()).items())
    K = 1e4 * (M2 - N) / (N * N)
    # invert to larger-better, normalized roughly to [0,1]
    return 1.0 / (1.0 + K)</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="semiconj.metrics.embeddings" class="doc doc-heading">
            <code>embeddings</code>


<a href="#semiconj.metrics.embeddings" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.embeddings.sentence_embedding_dispersion" class="doc doc-heading">
            <code class="highlight language-python">sentence_embedding_dispersion(sentences)</code>

<a href="#semiconj.metrics.embeddings.sentence_embedding_dispersion" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Compute mean pairwise cosine distance among sentence embeddings.</p>
<p>Behavior:
- If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string.
- Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness.
- Returns a value in [0,1]; higher means more dispersion (semantic variety).</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/embeddings.py</code></summary>
              <pre class="highlight"><code class="language-python">def sentence_embedding_dispersion(sentences: List[List[str]]) -&gt; float:
    """Compute mean pairwise cosine distance among sentence embeddings.

    Behavior:
    - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string.
    - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness.
    - Returns a value in [0,1]; higher means more dispersion (semantic variety).
    """
    if not sentences:
        logger.debug("sentence_embedding_dispersion: empty sentences -&gt; 0.0")
        return 0.0

    cfg = get_runtime_config()
    vecs: List[List[float]]

    use_ollama_model = getattr(cfg, "embeddings_ollama_model", "").strip()
    if use_ollama_model:
        try:
            # Any empty token list would complicate dimension handling; fallback in that case
            if any(len(s) == 0 for s in sentences):
                raise ValueError("empty sentence token list")
            # Lazy import to avoid hard dependency when unused
            from ..surrogates.ollama_client import get_shared_client  # type: ignore
            client = get_shared_client(host=getattr(cfg, "embeddings_ollama_host", "http://localhost:11434"))
            vecs = [client.embed(use_ollama_model, " ".join(s)) for s in sentences]
        except Exception as e:
            logger.warning(
                "sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)",
                e,
            )
            vecs = [_avg_vec(s) for s in sentences]
    else:
        vecs = [_avg_vec(s) for s in sentences]

    n = len(vecs)
    if n &lt; 2:
        logger.debug("sentence_embedding_dispersion: &lt;2 sentences -&gt; 0.0")
        return 0.0
    dsum = 0.0
    cnt = 0
    for i in range(n):
        for j in range(i + 1, n):
            sim = _cosine(vecs[i], vecs[j])
            dsum += (1.0 - sim)  # distance
            cnt += 1
    return dsum / max(1, cnt)</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="semiconj.metrics.entropy" class="doc doc-heading">
            <code>entropy</code>


<a href="#semiconj.metrics.entropy" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.entropy.entropy_from_counts" class="doc doc-heading">
            <code class="highlight language-python">entropy_from_counts(counts)</code>

<a href="#semiconj.metrics.entropy.entropy_from_counts" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Compute normalized Shannon entropy from a Counter of event counts.</p>
<p>The entropy is computed in bits and normalized by log2(K), where K is the
number of unique events, yielding a value in [0,1].</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>counts</code></b>
                  (<code><span title="collections.Counter">Counter</span></code>)
              –
              <div class="doc-md-description">
                <p>Counter mapping events to their frequencies.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="float">float</span></code>
              –
              <div class="doc-md-description">
                <p>Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <pre class="highlight"><code class="language-pycon">&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6)
1.0
&gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6)
0.0</code></pre>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/entropy.py</code></summary>
              <pre class="highlight"><code class="language-python">def entropy_from_counts(counts: Counter) -&gt; float:
    """Compute normalized Shannon entropy from a Counter of event counts.

    The entropy is computed in bits and normalized by log2(K), where K is the
    number of unique events, yielding a value in [0,1].

    Args:
        counts: Counter mapping events to their frequencies.

    Returns:
        Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions.

    Examples:
        &gt;&gt;&gt; from collections import Counter
        &gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6)
        1.0
        &gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6)
        0.0
    """
    total = sum(counts.values()) or 1
    H = 0.0
    for c in counts.values():
        p = c / total
        H -= p * math.log(p + 1e-12, 2)
    max_H = math.log(max(1, len(counts)), 2)
    if max_H == 0:
        return 0.0
    return H / max_H</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.entropy.ngram_entropy" class="doc doc-heading">
            <code class="highlight language-python">ngram_entropy(tokens, n=1)</code>

<a href="#semiconj.metrics.entropy.ngram_entropy" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Compute normalized entropy over token n-grams.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>tokens</code></b>
                  (<code><span title="typing.List">List</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tokens.</p>
              </div>
            </li>
            <li>
              <b><code>n</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>N-gram size (default 1 for unigrams).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="float">float</span></code>
              –
              <div class="doc-md-description">
                <p>Normalized Shannon entropy of the n-gram distribution in [0,1].</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <pre class="highlight"><code class="language-pycon">&gt;&gt;&gt; round(ngram_entropy(["a", "b", "a", "b"], n=1), 6)
1.0
&gt;&gt;&gt; round(ngram_entropy(["a", "a", "a"], n=1), 6)
0.0</code></pre>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/entropy.py</code></summary>
              <pre class="highlight"><code class="language-python">def ngram_entropy(tokens: List[str], n: int = 1) -&gt; float:
    """Compute normalized entropy over token n-grams.

    Args:
        tokens: List of tokens.
        n: N-gram size (default 1 for unigrams).

    Returns:
        Normalized Shannon entropy of the n-gram distribution in [0,1].

    Examples:
        &gt;&gt;&gt; round(ngram_entropy(["a", "b", "a", "b"], n=1), 6)
        1.0
        &gt;&gt;&gt; round(ngram_entropy(["a", "a", "a"], n=1), 6)
        0.0
    """
    if len(tokens) &lt; n:
        return 0.0
    return entropy_from_counts(Counter(ngrams(tokens, n)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.entropy.ngrams" class="doc doc-heading">
            <code class="highlight language-python">ngrams(tokens, n)</code>

<a href="#semiconj.metrics.entropy.ngrams" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Yield consecutive n-grams from a list of tokens.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>tokens</code></b>
                  (<code><span title="typing.List">List</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>Sequence of token strings.</p>
              </div>
            </li>
            <li>
              <b><code>n</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Size of the n-gram (n &gt;= 1).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Yields:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Iterable">Iterable</span>[<span title="typing.Tuple">Tuple</span>[<span title="str">str</span>, ...]]</code>
              –
              <div class="doc-md-description">
                <p>Tuples of length <code>n</code> representing each n-gram.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <pre class="highlight"><code class="language-pycon">&gt;&gt;&gt; list(ngrams(["a", "b", "c"], 2))
[('a', 'b'), ('b', 'c')]</code></pre>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/entropy.py</code></summary>
              <pre class="highlight"><code class="language-python">def ngrams(tokens: List[str], n: int) -&gt; Iterable[Tuple[str, ...]]:
    """Yield consecutive n-grams from a list of tokens.

    Args:
        tokens: Sequence of token strings.
        n: Size of the n-gram (n &gt;= 1).

    Yields:
        Tuples of length ``n`` representing each n-gram.

    Examples:
        &gt;&gt;&gt; list(ngrams(["a", "b", "c"], 2))
        [('a', 'b'), ('b', 'c')]
    """
    for i in range(len(tokens) - n + 1):
        yield tuple(tokens[i:i+n])</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="semiconj.metrics.figures" class="doc doc-heading">
            <code>figures</code>


<a href="#semiconj.metrics.figures" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.figures.figures_score" class="doc doc-heading">
            <code class="highlight language-python">figures_score(text)</code>

<a href="#semiconj.metrics.figures.figures_score" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Score figurative language intensity in [0,1].</p>
<p>Behavior:
- If a figures Ollama model is configured via runtime config, use it to score the text.
- Otherwise, fall back to the original heuristic (metaphor/irony cue density).
- In all cases, apply <code>cfg.figures_multiplier</code> and clip to [0,1].</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/figures.py</code></summary>
              <pre class="highlight"><code class="language-python">def figures_score(text: str) -&gt; float:
    """Score figurative language intensity in [0,1].

    Behavior:
    - If a figures Ollama model is configured via runtime config, use it to score the text.
    - Otherwise, fall back to the original heuristic (metaphor/irony cue density).
    - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1].
    """
    from logging import getLogger
    logger = getLogger(__name__)
    cfg = get_runtime_config()

    def _heuristic(t: str) -&gt; float:
        if not t.strip():
            logger.debug("figures_score: empty text -&gt; 0.0")
            return 0.0
        L = max(1, len(t.split()))
        m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS)
        i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS)
        raw = (m + i) / L
        return max(0.0, min(1.0, 5.0 * raw))

    def _ollama(t: str) -&gt; float:
        try:
            # Lazy imports to avoid hard dependency when unused
            from ..surrogates.ollama_client import get_shared_client  # type: ignore
            import json
            import re as _re
            if not getattr(cfg, "figures_ollama_model", "").strip():
                raise RuntimeError("No figures_ollama_model configured")
            client = get_shared_client(host=getattr(cfg, "figures_ollama_host", "http://localhost:11434"))
            system = (
                "You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1]."
            )
            rubric = (
                "Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text "
                "on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations."
            )
            prompt = "Text:\n" + t.strip() + "\n\n" + rubric + "\nRespond as JSON: {\"score\": &lt;float between 0 and 1&gt;}"
            raw = client.generate(
                model=getattr(cfg, "figures_ollama_model"),
                prompt=prompt,
                system=system,
                temperature=0.2,
                seed=getattr(cfg, "seed", None),
            )
            score_val = None
            if raw:
                start = raw.find('{')
                end = raw.rfind('}')
                if start != -1 and end != -1 and end &gt; start:
                    try:
                        obj = json.loads(raw[start:end+1])
                        score_val = obj.get("score", None)
                        if score_val is not None:
                            score_val = float(score_val)
                    except Exception:
                        score_val = None
            if score_val is None and raw:
                m = _re.search(r"([01](?:\\.\\d+)?)", raw)
                if m:
                    try:
                        score_val = float(m.group(1))
                    except Exception:
                        score_val = None
            if score_val is None:
                raise ValueError("Could not parse 'score' from Ollama response")
            return max(0.0, min(1.0, float(score_val)))
        except Exception as e:
            logger.warning("figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)", e)
            raise

    # Try Ollama if configured; otherwise heuristic
    base_score: float
    if getattr(cfg, "figures_ollama_model", "").strip():
        try:
            base_score = _ollama(text)
            return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))
        except Exception:
            logger.warning("figures_score: Ollama scoring failed; falling back to heuristic")
            pass  # fall through to heuristic
    base_score = _heuristic(text)
    return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="semiconj.metrics.intertextuality" class="doc doc-heading">
            <code>intertextuality</code>


<a href="#semiconj.metrics.intertextuality" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.intertextuality.domain_coverage_score" class="doc doc-heading">
            <code class="highlight language-python">domain_coverage_score(text)</code>

<a href="#semiconj.metrics.intertextuality.domain_coverage_score" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Score in [0,1] for how many domain keyword sets are hit.
Multi-domain coverage increases intertextuality.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/intertextuality.py</code></summary>
              <pre class="highlight"><code class="language-python">def domain_coverage_score(text: str) -&gt; float:
    """Score in [0,1] for how many domain keyword sets are hit.
    Multi-domain coverage increases intertextuality.
    """
    words = set(w.lower() for w in re.findall(r"[\w']+", text))
    hits = 0
    for ws in DOMAINS.values():
        if words &amp; ws:
            hits += 1
    return hits / max(1, len(DOMAINS))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="semiconj.metrics.intertextuality.ner_coverage" class="doc doc-heading">
            <code class="highlight language-python">ner_coverage(text)</code>

<a href="#semiconj.metrics.intertextuality.ner_coverage" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Named-entity coverage in [0,1].</p>
<p>Behavior:
- If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities.
- Otherwise, fall back to a capitalization-based heuristic proxy.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/intertextuality.py</code></summary>
              <pre class="highlight"><code class="language-python">def ner_coverage(text: str) -&gt; float:
    """Named-entity coverage in [0,1].

    Behavior:
    - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities.
    - Otherwise, fall back to a capitalization-based heuristic proxy.
    """
    try:
        from ..config import get_runtime_config
        cfg = get_runtime_config()
        model = getattr(cfg, "nlp_ollama_model", "").strip()
        if model:
            try:
                from ..surrogates.ollama_client import get_shared_client  # type: ignore
                client = get_shared_client(host=getattr(cfg, "nlp_ollama_host", "http://localhost:11434"))
                res = client.nlp(model=model, text=text)
                ents = res.get("entities", [])
                toks = res.get("tokens", [])
                total = len(toks) if isinstance(toks, list) and toks else None
                if isinstance(ents, list) and ents:
                    if not total:
                        # If tokenizer not provided or empty, estimate using regex length
                        total = len(re.findall(r"[\w']+", text))
                    total = max(1, int(total or 0))
                    cov = len(ents) / total
                    return max(0.0, min(1.0, cov))
            except Exception:
                pass
    except Exception:
        pass
    tokens = re.findall(r"[\w']+", text)
    if not tokens:
        return 0.0
    ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {"i"}]
    return min(1.0, len(ent_like) / max(1, len(tokens)))</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


  </div>

    </div>

</div><h2 id="submodules">Submodules<a class="headerlink" href="#submodules" title="Permanent link">&para;</a></h2>
<h3 id="codeswitch">codeswitch<a class="headerlink" href="#codeswitch" title="Permanent link">&para;</a></h3>


<div class="doc doc-object doc-module">



<a id="semiconj.metrics.codeswitch"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.codeswitch.codeswitch_index" class="doc doc-heading">
            <code class="highlight language-python">codeswitch_index(text)</code>

<a href="#semiconj.metrics.codeswitch.codeswitch_index" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Fraction of tokens that appear to be from a non-dominant language.
Heuristic using stopword overlaps across a few languages.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/codeswitch.py</code></summary>
              <pre class="highlight"><code class="language-python">def codeswitch_index(text: str) -&gt; float:
    """Fraction of tokens that appear to be from a non-dominant language.
    Heuristic using stopword overlaps across a few languages.
    """
    tokens = [t.lower() for t in re.findall(r"[\w']+", text)]
    if not tokens:
        return 0.0
    counts = {lang: 0 for lang in STOPWORDS}
    for t in tokens:
        for lang, sw in STOPWORDS.items():
            if t in sw:
                counts[lang] += 1
    dominant = max(counts, key=counts.get)
    dom_words = STOPWORDS[dominant]
    non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant))
    return min(1.0, non_dom / max(1, len(tokens)))</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="complexity">complexity<a class="headerlink" href="#complexity" title="Permanent link">&para;</a></h3>


<div class="doc doc-object doc-module">



<a id="semiconj.metrics.complexity"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.complexity.mtld" class="doc doc-heading">
            <code class="highlight language-python">mtld(tokens, ttr_threshold=0.72, min_segment=10)</code>

<a href="#semiconj.metrics.complexity.mtld" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Approximate MTLD (Measure of Textual Lexical Diversity).</p>
<p>Reference: McCarthy, P. M., &amp; Jarvis, S. (2010). MTLD, vocd-D, and HD-D:
A validation study of sophisticated approaches to lexical diversity assessment.</p>
<p>Returns a positive value, roughly stable across lengths; higher implies more diversity.
Note: This function returns a raw MTLD-like score, not normalized to [0,1].</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -&gt; float:
    """Approximate MTLD (Measure of Textual Lexical Diversity).

    Reference: McCarthy, P. M., &amp; Jarvis, S. (2010). MTLD, vocd-D, and HD-D:
    A validation study of sophisticated approaches to lexical diversity assessment.

    Returns a positive value, roughly stable across lengths; higher implies more diversity.
    Note: This function returns a raw MTLD-like score, not normalized to [0,1].
    """
    if not tokens:
        return 0.0
    types = set()
    factor_count = 0
    token_count = 0
    ttr = 1.0
    for tok in tokens:
        token_count += 1
        types.add(tok)
        ttr = len(types) / token_count
        if ttr &lt;= ttr_threshold and token_count &gt;= min_segment:
            factor_count += 1
            types.clear()
            token_count = 0
    if token_count &gt; 0:
        partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold)
        factor_count += partial
    return (len(tokens) / max(1e-9, factor_count))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.complexity.naive_pos_tags" class="doc doc-heading">
            <code class="highlight language-python">naive_pos_tags(tokens)</code>

<a href="#semiconj.metrics.complexity.naive_pos_tags" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Very rough POS tags if no tagger available.
This is a heuristic; replace with spaCy/UD tagger if available.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def naive_pos_tags(tokens: List[str]) -&gt; List[str]:
    """Very rough POS tags if no tagger available.
    This is a heuristic; replace with spaCy/UD tagger if available.
    """
    tags = []
    for w in tokens:
        if w.endswith('ly'):
            tags.append('RB')
        elif w.endswith('ing') or w.endswith('ed'):
            tags.append('VB')
        elif w[0:1].isupper():
            tags.append('NNP')
        elif w in {"is", "am", "are", "was", "were", "be", "been", "being", "do", "does", "did", "have", "has", "had"}:
            tags.append('VB')
        elif w in {"the", "a", "an"}:
            tags.append('DT')
        elif w in {"and", "or", "but", "so", "because"}:
            tags.append('CC')
        else:
            tags.append('NN')
    return tags</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.complexity.pos_entropy" class="doc doc-heading">
            <code class="highlight language-python">pos_entropy(tokens)</code>

<a href="#semiconj.metrics.complexity.pos_entropy" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Shannon entropy of POS tag distribution normalized to [0,1].</p>
<p>Normalization uses log2(|UPOS|) as the denominator to make scores comparable
across texts, regardless of how many tag types are observed in a short sample.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def pos_entropy(tokens: List[str]) -&gt; float:
    """Shannon entropy of POS tag distribution normalized to [0,1].

    Normalization uses log2(|UPOS|) as the denominator to make scores comparable
    across texts, regardless of how many tag types are observed in a short sample.
    """
    tags = pos_tags(tokens)
    total = len(tags) or 1
    counts = Counter(tags)
    H = 0.0
    for c in counts.values():
        p = c / total
        H -= p * math.log(p + 1e-12, 2)
    max_H = math.log(max(1, len(_UPOS_TAGS)), 2)
    return max(0.0, min(1.0, H / (max_H + 1e-9)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.complexity.pos_tags" class="doc doc-heading">
            <code class="highlight language-python">pos_tags(tokens)</code>

<a href="#semiconj.metrics.complexity.pos_tags" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Return POS tags for the provided tokens.</p>
<p>Behavior:
- If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API.
  The model is prompted to return strict JSON {"pos": [UPOS...]} aligned with the given tokens.
- Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm).
- Otherwise, fall back to the naive heuristic tagger.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def pos_tags(tokens: List[str]) -&gt; List[str]:
    """Return POS tags for the provided tokens.

    Behavior:
    - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API.
      The model is prompted to return strict JSON {"pos": [UPOS...]} aligned with the given tokens.
    - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm).
    - Otherwise, fall back to the naive heuristic tagger.
    """
    from ..config import get_runtime_config
    cfg = get_runtime_config()

    # Try Ollama-based POS tagging first if configured
    model = getattr(cfg, "nlp_ollama_model", "").strip()
    if model and tokens:
        try:
            from ..surrogates.ollama_client import OllamaClient  # type: ignore
            import json
            allowed = sorted(list(_UPOS_TAGS))
            client = OllamaClient(host=getattr(cfg, "nlp_ollama_host", "http://localhost:11434"))
            # Build a strict prompt to tag exactly the provided tokens
            system = (
                "You are a POS tagger. Respond in strict JSON only with a single key 'pos' "
                "containing Universal POS tags (UPOS) for each input token in order."
            )
            # Use json.dumps to pass the exact token list to avoid tokenization drift
            tokens_json = json.dumps(tokens)
            rubric = (
                "Tag each token with one of the UPOS tags: " + ", ".join(allowed) + ". "
                + "Return JSON exactly as: {\"pos\": [\"TAG1\", \"TAG2\", \"...\"]} with length equal to the number of input tokens."
            )
            prompt = (
                "Tokens:\n" + tokens_json + "\n\n" + rubric + "\n"
                "Do not include explanations or additional keys."
            )
            raw = client.generate(model=model, prompt=prompt, system=system, temperature=0.0, seed=getattr(cfg, "seed", None))
            start = raw.find('{')
            end = raw.rfind('}')
            if start != -1 and end != -1 and end &gt; start:
                try:
                    obj = json.loads(raw[start:end+1])
                    pos_list = obj.get("pos", [])
                    if isinstance(pos_list, list) and len(pos_list) == len(tokens):
                        tags: List[str] = []
                        for t in pos_list:
                            tag = str(t).upper()
                            tags.append(tag if tag in _UPOS_TAGS else "X")
                        return tags
                except Exception:
                    pass
        except Exception:
            # Fall through to spaCy/naive
            logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to naive POS tagger.')
            pass

    # Fallbacks
    if cfg.pos_tagger == "spacy":
        return _maybe_spacy_pos(tokens)
    return naive_pos_tags(tokens)</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.complexity.senses_per_lemma" class="doc doc-heading">
            <code class="highlight language-python">senses_per_lemma(tokens)</code>

<a href="#semiconj.metrics.complexity.senses_per_lemma" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Mean WordNet senses per lemma (raw; not normalized).</p>
<p>Strict behavior: requires NLTK WordNet corpus to be installed and available.
Raises ImportError/LookupError if NLTK or its WordNet data is missing.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def senses_per_lemma(tokens: List[str]) -&gt; float:
    """Mean WordNet senses per lemma (raw; not normalized).

    Strict behavior: requires NLTK WordNet corpus to be installed and available.
    Raises ImportError/LookupError if NLTK or its WordNet data is missing.
    """
    try:
        from nltk.corpus import wordnet as wn  # type: ignore
    except Exception as e:
        raise ImportError("NLTK is required for senses_per_lemma. Install nltk and wordnet data.") from e
    lemmas = set(tokens)
    if not lemmas:
        return 0.0
    total = 0
    try:
        for w in lemmas:
            total += len(wn.synsets(w))
    except Exception as e:
        # Typically LookupError when wordnet data is missing
        raise LookupError("NLTK WordNet data not found. Run: python -m nltk.downloader wordnet") from e
    return total / max(1, len(lemmas))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.complexity.tokenize" class="doc doc-heading">
            <code class="highlight language-python">tokenize(text)</code>

<a href="#semiconj.metrics.complexity.tokenize" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Tokenize text.</p>
<p>Behavior:
- If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens.
- Otherwise, fall back to a simple regex-based tokenizer.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def tokenize(text: str) -&gt; List[str]:
    """Tokenize text.

    Behavior:
    - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens.
    - Otherwise, fall back to a simple regex-based tokenizer.
    """
    try:
        from ..config import get_runtime_config
        cfg = get_runtime_config()
        model = getattr(cfg, "nlp_ollama_model", "").strip()
        if model:
            try:
                from ..surrogates.ollama_client import get_shared_client  # type: ignore
                client = get_shared_client(host=getattr(cfg, "nlp_ollama_host", "http://localhost:11434"))
                res = client.nlp(model=model, text=text)
                toks = res.get("tokens", [])
                if isinstance(toks, list) and toks:
                    # Normalize to lower-case to preserve previous behavior
                    return [str(t).lower() for t in toks if isinstance(t, str)]
            except Exception:
                # fall back to regex below
                logging.getLogger(__name__).warning(
                    f"Ollama NLP model '{model}' not found. Falling back to regex-based tokenization."
                )
                pass
    except Exception:
        # If config import fails for any reason, use regex
        pass
    return _WORD_RE.findall(text.lower())</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.complexity.yules_k" class="doc doc-heading">
            <code class="highlight language-python">yules_k(tokens)</code>

<a href="#semiconj.metrics.complexity.yules_k" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Yule's K lexical diversity measure.</p>
<p>Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary.
Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1],
keeping a larger-is-better scale and bounding at 1 when K→0.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/complexity.py</code></summary>
              <pre class="highlight"><code class="language-python">def yules_k(tokens: List[str]) -&gt; float:
    """Yule's K lexical diversity measure.

    Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary.
    Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1],
    keeping a larger-is-better scale and bounding at 1 when K→0.
    """
    if not tokens:
        return 0.0
    freqs = Counter(tokens)
    N = sum(freqs.values())
    M2 = sum(n * n * v for n, v in Counter(freqs.values()).items())
    K = 1e4 * (M2 - N) / (N * N)
    # invert to larger-better, normalized roughly to [0,1]
    return 1.0 / (1.0 + K)</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="embeddings">embeddings<a class="headerlink" href="#embeddings" title="Permanent link">&para;</a></h3>


<div class="doc doc-object doc-module">



<a id="semiconj.metrics.embeddings"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.embeddings.sentence_embedding_dispersion" class="doc doc-heading">
            <code class="highlight language-python">sentence_embedding_dispersion(sentences)</code>

<a href="#semiconj.metrics.embeddings.sentence_embedding_dispersion" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute mean pairwise cosine distance among sentence embeddings.</p>
<p>Behavior:
- If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string.
- Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness.
- Returns a value in [0,1]; higher means more dispersion (semantic variety).</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/embeddings.py</code></summary>
              <pre class="highlight"><code class="language-python">def sentence_embedding_dispersion(sentences: List[List[str]]) -&gt; float:
    """Compute mean pairwise cosine distance among sentence embeddings.

    Behavior:
    - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string.
    - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness.
    - Returns a value in [0,1]; higher means more dispersion (semantic variety).
    """
    if not sentences:
        logger.debug("sentence_embedding_dispersion: empty sentences -&gt; 0.0")
        return 0.0

    cfg = get_runtime_config()
    vecs: List[List[float]]

    use_ollama_model = getattr(cfg, "embeddings_ollama_model", "").strip()
    if use_ollama_model:
        try:
            # Any empty token list would complicate dimension handling; fallback in that case
            if any(len(s) == 0 for s in sentences):
                raise ValueError("empty sentence token list")
            # Lazy import to avoid hard dependency when unused
            from ..surrogates.ollama_client import get_shared_client  # type: ignore
            client = get_shared_client(host=getattr(cfg, "embeddings_ollama_host", "http://localhost:11434"))
            vecs = [client.embed(use_ollama_model, " ".join(s)) for s in sentences]
        except Exception as e:
            logger.warning(
                "sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)",
                e,
            )
            vecs = [_avg_vec(s) for s in sentences]
    else:
        vecs = [_avg_vec(s) for s in sentences]

    n = len(vecs)
    if n &lt; 2:
        logger.debug("sentence_embedding_dispersion: &lt;2 sentences -&gt; 0.0")
        return 0.0
    dsum = 0.0
    cnt = 0
    for i in range(n):
        for j in range(i + 1, n):
            sim = _cosine(vecs[i], vecs[j])
            dsum += (1.0 - sim)  # distance
            cnt += 1
    return dsum / max(1, cnt)</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="entropy">entropy<a class="headerlink" href="#entropy" title="Permanent link">&para;</a></h3>


<div class="doc doc-object doc-module">



<a id="semiconj.metrics.entropy"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.entropy.entropy_from_counts" class="doc doc-heading">
            <code class="highlight language-python">entropy_from_counts(counts)</code>

<a href="#semiconj.metrics.entropy.entropy_from_counts" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute normalized Shannon entropy from a Counter of event counts.</p>
<p>The entropy is computed in bits and normalized by log2(K), where K is the
number of unique events, yielding a value in [0,1].</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>counts</code></b>
                  (<code><span title="collections.Counter">Counter</span></code>)
              –
              <div class="doc-md-description">
                <p>Counter mapping events to their frequencies.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="float">float</span></code>
              –
              <div class="doc-md-description">
                <p>Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <pre class="highlight"><code class="language-pycon">&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6)
1.0
&gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6)
0.0</code></pre>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/entropy.py</code></summary>
              <pre class="highlight"><code class="language-python">def entropy_from_counts(counts: Counter) -&gt; float:
    """Compute normalized Shannon entropy from a Counter of event counts.

    The entropy is computed in bits and normalized by log2(K), where K is the
    number of unique events, yielding a value in [0,1].

    Args:
        counts: Counter mapping events to their frequencies.

    Returns:
        Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions.

    Examples:
        &gt;&gt;&gt; from collections import Counter
        &gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6)
        1.0
        &gt;&gt;&gt; round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6)
        0.0
    """
    total = sum(counts.values()) or 1
    H = 0.0
    for c in counts.values():
        p = c / total
        H -= p * math.log(p + 1e-12, 2)
    max_H = math.log(max(1, len(counts)), 2)
    if max_H == 0:
        return 0.0
    return H / max_H</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.entropy.ngram_entropy" class="doc doc-heading">
            <code class="highlight language-python">ngram_entropy(tokens, n=1)</code>

<a href="#semiconj.metrics.entropy.ngram_entropy" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute normalized entropy over token n-grams.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>tokens</code></b>
                  (<code><span title="typing.List">List</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tokens.</p>
              </div>
            </li>
            <li>
              <b><code>n</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>N-gram size (default 1 for unigrams).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="float">float</span></code>
              –
              <div class="doc-md-description">
                <p>Normalized Shannon entropy of the n-gram distribution in [0,1].</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <pre class="highlight"><code class="language-pycon">&gt;&gt;&gt; round(ngram_entropy(["a", "b", "a", "b"], n=1), 6)
1.0
&gt;&gt;&gt; round(ngram_entropy(["a", "a", "a"], n=1), 6)
0.0</code></pre>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/entropy.py</code></summary>
              <pre class="highlight"><code class="language-python">def ngram_entropy(tokens: List[str], n: int = 1) -&gt; float:
    """Compute normalized entropy over token n-grams.

    Args:
        tokens: List of tokens.
        n: N-gram size (default 1 for unigrams).

    Returns:
        Normalized Shannon entropy of the n-gram distribution in [0,1].

    Examples:
        &gt;&gt;&gt; round(ngram_entropy(["a", "b", "a", "b"], n=1), 6)
        1.0
        &gt;&gt;&gt; round(ngram_entropy(["a", "a", "a"], n=1), 6)
        0.0
    """
    if len(tokens) &lt; n:
        return 0.0
    return entropy_from_counts(Counter(ngrams(tokens, n)))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.entropy.ngrams" class="doc doc-heading">
            <code class="highlight language-python">ngrams(tokens, n)</code>

<a href="#semiconj.metrics.entropy.ngrams" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Yield consecutive n-grams from a list of tokens.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>tokens</code></b>
                  (<code><span title="typing.List">List</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>Sequence of token strings.</p>
              </div>
            </li>
            <li>
              <b><code>n</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Size of the n-gram (n &gt;= 1).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Yields:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Iterable">Iterable</span>[<span title="typing.Tuple">Tuple</span>[<span title="str">str</span>, ...]]</code>
              –
              <div class="doc-md-description">
                <p>Tuples of length <code>n</code> representing each n-gram.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<p><span class="doc-section-title">Examples:</span></p>
    <pre class="highlight"><code class="language-pycon">&gt;&gt;&gt; list(ngrams(["a", "b", "c"], 2))
[('a', 'b'), ('b', 'c')]</code></pre>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/entropy.py</code></summary>
              <pre class="highlight"><code class="language-python">def ngrams(tokens: List[str], n: int) -&gt; Iterable[Tuple[str, ...]]:
    """Yield consecutive n-grams from a list of tokens.

    Args:
        tokens: Sequence of token strings.
        n: Size of the n-gram (n &gt;= 1).

    Yields:
        Tuples of length ``n`` representing each n-gram.

    Examples:
        &gt;&gt;&gt; list(ngrams(["a", "b", "c"], 2))
        [('a', 'b'), ('b', 'c')]
    """
    for i in range(len(tokens) - n + 1):
        yield tuple(tokens[i:i+n])</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="figures">figures<a class="headerlink" href="#figures" title="Permanent link">&para;</a></h3>


<div class="doc doc-object doc-module">



<a id="semiconj.metrics.figures"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.figures.figures_score" class="doc doc-heading">
            <code class="highlight language-python">figures_score(text)</code>

<a href="#semiconj.metrics.figures.figures_score" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Score figurative language intensity in [0,1].</p>
<p>Behavior:
- If a figures Ollama model is configured via runtime config, use it to score the text.
- Otherwise, fall back to the original heuristic (metaphor/irony cue density).
- In all cases, apply <code>cfg.figures_multiplier</code> and clip to [0,1].</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/figures.py</code></summary>
              <pre class="highlight"><code class="language-python">def figures_score(text: str) -&gt; float:
    """Score figurative language intensity in [0,1].

    Behavior:
    - If a figures Ollama model is configured via runtime config, use it to score the text.
    - Otherwise, fall back to the original heuristic (metaphor/irony cue density).
    - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1].
    """
    from logging import getLogger
    logger = getLogger(__name__)
    cfg = get_runtime_config()

    def _heuristic(t: str) -&gt; float:
        if not t.strip():
            logger.debug("figures_score: empty text -&gt; 0.0")
            return 0.0
        L = max(1, len(t.split()))
        m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS)
        i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS)
        raw = (m + i) / L
        return max(0.0, min(1.0, 5.0 * raw))

    def _ollama(t: str) -&gt; float:
        try:
            # Lazy imports to avoid hard dependency when unused
            from ..surrogates.ollama_client import get_shared_client  # type: ignore
            import json
            import re as _re
            if not getattr(cfg, "figures_ollama_model", "").strip():
                raise RuntimeError("No figures_ollama_model configured")
            client = get_shared_client(host=getattr(cfg, "figures_ollama_host", "http://localhost:11434"))
            system = (
                "You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1]."
            )
            rubric = (
                "Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text "
                "on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations."
            )
            prompt = "Text:\n" + t.strip() + "\n\n" + rubric + "\nRespond as JSON: {\"score\": &lt;float between 0 and 1&gt;}"
            raw = client.generate(
                model=getattr(cfg, "figures_ollama_model"),
                prompt=prompt,
                system=system,
                temperature=0.2,
                seed=getattr(cfg, "seed", None),
            )
            score_val = None
            if raw:
                start = raw.find('{')
                end = raw.rfind('}')
                if start != -1 and end != -1 and end &gt; start:
                    try:
                        obj = json.loads(raw[start:end+1])
                        score_val = obj.get("score", None)
                        if score_val is not None:
                            score_val = float(score_val)
                    except Exception:
                        score_val = None
            if score_val is None and raw:
                m = _re.search(r"([01](?:\\.\\d+)?)", raw)
                if m:
                    try:
                        score_val = float(m.group(1))
                    except Exception:
                        score_val = None
            if score_val is None:
                raise ValueError("Could not parse 'score' from Ollama response")
            return max(0.0, min(1.0, float(score_val)))
        except Exception as e:
            logger.warning("figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)", e)
            raise

    # Try Ollama if configured; otherwise heuristic
    base_score: float
    if getattr(cfg, "figures_ollama_model", "").strip():
        try:
            base_score = _ollama(text)
            return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))
        except Exception:
            logger.warning("figures_score: Ollama scoring failed; falling back to heuristic")
            pass  # fall through to heuristic
    base_score = _heuristic(text)
    return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="intertextuality">intertextuality<a class="headerlink" href="#intertextuality" title="Permanent link">&para;</a></h3>


<div class="doc doc-object doc-module">



<a id="semiconj.metrics.intertextuality"></a>
    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.intertextuality.domain_coverage_score" class="doc doc-heading">
            <code class="highlight language-python">domain_coverage_score(text)</code>

<a href="#semiconj.metrics.intertextuality.domain_coverage_score" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Score in [0,1] for how many domain keyword sets are hit.
Multi-domain coverage increases intertextuality.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/intertextuality.py</code></summary>
              <pre class="highlight"><code class="language-python">def domain_coverage_score(text: str) -&gt; float:
    """Score in [0,1] for how many domain keyword sets are hit.
    Multi-domain coverage increases intertextuality.
    """
    words = set(w.lower() for w in re.findall(r"[\w']+", text))
    hits = 0
    for ws in DOMAINS.values():
        if words &amp; ws:
            hits += 1
    return hits / max(1, len(DOMAINS))</code></pre>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="semiconj.metrics.intertextuality.ner_coverage" class="doc doc-heading">
            <code class="highlight language-python">ner_coverage(text)</code>

<a href="#semiconj.metrics.intertextuality.ner_coverage" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Named-entity coverage in [0,1].</p>
<p>Behavior:
- If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities.
- Otherwise, fall back to a capitalization-based heuristic proxy.</p>


            <details class="quote">
              <summary>Source code in <code>semiconj/metrics/intertextuality.py</code></summary>
              <pre class="highlight"><code class="language-python">def ner_coverage(text: str) -&gt; float:
    """Named-entity coverage in [0,1].

    Behavior:
    - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities.
    - Otherwise, fall back to a capitalization-based heuristic proxy.
    """
    try:
        from ..config import get_runtime_config
        cfg = get_runtime_config()
        model = getattr(cfg, "nlp_ollama_model", "").strip()
        if model:
            try:
                from ..surrogates.ollama_client import get_shared_client  # type: ignore
                client = get_shared_client(host=getattr(cfg, "nlp_ollama_host", "http://localhost:11434"))
                res = client.nlp(model=model, text=text)
                ents = res.get("entities", [])
                toks = res.get("tokens", [])
                total = len(toks) if isinstance(toks, list) and toks else None
                if isinstance(ents, list) and ents:
                    if not total:
                        # If tokenizer not provided or empty, estimate using regex length
                        total = len(re.findall(r"[\w']+", text))
                    total = max(1, int(total or 0))
                    cov = len(ents) / total
                    return max(0.0, min(1.0, cov))
            except Exception:
                pass
    except Exception:
        pass
    tokens = re.findall(r"[\w']+", text)
    if not tokens:
        return 0.0
    ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {"i"}]
    return min(1.0, len(ent_like) / max(1, len(tokens)))</code></pre>
            </details>
    </div>

</div>



  </div>

    </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../reporting/" class="btn btn-neutral float-left" title="Reporting"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../surrogates/" class="btn btn-neutral float-right" title="Surrogates (pkg)">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../reporting/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../surrogates/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
