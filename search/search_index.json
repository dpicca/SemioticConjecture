{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Semiotic Conjecture \u00b6 Introduction \u00b6 A lightweight, reproducible pipeline to measure semiotic amplitude and decodability of model outputs, and to estimate an empirical frontier in the ecosystem. We adopt a Peircean view of signs: each model output M is analyzed into a triad of sign, object, and interpretant. Decodability depends on how consistently a community H of interpreters converges on compatible interpretants under a context C that constrains meaning. As semiotic amplitude S(M) (variety of codes, strata, intertextuality) grows, effective decodability D(M;H,C) tends to shrink, producing an empirical frontier k(H,C) in the S\u2013D plane. Mathematical formulation: $$ X(M;H,C) \\,=\\, S(M)\\, D(M;H,C) $$ $$ D(M;H,C) \\,=\\, \\Omega(H)\\, \\rho(C)\\, D_{\\text{intr}}(M) $$ $$ S(M)\\, D(M;H,C) \\,\\le\\, k(H,C) $$ with S, D_intr, \u03a9, \u03c1 \u2208 [0,1] and k(H,C) estimated non\u2011parametrically. Quickstart \u00b6 Prepare a corpus at data/corpus.csv or use data/sample_corpus.csv . Ensure Ollama is running locally (default http://localhost:11434 ) and pull a few model families (e.g., llama3, mistral, qwen, phi3). Run the pipeline (Ollama surrogates): python -m semiconj.cli run \\ --input data/sample_corpus.csv \\ --out out \\ --ollama-models \"llama3,mistral,qwen\" \\ --K 12 Offline dry run (heuristic surrogates): python -m semiconj.cli run --input data/sample_corpus.csv --out out --surrogates heuristic Notes on Dependencies \u00b6 Core: pip install -r requirements.txt Sci extras: pip install -r requirements-sci.txt Plot extras: pip install -r requirements-plot.txt NLP extras: pip install -r requirements-nlp.txt Dev tools: pip install -r requirements-dev.txt Conda (optional): conda env create -f environment.yml && conda activate semiconj Surrogates: Ollama mode requires a local Ollama server and pulled models (HTTP API; falls back to ollama run CLI). Heuristic mode works fully offline for dry runs. Input and output example \u00b6 Input CSV must include at least these columns: - id: unique identifier - text: raw text excerpt (150\u2013300 words recommended) - Optional metadata columns prefixed with meta_ (e.g., meta_domain , meta_register ) Example input (data/sample_corpus.csv): id,text,meta_domain ex1,\"The algorithm optimizes network flows. It is used in large software systems and represents a core model for routing.\",technology ex2,\"Cells divide and proteins fold. Biology often explores how an organism adapts to an environment. Evolution is a process.\",biology ex3,\"The canvas is a metaphor for memory. A painting captures what the novel cannot, or so the curator claims.\",art Example outputs (written to out/): S_metrics.csv id,semantics,intertextuality,figures,lexicon,pos,codeswitch,S ex1,0.586813,0.135965,0.000000,0.800000,0.332854,0.105263,0.368584 ex2,0.598695,0.162281,0.000000,0.554432,0.291685,0.052632,0.335144 D_effective.csv id,context,D ex1,C_open,0.000000 ex2,C_open,0.000000 ex1,C_medium,0.166667 ex2,C_medium,0.166667 Other files may include: cleaned_input.csv, D_intr.csv, Omega.csv, Rho.csv, Frontier.csv, Analyses.csv (see Output schema for details). Operationalization \u00b6 What is computed and where: - S(M) (semiotic amplitude) \u2014 computed per text by semiconj.cli.compute_S_components and aggregated with semiconj.decodability.aggregate_S . - D_intr(M) (intrinsic decodability) \u2014 semiconj.semiotic.parser.compute_d_intr extracts triads and coherence components. - \u03a9(H), \u03c1(C) \u2014 semiconj.surrogates.community.omega_and_rho ; classes Interpreter and OllamaInterpreter generate labels/summaries. - D(M;H,C) \u2014 semiconj.decodability.effective_decodability combines D_intr with \u03a9 and \u03c1. - Frontier k(H,C) \u2014 semiconj.frontier.estimate_frontier (95th\u2011percentile curve over S bins). - Corpus I/O \u2014 semiconj.corpus.read_corpus , validate_corpus , excerpts_to_rows , write_csv . - CLI orchestration \u2014 semiconj.cli.run_pipeline runs the full pipeline and saves outputs. - Reporting/plots \u2014 semiconj.reporting.save_metrics , maybe_plot_frontier . - Metrics building blocks \u2014 semiconj.metrics.* (complexity, entropy, embeddings, intertextuality, figures, codeswitch). Workflow \u00b6 This section describes the full pipeline, from CLI invocation to outputs, including which functions/classes call which others. At a glance: - 1. CLI entry and configuration - 2. Corpus loading and cleaning - 3. Compute S(M) per text - 4. Compute D_intr(M) per text - 5. Estimate \u03a9(H) and \u03c1(C) - 6. Compute effective decodability D(M;H,C) - 7. Estimate empirical frontier k(H,C) - 8. Ancillary analyses 1. CLI entry and configuration \u00b6 Entry point: semiconj.cli.main parses arguments (argparse) and sets logging. Configuration: semiconj.config.validate_config(DEFAULT_WEIGHTS, CONTEXTS) ensures weights sum to 1 and contexts are valid; then semiconj.config.set_runtime_config(...) applies runtime options (seed, embedding_dim, figures multiplier, pos_tagger). Orchestration: semiconj.cli.main calls semiconj.cli.run_pipeline(input_csv, out_dir, k=K, surrogates=..., ollama_models=..., ollama_host=...) . See also: CLI , Config . 2. Corpus loading and cleaning \u00b6 Read: semiconj.corpus.read_corpus(input_csv) -> List[Excerpt] . Validate: semiconj.corpus.validate_corpus(excerpts, min_words=10) drops missing/dupe/short items. Persist cleaned copy: semiconj.corpus.excerpts_to_rows \u2192 semiconj.corpus.write_csv ( out/cleaned_input.csv ). See also: Corpus . 3. Compute S(M) per text (semiotic amplitude) \u00b6 In run_pipeline , loop over validated excerpts. For each text: semiconj.cli.compute_S_components(text) computes component scores. Components called: semiconj.metrics.embeddings.sentence_embedding_dispersion semiconj.metrics.intertextuality.domain_coverage_score semiconj.metrics.intertextuality.ner_coverage semiconj.metrics.figures.figures_score semiconj.metrics.complexity.mtld , yules_k , senses_per_lemma semiconj.metrics.entropy.ngram_entropy (bigrams) semiconj.metrics.complexity.pos_entropy semiconj.metrics.codeswitch.codeswitch_index Aggregate: semiconj.decodability.aggregate_S(components, semiconj.config.DEFAULT_WEIGHTS) . Save: semiconj.reporting.save_metrics(out/S_metrics.csv, rows) . See also: Metrics , Decodability , Reporting . 4. Compute D_intr(M) per text (intrinsic decodability) \u00b6 For each text: semiconj.semiotic.parser.compute_d_intr(text) , which internally uses: semiconj.semiotic.parser.extract_triads (rule-based triads: sign, object, interpretant) semiconj.semiotic.parser.global_coherence (1 \u2212 dispersion from sentence embeddings) semiconj.semiotic.parser.interpretive_entropy (1 \u2212 mean similarity among interpretants) semiconj.semiotic.parser.interpretant_vectors (uses metrics.embeddings._avg_vec ) Save components and D_intr : semiconj.reporting.save_metrics(out/D_intr.csv, rows) . See also: Semiotic (pkg) . 5. Estimate \u03a9(H) and \u03c1(C) via surrogate communities \u00b6 Contexts: names come from semiconj.config.CONTEXTS (e.g., C_open , C_medium , C_rigid ). Strategy selection in run_pipeline : Heuristic: omega_and_rho(texts, contexts, k=K, strategy=\"heuristic\") Community: semiconj.surrogates.community.build_community -> List[Interpreter] Per (text, context): Interpreter.produce returns (label, summary) Agreement: fleiss_kappa over label assignments; semantic convergence via cosine on summaries Context rigidity \u03c1 : computed from variance reduction of summary vectors across contexts ( variance_of_vectors ) Ollama: omega_and_rho(..., strategy=\"ollama\", models=[...], ollama_host=...) Community: semiconj.surrogates.community.build_ollama_community -> List[OllamaInterpreter] Otherwise identical aggregation to compute \u03a9 and \u03c1 Persist: save_metrics(out/Omega.csv) , save_metrics(out/Rho.csv) . See also: Surrogates (pkg) . 6. Compute effective decodability D(M;H,C) \u00b6 For each context and id: semiconj.decodability.effective_decodability(D_intr[i], omega_by_ctx[c], rho_by_ctx[c]) . Save: semiconj.reporting.save_metrics(out/D_effective.csv, rows) . See also: Decodability , Reporting . 7. Estimate empirical frontier k(H,C) \u00b6 Aggregate D by id (mean across contexts) \u2192 D_mean . Frontier: semiconj.frontier.estimate_frontier(S_values, D_mean) returns (S_bin, k95) pairs. Save CSV: semiconj.reporting.save_metrics(out/Frontier.csv, points) . Optional plot: semiconj.reporting.maybe_plot_frontier(out/frontier.png, points) if matplotlib present. See also: Frontier , Reporting . 8. Ancillary analyses \u00b6 Example: semiconj.analysis.kendall_tau(S_values, D_intr_values) \u2192 correlation summary. Save: semiconj.reporting.save_metrics(out/Analyses.csv, rows) . See also: Analysis . Data flow summary Inputs: data/corpus.csv -> read_corpus -> validate_corpus -> out/cleaned_input.csv -> compute_S_components -> aggregate_S -> out/S_metrics.csv -> compute_d_intr -> out/D_intr.csv -> omega_and_rho -> out/Omega.csv, out/Rho.csv -> effective_decodability -> out/D_effective.csv -> estimate_frontier (+ maybe_plot_frontier) -> out/Frontier.csv (+ frontier.png) -> kendall_tau -> out/Analyses.csv","title":"Semiotic Conjecture"},{"location":"#semiotic-conjecture","text":"","title":"Semiotic Conjecture"},{"location":"#introduction","text":"A lightweight, reproducible pipeline to measure semiotic amplitude and decodability of model outputs, and to estimate an empirical frontier in the ecosystem. We adopt a Peircean view of signs: each model output M is analyzed into a triad of sign, object, and interpretant. Decodability depends on how consistently a community H of interpreters converges on compatible interpretants under a context C that constrains meaning. As semiotic amplitude S(M) (variety of codes, strata, intertextuality) grows, effective decodability D(M;H,C) tends to shrink, producing an empirical frontier k(H,C) in the S\u2013D plane. Mathematical formulation: $$ X(M;H,C) \\,=\\, S(M)\\, D(M;H,C) $$ $$ D(M;H,C) \\,=\\, \\Omega(H)\\, \\rho(C)\\, D_{\\text{intr}}(M) $$ $$ S(M)\\, D(M;H,C) \\,\\le\\, k(H,C) $$ with S, D_intr, \u03a9, \u03c1 \u2208 [0,1] and k(H,C) estimated non\u2011parametrically.","title":"Introduction"},{"location":"#quickstart","text":"Prepare a corpus at data/corpus.csv or use data/sample_corpus.csv . Ensure Ollama is running locally (default http://localhost:11434 ) and pull a few model families (e.g., llama3, mistral, qwen, phi3). Run the pipeline (Ollama surrogates): python -m semiconj.cli run \\ --input data/sample_corpus.csv \\ --out out \\ --ollama-models \"llama3,mistral,qwen\" \\ --K 12 Offline dry run (heuristic surrogates): python -m semiconj.cli run --input data/sample_corpus.csv --out out --surrogates heuristic","title":"Quickstart"},{"location":"#notes-on-dependencies","text":"Core: pip install -r requirements.txt Sci extras: pip install -r requirements-sci.txt Plot extras: pip install -r requirements-plot.txt NLP extras: pip install -r requirements-nlp.txt Dev tools: pip install -r requirements-dev.txt Conda (optional): conda env create -f environment.yml && conda activate semiconj Surrogates: Ollama mode requires a local Ollama server and pulled models (HTTP API; falls back to ollama run CLI). Heuristic mode works fully offline for dry runs.","title":"Notes on Dependencies"},{"location":"#input-and-output-example","text":"Input CSV must include at least these columns: - id: unique identifier - text: raw text excerpt (150\u2013300 words recommended) - Optional metadata columns prefixed with meta_ (e.g., meta_domain , meta_register ) Example input (data/sample_corpus.csv): id,text,meta_domain ex1,\"The algorithm optimizes network flows. It is used in large software systems and represents a core model for routing.\",technology ex2,\"Cells divide and proteins fold. Biology often explores how an organism adapts to an environment. Evolution is a process.\",biology ex3,\"The canvas is a metaphor for memory. A painting captures what the novel cannot, or so the curator claims.\",art Example outputs (written to out/): S_metrics.csv id,semantics,intertextuality,figures,lexicon,pos,codeswitch,S ex1,0.586813,0.135965,0.000000,0.800000,0.332854,0.105263,0.368584 ex2,0.598695,0.162281,0.000000,0.554432,0.291685,0.052632,0.335144 D_effective.csv id,context,D ex1,C_open,0.000000 ex2,C_open,0.000000 ex1,C_medium,0.166667 ex2,C_medium,0.166667 Other files may include: cleaned_input.csv, D_intr.csv, Omega.csv, Rho.csv, Frontier.csv, Analyses.csv (see Output schema for details).","title":"Input and output example"},{"location":"#operationalization","text":"What is computed and where: - S(M) (semiotic amplitude) \u2014 computed per text by semiconj.cli.compute_S_components and aggregated with semiconj.decodability.aggregate_S . - D_intr(M) (intrinsic decodability) \u2014 semiconj.semiotic.parser.compute_d_intr extracts triads and coherence components. - \u03a9(H), \u03c1(C) \u2014 semiconj.surrogates.community.omega_and_rho ; classes Interpreter and OllamaInterpreter generate labels/summaries. - D(M;H,C) \u2014 semiconj.decodability.effective_decodability combines D_intr with \u03a9 and \u03c1. - Frontier k(H,C) \u2014 semiconj.frontier.estimate_frontier (95th\u2011percentile curve over S bins). - Corpus I/O \u2014 semiconj.corpus.read_corpus , validate_corpus , excerpts_to_rows , write_csv . - CLI orchestration \u2014 semiconj.cli.run_pipeline runs the full pipeline and saves outputs. - Reporting/plots \u2014 semiconj.reporting.save_metrics , maybe_plot_frontier . - Metrics building blocks \u2014 semiconj.metrics.* (complexity, entropy, embeddings, intertextuality, figures, codeswitch).","title":"Operationalization"},{"location":"#workflow","text":"This section describes the full pipeline, from CLI invocation to outputs, including which functions/classes call which others. At a glance: - 1. CLI entry and configuration - 2. Corpus loading and cleaning - 3. Compute S(M) per text - 4. Compute D_intr(M) per text - 5. Estimate \u03a9(H) and \u03c1(C) - 6. Compute effective decodability D(M;H,C) - 7. Estimate empirical frontier k(H,C) - 8. Ancillary analyses","title":"Workflow"},{"location":"#1-cli-entry-and-configuration","text":"Entry point: semiconj.cli.main parses arguments (argparse) and sets logging. Configuration: semiconj.config.validate_config(DEFAULT_WEIGHTS, CONTEXTS) ensures weights sum to 1 and contexts are valid; then semiconj.config.set_runtime_config(...) applies runtime options (seed, embedding_dim, figures multiplier, pos_tagger). Orchestration: semiconj.cli.main calls semiconj.cli.run_pipeline(input_csv, out_dir, k=K, surrogates=..., ollama_models=..., ollama_host=...) . See also: CLI , Config .","title":"1. CLI entry and configuration"},{"location":"#2-corpus-loading-and-cleaning","text":"Read: semiconj.corpus.read_corpus(input_csv) -> List[Excerpt] . Validate: semiconj.corpus.validate_corpus(excerpts, min_words=10) drops missing/dupe/short items. Persist cleaned copy: semiconj.corpus.excerpts_to_rows \u2192 semiconj.corpus.write_csv ( out/cleaned_input.csv ). See also: Corpus .","title":"2. Corpus loading and cleaning"},{"location":"#3-compute-sm-per-text-semiotic-amplitude","text":"In run_pipeline , loop over validated excerpts. For each text: semiconj.cli.compute_S_components(text) computes component scores. Components called: semiconj.metrics.embeddings.sentence_embedding_dispersion semiconj.metrics.intertextuality.domain_coverage_score semiconj.metrics.intertextuality.ner_coverage semiconj.metrics.figures.figures_score semiconj.metrics.complexity.mtld , yules_k , senses_per_lemma semiconj.metrics.entropy.ngram_entropy (bigrams) semiconj.metrics.complexity.pos_entropy semiconj.metrics.codeswitch.codeswitch_index Aggregate: semiconj.decodability.aggregate_S(components, semiconj.config.DEFAULT_WEIGHTS) . Save: semiconj.reporting.save_metrics(out/S_metrics.csv, rows) . See also: Metrics , Decodability , Reporting .","title":"3. Compute S(M) per text (semiotic amplitude)"},{"location":"#4-compute-d_intrm-per-text-intrinsic-decodability","text":"For each text: semiconj.semiotic.parser.compute_d_intr(text) , which internally uses: semiconj.semiotic.parser.extract_triads (rule-based triads: sign, object, interpretant) semiconj.semiotic.parser.global_coherence (1 \u2212 dispersion from sentence embeddings) semiconj.semiotic.parser.interpretive_entropy (1 \u2212 mean similarity among interpretants) semiconj.semiotic.parser.interpretant_vectors (uses metrics.embeddings._avg_vec ) Save components and D_intr : semiconj.reporting.save_metrics(out/D_intr.csv, rows) . See also: Semiotic (pkg) .","title":"4. Compute D_intr(M) per text (intrinsic decodability)"},{"location":"#5-estimate-h-and-c-via-surrogate-communities","text":"Contexts: names come from semiconj.config.CONTEXTS (e.g., C_open , C_medium , C_rigid ). Strategy selection in run_pipeline : Heuristic: omega_and_rho(texts, contexts, k=K, strategy=\"heuristic\") Community: semiconj.surrogates.community.build_community -> List[Interpreter] Per (text, context): Interpreter.produce returns (label, summary) Agreement: fleiss_kappa over label assignments; semantic convergence via cosine on summaries Context rigidity \u03c1 : computed from variance reduction of summary vectors across contexts ( variance_of_vectors ) Ollama: omega_and_rho(..., strategy=\"ollama\", models=[...], ollama_host=...) Community: semiconj.surrogates.community.build_ollama_community -> List[OllamaInterpreter] Otherwise identical aggregation to compute \u03a9 and \u03c1 Persist: save_metrics(out/Omega.csv) , save_metrics(out/Rho.csv) . See also: Surrogates (pkg) .","title":"5. Estimate \u03a9(H) and \u03c1(C) via surrogate communities"},{"location":"#6-compute-effective-decodability-dmhc","text":"For each context and id: semiconj.decodability.effective_decodability(D_intr[i], omega_by_ctx[c], rho_by_ctx[c]) . Save: semiconj.reporting.save_metrics(out/D_effective.csv, rows) . See also: Decodability , Reporting .","title":"6. Compute effective decodability D(M;H,C)"},{"location":"#7-estimate-empirical-frontier-khc","text":"Aggregate D by id (mean across contexts) \u2192 D_mean . Frontier: semiconj.frontier.estimate_frontier(S_values, D_mean) returns (S_bin, k95) pairs. Save CSV: semiconj.reporting.save_metrics(out/Frontier.csv, points) . Optional plot: semiconj.reporting.maybe_plot_frontier(out/frontier.png, points) if matplotlib present. See also: Frontier , Reporting .","title":"7. Estimate empirical frontier k(H,C)"},{"location":"#8-ancillary-analyses","text":"Example: semiconj.analysis.kendall_tau(S_values, D_intr_values) \u2192 correlation summary. Save: semiconj.reporting.save_metrics(out/Analyses.csv, rows) . See also: Analysis . Data flow summary Inputs: data/corpus.csv -> read_corpus -> validate_corpus -> out/cleaned_input.csv -> compute_S_components -> aggregate_S -> out/S_metrics.csv -> compute_d_intr -> out/D_intr.csv -> omega_and_rho -> out/Omega.csv, out/Rho.csv -> effective_decodability -> out/D_effective.csv -> estimate_frontier (+ maybe_plot_frontier) -> out/Frontier.csv (+ frontier.png) -> kendall_tau -> out/Analyses.csv","title":"8. Ancillary analyses"},{"location":"output_schema/","text":"Output schema \u00b6 This page documents the CSV files written by the CLI pipeline. It focuses on what each file contains, how it is produced, and the exact column schemas. Note : General CSV conventions used across all outputs - Encoding: UTF-8, newline terminator \\n - Floats: formatted to 6 decimals - Column order: first-seen order is preserved by the writer Overview \u00b6 File Purpose Emitted by Key columns cleaned_input.csv Validated copy of the input corpus semiconj.corpus.excerpts_to_rows \u2192 write_csv id, text, meta_* S_metrics.csv Semiotic amplitude components per text and aggregate S semiconj.cli.compute_S_components , semiconj.decodability.aggregate_S semantics, intertextuality, figures, lexicon, pos, codeswitch, S D_intr.csv Intrinsic decodability components and aggregate semiconj.semiotic.parser.compute_d_intr structural_precision, convergence, coherence, entropy, D_intr Omega.csv Corpus-level interpretive homogeneity per context semiconj.surrogates.community.omega_and_rho context, Omega Rho.csv Corpus-level contextual rigidity/constraint per context semiconj.surrogates.community.omega_and_rho context, Rho D_effective.csv Effective decodability per text and context semiconj.decodability.effective_decodability id, context, D Frontier.csv Non\u2011parametric frontier points (95th percentile of X=S\u00b7D per S bin) semiconj.frontier.estimate_frontier S_bin, k95 Analyses.csv Extra analytical summaries semiconj.analysis.* metric, value cleaned_input.csv \u00b6 Purpose: cleaned copy of the input corpus after validation Emitted by: semiconj.corpus.excerpts_to_rows and semiconj.corpus.write_csv Columns Column Type Range/Domain Description id string \u2014 Unique identifier, first occurrence kept on duplicates text string \u2014 Raw text excerpt meta_* string optional Any additional input columns are kept; non meta_ names are prefixed Notes - Rows failing validation (missing id/text, duplicate id, too short) are removed prior to writing. S_metrics.csv \u00b6 Purpose: per\u2011text semiotic amplitude components and aggregate Emitted by: semiconj.cli.compute_S_components + semiconj.decodability.aggregate_S via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description id string \u2014 Record identifier semantics float [0,1] Avg of sentence embedding dispersion and domain coverage intertextuality float [0,1] Avg of NER-like coverage and domain coverage figures float [0,1] Metaphor/irony proxy score lexicon float [0,1] MTLD (normalized), Yule\u2019s K (mapped), senses/lemma (norm), bigram entropy (avg) pos float [0,1] POS-tag distribution entropy (UPOS-normalized) codeswitch float [0,1] Code\u2011switching index S float [0,1] Weighted aggregate using semiconj.config.DEFAULT_WEIGHTS D_intr.csv \u00b6 Purpose: intrinsic decodability components and aggregate per text Emitted by: semiconj.semiotic.parser.compute_d_intr via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description id string \u2014 Record identifier structural_precision float [0,1] Triads per sentence (clipped) convergence float [0,1] Mean pairwise cosine among interpretants coherence float [0,1] 1 \u2212 dispersion across sentences entropy float [0,1] 1 \u2212 mean similarity among interpretants (penalty) D_intr float [0,1] Aggregated intrinsic decodability Omega.csv \u00b6 Purpose: corpus-level interpretive homogeneity per context Emitted by: semiconj.surrogates.community.omega_and_rho via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description context string typically {C_open, C_medium, C_rigid} Context identifier (configurable) Omega float [0,1] Interpretive homogeneity of the community Rho.csv \u00b6 Purpose: corpus-level contextual rigidity/constraint per context Emitted by: semiconj.surrogates.community.omega_and_rho via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description context string typically {C_open, C_medium, C_rigid} Context identifier (configurable) Rho float [0,1] Rigidity/constraint of the context D_effective.csv \u00b6 Purpose: effective decodability per text and context Emitted by: semiconj.decodability.effective_decodability via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description id string \u2014 Record identifier context string \u2014 Context name used to compute D D float [0,1] Effective decodability = \u03a9 \u00b7 \u03c1 \u00b7 D_intr Frontier.csv \u00b6 Purpose: non\u2011parametric frontier k(H,C) estimated as the 95th percentile of X = S\u00b7D within S bins Emitted by: semiconj.frontier.estimate_frontier via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description S_bin float [0,1] Representative S value (bin center) k95 float [0,1] 95th percentile of X = S\u00b7D in that bin Analyses.csv \u00b6 Purpose: extra analytical summaries (may grow over time) Emitted by: semiconj.analysis.* via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description metric string \u2014 Metric name (e.g., Kendall_tau_S_vs_Dintr ) value float \u2014 Metric value Examples \u00b6 S_metrics.csv (excerpt) id,semantics,intertextuality,figures,lexicon,pos,codeswitch,S ex1,0.586813,0.135965,0.000000,0.800000,0.332854,0.105263,0.368584 ex2,0.598695,0.162281,0.000000,0.554432,0.291685,0.052632,0.335144 D_effective.csv (excerpt) id,context,D ex1,C_open,0.000000 ex2,C_open,0.000000 ex1,C_medium,0.166667 ex2,C_medium,0.166667","title":"Output schema"},{"location":"output_schema/#output-schema","text":"This page documents the CSV files written by the CLI pipeline. It focuses on what each file contains, how it is produced, and the exact column schemas. Note : General CSV conventions used across all outputs - Encoding: UTF-8, newline terminator \\n - Floats: formatted to 6 decimals - Column order: first-seen order is preserved by the writer","title":"Output schema"},{"location":"output_schema/#overview","text":"File Purpose Emitted by Key columns cleaned_input.csv Validated copy of the input corpus semiconj.corpus.excerpts_to_rows \u2192 write_csv id, text, meta_* S_metrics.csv Semiotic amplitude components per text and aggregate S semiconj.cli.compute_S_components , semiconj.decodability.aggregate_S semantics, intertextuality, figures, lexicon, pos, codeswitch, S D_intr.csv Intrinsic decodability components and aggregate semiconj.semiotic.parser.compute_d_intr structural_precision, convergence, coherence, entropy, D_intr Omega.csv Corpus-level interpretive homogeneity per context semiconj.surrogates.community.omega_and_rho context, Omega Rho.csv Corpus-level contextual rigidity/constraint per context semiconj.surrogates.community.omega_and_rho context, Rho D_effective.csv Effective decodability per text and context semiconj.decodability.effective_decodability id, context, D Frontier.csv Non\u2011parametric frontier points (95th percentile of X=S\u00b7D per S bin) semiconj.frontier.estimate_frontier S_bin, k95 Analyses.csv Extra analytical summaries semiconj.analysis.* metric, value","title":"Overview"},{"location":"output_schema/#cleaned_inputcsv","text":"Purpose: cleaned copy of the input corpus after validation Emitted by: semiconj.corpus.excerpts_to_rows and semiconj.corpus.write_csv Columns Column Type Range/Domain Description id string \u2014 Unique identifier, first occurrence kept on duplicates text string \u2014 Raw text excerpt meta_* string optional Any additional input columns are kept; non meta_ names are prefixed Notes - Rows failing validation (missing id/text, duplicate id, too short) are removed prior to writing.","title":"cleaned_input.csv"},{"location":"output_schema/#s_metricscsv","text":"Purpose: per\u2011text semiotic amplitude components and aggregate Emitted by: semiconj.cli.compute_S_components + semiconj.decodability.aggregate_S via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description id string \u2014 Record identifier semantics float [0,1] Avg of sentence embedding dispersion and domain coverage intertextuality float [0,1] Avg of NER-like coverage and domain coverage figures float [0,1] Metaphor/irony proxy score lexicon float [0,1] MTLD (normalized), Yule\u2019s K (mapped), senses/lemma (norm), bigram entropy (avg) pos float [0,1] POS-tag distribution entropy (UPOS-normalized) codeswitch float [0,1] Code\u2011switching index S float [0,1] Weighted aggregate using semiconj.config.DEFAULT_WEIGHTS","title":"S_metrics.csv"},{"location":"output_schema/#d_intrcsv","text":"Purpose: intrinsic decodability components and aggregate per text Emitted by: semiconj.semiotic.parser.compute_d_intr via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description id string \u2014 Record identifier structural_precision float [0,1] Triads per sentence (clipped) convergence float [0,1] Mean pairwise cosine among interpretants coherence float [0,1] 1 \u2212 dispersion across sentences entropy float [0,1] 1 \u2212 mean similarity among interpretants (penalty) D_intr float [0,1] Aggregated intrinsic decodability","title":"D_intr.csv"},{"location":"output_schema/#omegacsv","text":"Purpose: corpus-level interpretive homogeneity per context Emitted by: semiconj.surrogates.community.omega_and_rho via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description context string typically {C_open, C_medium, C_rigid} Context identifier (configurable) Omega float [0,1] Interpretive homogeneity of the community","title":"Omega.csv"},{"location":"output_schema/#rhocsv","text":"Purpose: corpus-level contextual rigidity/constraint per context Emitted by: semiconj.surrogates.community.omega_and_rho via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description context string typically {C_open, C_medium, C_rigid} Context identifier (configurable) Rho float [0,1] Rigidity/constraint of the context","title":"Rho.csv"},{"location":"output_schema/#d_effectivecsv","text":"Purpose: effective decodability per text and context Emitted by: semiconj.decodability.effective_decodability via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description id string \u2014 Record identifier context string \u2014 Context name used to compute D D float [0,1] Effective decodability = \u03a9 \u00b7 \u03c1 \u00b7 D_intr","title":"D_effective.csv"},{"location":"output_schema/#frontiercsv","text":"Purpose: non\u2011parametric frontier k(H,C) estimated as the 95th percentile of X = S\u00b7D within S bins Emitted by: semiconj.frontier.estimate_frontier via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description S_bin float [0,1] Representative S value (bin center) k95 float [0,1] 95th percentile of X = S\u00b7D in that bin","title":"Frontier.csv"},{"location":"output_schema/#analysescsv","text":"Purpose: extra analytical summaries (may grow over time) Emitted by: semiconj.analysis.* via semiconj.reporting.save_metrics Columns Column Type Range/Domain Description metric string \u2014 Metric name (e.g., Kendall_tau_S_vs_Dintr ) value float \u2014 Metric value","title":"Analyses.csv"},{"location":"output_schema/#examples","text":"S_metrics.csv (excerpt) id,semantics,intertextuality,figures,lexicon,pos,codeswitch,S ex1,0.586813,0.135965,0.000000,0.800000,0.332854,0.105263,0.368584 ex2,0.598695,0.162281,0.000000,0.554432,0.291685,0.052632,0.335144 D_effective.csv (excerpt) id,context,D ex1,C_open,0.000000 ex2,C_open,0.000000 ex1,C_medium,0.166667 ex2,C_medium,0.166667","title":"Examples"},{"location":"reference/","text":"API Reference \u00b6 semiconj \u00b6 Semiotic Conjecture \u2014 Plan A pipeline. Top-level convenience exports.","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#semiconj","text":"Semiotic Conjecture \u2014 Plan A pipeline. Top-level convenience exports.","title":"semiconj"},{"location":"api/","text":"API Overview \u00b6 semiconj \u00b6 Semiotic Conjecture \u2014 Plan A pipeline. Top-level convenience exports.","title":"Overview"},{"location":"api/#api-overview","text":"","title":"API Overview"},{"location":"api/#semiconj","text":"Semiotic Conjecture \u2014 Plan A pipeline. Top-level convenience exports.","title":"semiconj"},{"location":"api/analysis/","text":"Analysis \u00b6 kendall_tau(x, y) \u00b6 Compute Kendall's tau correlation coefficient (strict: requires SciPy). Parameters: x ( List [ float ] ) \u2013 First sequence of numbers. y ( List [ float ] ) \u2013 Second sequence of numbers, same length as x . Returns: float \u2013 The Kendall's tau correlation as a float (nan-safe cast to 0.0). Raises: ImportError \u2013 If SciPy is not installed. Examples: >>> kendall_tau([1, 2, 3], [1, 2, 3]) 1.0 Source code in semiconj/analysis.py def kendall_tau(x: List[float], y: List[float]) -> float: \"\"\"Compute Kendall's tau correlation coefficient (strict: requires SciPy). Args: x: First sequence of numbers. y: Second sequence of numbers, same length as ``x``. Returns: The Kendall's tau correlation as a float (nan-safe cast to 0.0). Raises: ImportError: If SciPy is not installed. Examples: >>> kendall_tau([1, 2, 3], [1, 2, 3]) # doctest: +SKIP 1.0 \"\"\" try: from scipy.stats import kendalltau # type: ignore except Exception as e: raise ImportError(\"scipy is required for kendall_tau. Install scipy.\") from e r, _ = kendalltau(x, y) return float(r or 0.0) rank(a) \u00b6 Return average ranks for a sequence, handling ties. Items with identical values receive the same average rank (1-based indexing averaged over the tied span). For example, [10, 10, 20] -> ranks [1.5, 1.5, 3]. Parameters: a ( List [ float ] ) \u2013 Sequence of numeric values. Returns: List [ float ] \u2013 A list of ranks (floats) of the same length as a . Examples: >>> rank([10, 10, 20]) [1.5, 1.5, 3.0] >>> rank([3, 1, 2]) [3.0, 1.0, 2.0] Source code in semiconj/analysis.py def rank(a: List[float]) -> List[float]: \"\"\"Return average ranks for a sequence, handling ties. Items with identical values receive the same average rank (1-based indexing averaged over the tied span). For example, [10, 10, 20] -> ranks [1.5, 1.5, 3]. Args: a: Sequence of numeric values. Returns: A list of ranks (floats) of the same length as ``a``. Examples: >>> rank([10, 10, 20]) [1.5, 1.5, 3.0] >>> rank([3, 1, 2]) [3.0, 1.0, 2.0] \"\"\" # Average ranks for ties order = sorted(range(len(a)), key=lambda i: a[i]) r = [0.0] * len(a) i = 0 while i < len(order): j = i while j + 1 < len(order) and a[order[j+1]] == a[order[i]]: j += 1 avg = (i + j + 2) / 2.0 for k in range(i, j+1): r[order[k]] = avg i = j + 1 return r simple_regression(y, X) \u00b6 Ordinary Least Squares via normal equations (no regularization). Adds an intercept internally and solves beta = (X'X)^(-1) X'y using a Moore\u2013Penrose pseudoinverse for numerical stability. Parameters: y ( List [ float ] ) \u2013 Response vector of length n. X ( List [ List [ float ]] ) \u2013 Design matrix of shape n x p (without intercept column). Returns: Tuple [ List [ float ], float ] \u2013 A pair (beta, R2) where beta includes the intercept as beta[0]. Raises: ImportError \u2013 If numpy is not installed. ValueError \u2013 If X is empty. Examples: >>> y = [1.0, 2.0, 3.0] >>> X = [[1.0], [2.0], [3.0]] >>> beta, R2 = simple_regression(y, X) >>> len(beta) # intercept + slope 2 Source code in semiconj/analysis.py def simple_regression(y: List[float], X: List[List[float]]) -> Tuple[List[float], float]: \"\"\"Ordinary Least Squares via normal equations (no regularization). Adds an intercept internally and solves beta = (X'X)^(-1) X'y using a Moore\u2013Penrose pseudoinverse for numerical stability. Args: y: Response vector of length n. X: Design matrix of shape n x p (without intercept column). Returns: A pair (beta, R2) where beta includes the intercept as beta[0]. Raises: ImportError: If numpy is not installed. ValueError: If X is empty. Examples: >>> y = [1.0, 2.0, 3.0] >>> X = [[1.0], [2.0], [3.0]] >>> beta, R2 = simple_regression(y, X) # doctest: +SKIP >>> len(beta) # intercept + slope 2 \"\"\" try: import numpy as np # type: ignore except Exception as e: raise ImportError(\"numpy is required for simple_regression. Install numpy.\") from e Xn = np.asarray(X, float) if Xn.size == 0: raise ValueError(\"X must be non-empty for regression.\") n, p = Xn.shape Xd = np.column_stack([np.ones(n), Xn]) yv = np.asarray(y, float) beta = np.linalg.pinv(Xd.T @ Xd) @ (Xd.T @ yv) yhat = Xd @ beta ss_tot = float(((yv - yv.mean()) ** 2).sum()) ss_res = float(((yv - yhat) ** 2).sum()) R2 = 1 - ss_res / (ss_tot + 1e-9) return beta.tolist(), float(R2) spearman_rho(x, y) \u00b6 Compute Spearman's rank correlation coefficient. Parameters: x ( List [ float ] ) \u2013 First sequence of numbers. y ( List [ float ] ) \u2013 Second sequence of numbers, same length as x . Returns: float \u2013 The Spearman rank correlation coefficient in [-1, 1]. Examples: >>> spearman_rho([1, 2, 3], [3, 2, 1]) -1.0 >>> round(spearman_rho([1, 2, 3], [10, 20, 30]), 6) 1.0 Source code in semiconj/analysis.py def spearman_rho(x: List[float], y: List[float]) -> float: \"\"\"Compute Spearman's rank correlation coefficient. Args: x: First sequence of numbers. y: Second sequence of numbers, same length as ``x``. Returns: The Spearman rank correlation coefficient in [-1, 1]. Examples: >>> spearman_rho([1, 2, 3], [3, 2, 1]) -1.0 >>> round(spearman_rho([1, 2, 3], [10, 20, 30]), 6) 1.0 \"\"\" n = len(x) if n == 0: return 0.0 rx = rank(x) ry = rank(y) d2 = sum((rx[i] - ry[i]) ** 2 for i in range(n)) return 1 - 6 * d2 / (n * (n*n - 1) + 1e-9)","title":"Analysis"},{"location":"api/analysis/#analysis","text":"","title":"Analysis"},{"location":"api/analysis/#semiconj.analysis.kendall_tau","text":"Compute Kendall's tau correlation coefficient (strict: requires SciPy). Parameters: x ( List [ float ] ) \u2013 First sequence of numbers. y ( List [ float ] ) \u2013 Second sequence of numbers, same length as x . Returns: float \u2013 The Kendall's tau correlation as a float (nan-safe cast to 0.0). Raises: ImportError \u2013 If SciPy is not installed. Examples: >>> kendall_tau([1, 2, 3], [1, 2, 3]) 1.0 Source code in semiconj/analysis.py def kendall_tau(x: List[float], y: List[float]) -> float: \"\"\"Compute Kendall's tau correlation coefficient (strict: requires SciPy). Args: x: First sequence of numbers. y: Second sequence of numbers, same length as ``x``. Returns: The Kendall's tau correlation as a float (nan-safe cast to 0.0). Raises: ImportError: If SciPy is not installed. Examples: >>> kendall_tau([1, 2, 3], [1, 2, 3]) # doctest: +SKIP 1.0 \"\"\" try: from scipy.stats import kendalltau # type: ignore except Exception as e: raise ImportError(\"scipy is required for kendall_tau. Install scipy.\") from e r, _ = kendalltau(x, y) return float(r or 0.0)","title":"kendall_tau"},{"location":"api/analysis/#semiconj.analysis.rank","text":"Return average ranks for a sequence, handling ties. Items with identical values receive the same average rank (1-based indexing averaged over the tied span). For example, [10, 10, 20] -> ranks [1.5, 1.5, 3]. Parameters: a ( List [ float ] ) \u2013 Sequence of numeric values. Returns: List [ float ] \u2013 A list of ranks (floats) of the same length as a . Examples: >>> rank([10, 10, 20]) [1.5, 1.5, 3.0] >>> rank([3, 1, 2]) [3.0, 1.0, 2.0] Source code in semiconj/analysis.py def rank(a: List[float]) -> List[float]: \"\"\"Return average ranks for a sequence, handling ties. Items with identical values receive the same average rank (1-based indexing averaged over the tied span). For example, [10, 10, 20] -> ranks [1.5, 1.5, 3]. Args: a: Sequence of numeric values. Returns: A list of ranks (floats) of the same length as ``a``. Examples: >>> rank([10, 10, 20]) [1.5, 1.5, 3.0] >>> rank([3, 1, 2]) [3.0, 1.0, 2.0] \"\"\" # Average ranks for ties order = sorted(range(len(a)), key=lambda i: a[i]) r = [0.0] * len(a) i = 0 while i < len(order): j = i while j + 1 < len(order) and a[order[j+1]] == a[order[i]]: j += 1 avg = (i + j + 2) / 2.0 for k in range(i, j+1): r[order[k]] = avg i = j + 1 return r","title":"rank"},{"location":"api/analysis/#semiconj.analysis.simple_regression","text":"Ordinary Least Squares via normal equations (no regularization). Adds an intercept internally and solves beta = (X'X)^(-1) X'y using a Moore\u2013Penrose pseudoinverse for numerical stability. Parameters: y ( List [ float ] ) \u2013 Response vector of length n. X ( List [ List [ float ]] ) \u2013 Design matrix of shape n x p (without intercept column). Returns: Tuple [ List [ float ], float ] \u2013 A pair (beta, R2) where beta includes the intercept as beta[0]. Raises: ImportError \u2013 If numpy is not installed. ValueError \u2013 If X is empty. Examples: >>> y = [1.0, 2.0, 3.0] >>> X = [[1.0], [2.0], [3.0]] >>> beta, R2 = simple_regression(y, X) >>> len(beta) # intercept + slope 2 Source code in semiconj/analysis.py def simple_regression(y: List[float], X: List[List[float]]) -> Tuple[List[float], float]: \"\"\"Ordinary Least Squares via normal equations (no regularization). Adds an intercept internally and solves beta = (X'X)^(-1) X'y using a Moore\u2013Penrose pseudoinverse for numerical stability. Args: y: Response vector of length n. X: Design matrix of shape n x p (without intercept column). Returns: A pair (beta, R2) where beta includes the intercept as beta[0]. Raises: ImportError: If numpy is not installed. ValueError: If X is empty. Examples: >>> y = [1.0, 2.0, 3.0] >>> X = [[1.0], [2.0], [3.0]] >>> beta, R2 = simple_regression(y, X) # doctest: +SKIP >>> len(beta) # intercept + slope 2 \"\"\" try: import numpy as np # type: ignore except Exception as e: raise ImportError(\"numpy is required for simple_regression. Install numpy.\") from e Xn = np.asarray(X, float) if Xn.size == 0: raise ValueError(\"X must be non-empty for regression.\") n, p = Xn.shape Xd = np.column_stack([np.ones(n), Xn]) yv = np.asarray(y, float) beta = np.linalg.pinv(Xd.T @ Xd) @ (Xd.T @ yv) yhat = Xd @ beta ss_tot = float(((yv - yv.mean()) ** 2).sum()) ss_res = float(((yv - yhat) ** 2).sum()) R2 = 1 - ss_res / (ss_tot + 1e-9) return beta.tolist(), float(R2)","title":"simple_regression"},{"location":"api/analysis/#semiconj.analysis.spearman_rho","text":"Compute Spearman's rank correlation coefficient. Parameters: x ( List [ float ] ) \u2013 First sequence of numbers. y ( List [ float ] ) \u2013 Second sequence of numbers, same length as x . Returns: float \u2013 The Spearman rank correlation coefficient in [-1, 1]. Examples: >>> spearman_rho([1, 2, 3], [3, 2, 1]) -1.0 >>> round(spearman_rho([1, 2, 3], [10, 20, 30]), 6) 1.0 Source code in semiconj/analysis.py def spearman_rho(x: List[float], y: List[float]) -> float: \"\"\"Compute Spearman's rank correlation coefficient. Args: x: First sequence of numbers. y: Second sequence of numbers, same length as ``x``. Returns: The Spearman rank correlation coefficient in [-1, 1]. Examples: >>> spearman_rho([1, 2, 3], [3, 2, 1]) -1.0 >>> round(spearman_rho([1, 2, 3], [10, 20, 30]), 6) 1.0 \"\"\" n = len(x) if n == 0: return 0.0 rx = rank(x) ry = rank(y) d2 = sum((rx[i] - ry[i]) ** 2 for i in range(n)) return 1 - 6 * d2 / (n * (n*n - 1) + 1e-9)","title":"spearman_rho"},{"location":"api/cli/","text":"CLI \u00b6","title":"CLI"},{"location":"api/cli/#cli","text":"","title":"CLI"},{"location":"api/config/","text":"Config \u00b6 Context dataclass \u00b6 Context configuration describing interpretive rigidity. Attributes: name ( str ) \u2013 Context identifier (e.g., 'C_open', 'C_medium', 'C_rigid'). description ( str ) \u2013 Human-readable description of the context constraints. Examples: >>> Context('C_open', 'No instructions.') Context(name='C_open', description='No instructions.') Source code in semiconj/config.py @dataclass(frozen=True) class Context: \"\"\"Context configuration describing interpretive rigidity. Attributes: name: Context identifier (e.g., 'C_open', 'C_medium', 'C_rigid'). description: Human-readable description of the context constraints. Examples: >>> Context('C_open', 'No instructions.') Context(name='C_open', description='No instructions.') \"\"\" name: str description: str RuntimeConfig dataclass \u00b6 Centralized runtime configuration. Attributes: seed ( int ) \u2013 Global random seed for reproducibility. embedding_dim ( int ) \u2013 Default embedding dimension for hashing/averaging embeddings. figures_multiplier ( float ) \u2013 Multiplier applied to figures_score to tune its contribution. pos_tagger ( str ) \u2013 Which POS tagger to use (\"naive\" or \"spacy\"). figures_ollama_model ( str ) \u2013 If non-empty, use this Ollama model to compute figures_score. figures_ollama_host ( str ) \u2013 Ollama server host URL for figures scoring. embeddings_ollama_model ( str ) \u2013 If non-empty, use this Ollama embedding model (e.g., 'nomic-embed-text') for sentence embeddings. embeddings_ollama_host ( str ) \u2013 Ollama server host URL for embeddings (defaults to localhost). nlp_ollama_model ( str ) \u2013 If non-empty, use this Ollama model (e.g., 'gpt-oss') for tokenization, sentence splitting, and NER. nlp_ollama_host ( str ) \u2013 Ollama server host URL for NLP tasks (defaults to localhost). Examples: >>> cfg = RuntimeConfig(seed=123, embedding_dim=64, figures_multiplier=1.5, pos_tagger='naive') >>> cfg.seed, cfg.embedding_dim, cfg.pos_tagger (123, 64, 'naive') Source code in semiconj/config.py @dataclass class RuntimeConfig: \"\"\"Centralized runtime configuration. Attributes: seed: Global random seed for reproducibility. embedding_dim: Default embedding dimension for hashing/averaging embeddings. figures_multiplier: Multiplier applied to figures_score to tune its contribution. pos_tagger: Which POS tagger to use (\"naive\" or \"spacy\"). figures_ollama_model: If non-empty, use this Ollama model to compute figures_score. figures_ollama_host: Ollama server host URL for figures scoring. embeddings_ollama_model: If non-empty, use this Ollama embedding model (e.g., 'nomic-embed-text') for sentence embeddings. embeddings_ollama_host: Ollama server host URL for embeddings (defaults to localhost). nlp_ollama_model: If non-empty, use this Ollama model (e.g., 'gpt-oss') for tokenization, sentence splitting, and NER. nlp_ollama_host: Ollama server host URL for NLP tasks (defaults to localhost). Examples: >>> cfg = RuntimeConfig(seed=123, embedding_dim=64, figures_multiplier=1.5, pos_tagger='naive') >>> cfg.seed, cfg.embedding_dim, cfg.pos_tagger (123, 64, 'naive') \"\"\" seed: int = 42 embedding_dim: int = 128 figures_multiplier: float = 1.0 pos_tagger: str = \"\" figures_ollama_model: str = \"gpt-oss:latest\" figures_ollama_host: str = \"http://localhost:11434\" embeddings_ollama_model: str = \"nomic-embed-text:v1.5\" embeddings_ollama_host: str = \"http://localhost:11434\" nlp_ollama_model: str = \"gpt-oss:latest\" nlp_ollama_host: str = \"http://localhost:11434\" get_runtime_config() \u00b6 Return the current runtime configuration object. Returns: RuntimeConfig \u2013 The live RuntimeConfig instance used across the library. Examples: >>> cfg = get_runtime_config() >>> isinstance(cfg, RuntimeConfig) True Source code in semiconj/config.py def get_runtime_config() -> RuntimeConfig: \"\"\"Return the current runtime configuration object. Returns: The live RuntimeConfig instance used across the library. Examples: >>> cfg = get_runtime_config() >>> isinstance(cfg, RuntimeConfig) True \"\"\" return _runtime_config set_runtime_config(seed=None, embedding_dim=None, figures_multiplier=None, pos_tagger=None, figures_ollama_model=None, figures_ollama_host=None, embeddings_ollama_model=None, embeddings_ollama_host=None, nlp_ollama_model=None, nlp_ollama_host=None) \u00b6 Update the global runtime configuration with provided values. Parameters: seed ( Optional [ int ] , default: None ) \u2013 Global random seed to set. embedding_dim ( Optional [ int ] , default: None ) \u2013 Default embedding dimension for hashing/averaging embeddings. figures_multiplier ( Optional [ float ] , default: None ) \u2013 Multiplier applied to figures_score for tuning. pos_tagger ( Optional [ str ] , default: None ) \u2013 Either \"naive\" or \"spacy\" (strictly validated). figures_ollama_model ( Optional [ str ] , default: None ) \u2013 If provided, enable Ollama-based figures scoring using this model name. figures_ollama_host ( Optional [ str ] , default: None ) \u2013 Ollama HTTP host used for figures scoring (defaults to localhost). embeddings_ollama_model ( Optional [ str ] , default: None ) \u2013 If provided, enable Ollama-based sentence embeddings using this model name. embeddings_ollama_host ( Optional [ str ] , default: None ) \u2013 Ollama HTTP host used for embeddings (defaults to localhost). nlp_ollama_model ( Optional [ str ] , default: None ) \u2013 If provided, enable Ollama-based NLP (tokenization, sentence splitting, NER) using this model name. nlp_ollama_host ( Optional [ str ] , default: None ) \u2013 Ollama HTTP host used for NLP tasks (defaults to localhost). Raises: ValueError \u2013 If pos_tagger is provided and not one of {\"naive\", \"spacy\"}. Examples: >>> set_runtime_config(seed=123, embedding_dim=64, pos_tagger='naive') >>> get_runtime_config().seed 123 Source code in semiconj/config.py def set_runtime_config(seed: Optional[int] = None, embedding_dim: Optional[int] = None, figures_multiplier: Optional[float] = None, pos_tagger: Optional[str] = None, figures_ollama_model: Optional[str] = None, figures_ollama_host: Optional[str] = None, embeddings_ollama_model: Optional[str] = None, embeddings_ollama_host: Optional[str] = None, nlp_ollama_model: Optional[str] = None, nlp_ollama_host: Optional[str] = None) -> None: \"\"\"Update the global runtime configuration with provided values. Args: seed: Global random seed to set. embedding_dim: Default embedding dimension for hashing/averaging embeddings. figures_multiplier: Multiplier applied to figures_score for tuning. pos_tagger: Either \"naive\" or \"spacy\" (strictly validated). figures_ollama_model: If provided, enable Ollama-based figures scoring using this model name. figures_ollama_host: Ollama HTTP host used for figures scoring (defaults to localhost). embeddings_ollama_model: If provided, enable Ollama-based sentence embeddings using this model name. embeddings_ollama_host: Ollama HTTP host used for embeddings (defaults to localhost). nlp_ollama_model: If provided, enable Ollama-based NLP (tokenization, sentence splitting, NER) using this model name. nlp_ollama_host: Ollama HTTP host used for NLP tasks (defaults to localhost). Raises: ValueError: If ``pos_tagger`` is provided and not one of {\"naive\", \"spacy\"}. Examples: >>> set_runtime_config(seed=123, embedding_dim=64, pos_tagger='naive') # doctest: +SKIP >>> get_runtime_config().seed # doctest: +SKIP 123 \"\"\" if seed is not None: _runtime_config.seed = int(seed) if embedding_dim is not None: _runtime_config.embedding_dim = int(embedding_dim) if figures_multiplier is not None: _runtime_config.figures_multiplier = float(figures_multiplier) if pos_tagger is not None: pt = str(pos_tagger).lower() if pt not in {\"naive\", \"spacy\"}: raise ValueError(\"pos_tagger must be 'naive' or 'spacy'\") _runtime_config.pos_tagger = pt if figures_ollama_model is not None: _runtime_config.figures_ollama_model = str(figures_ollama_model) if figures_ollama_host is not None: _runtime_config.figures_ollama_host = str(figures_ollama_host) if embeddings_ollama_model is not None: _runtime_config.embeddings_ollama_model = str(embeddings_ollama_model) if embeddings_ollama_host is not None: _runtime_config.embeddings_ollama_host = str(embeddings_ollama_host) if nlp_ollama_model is not None: _runtime_config.nlp_ollama_model = str(nlp_ollama_model) if nlp_ollama_host is not None: _runtime_config.nlp_ollama_host = str(nlp_ollama_host) validate_config(weights, contexts) \u00b6 Validate configuration consistency. Parameters: weights ( Dict [ str , float ] ) \u2013 Mapping of S(M) component names to weights expected to sum to 1.0. contexts ( List [ Context ] ) \u2013 List of Context objects describing available interpretive contexts. Raises: ValueError \u2013 If weights do not sum to 1.0 (\u00b11e-6), if no contexts are provided, or if any context has an empty name. Notes This function enforces strict invariants used by the pipeline. Call it early to fail fast on misconfiguration. Examples: >>> good_w = {'a': 0.5, 'b': 0.5} >>> good_c = [Context('C_open', '...')] >>> validate_config(good_w, good_c) >>> bad_w = {'a': 0.7, 'b': 0.2} >>> try: ... validate_config(bad_w, good_c) ... except ValueError: ... print('bad weights') bad weights Source code in semiconj/config.py def validate_config(weights: Dict[str, float], contexts: List[Context]) -> None: \"\"\"Validate configuration consistency. Args: weights: Mapping of S(M) component names to weights expected to sum to 1.0. contexts: List of Context objects describing available interpretive contexts. Raises: ValueError: If weights do not sum to 1.0 (\u00b11e-6), if no contexts are provided, or if any context has an empty name. Notes: This function enforces strict invariants used by the pipeline. Call it early to fail fast on misconfiguration. Examples: >>> good_w = {'a': 0.5, 'b': 0.5} >>> good_c = [Context('C_open', '...')] >>> validate_config(good_w, good_c) >>> bad_w = {'a': 0.7, 'b': 0.2} >>> try: ... validate_config(bad_w, good_c) ... except ValueError: ... print('bad weights') bad weights \"\"\" total = sum(weights.values()) if abs(total - 1.0) > 1e-6: raise ValueError(f\"S component weights must sum to 1.0 (got {total:.6f}).\") if not contexts: raise ValueError(\"At least one context must be defined.\") if any(not c.name for c in contexts): raise ValueError(\"Each context must have a non-empty name.\")","title":"Config"},{"location":"api/config/#config","text":"","title":"Config"},{"location":"api/config/#semiconj.config.Context","text":"Context configuration describing interpretive rigidity. Attributes: name ( str ) \u2013 Context identifier (e.g., 'C_open', 'C_medium', 'C_rigid'). description ( str ) \u2013 Human-readable description of the context constraints. Examples: >>> Context('C_open', 'No instructions.') Context(name='C_open', description='No instructions.') Source code in semiconj/config.py @dataclass(frozen=True) class Context: \"\"\"Context configuration describing interpretive rigidity. Attributes: name: Context identifier (e.g., 'C_open', 'C_medium', 'C_rigid'). description: Human-readable description of the context constraints. Examples: >>> Context('C_open', 'No instructions.') Context(name='C_open', description='No instructions.') \"\"\" name: str description: str","title":"Context"},{"location":"api/config/#semiconj.config.RuntimeConfig","text":"Centralized runtime configuration. Attributes: seed ( int ) \u2013 Global random seed for reproducibility. embedding_dim ( int ) \u2013 Default embedding dimension for hashing/averaging embeddings. figures_multiplier ( float ) \u2013 Multiplier applied to figures_score to tune its contribution. pos_tagger ( str ) \u2013 Which POS tagger to use (\"naive\" or \"spacy\"). figures_ollama_model ( str ) \u2013 If non-empty, use this Ollama model to compute figures_score. figures_ollama_host ( str ) \u2013 Ollama server host URL for figures scoring. embeddings_ollama_model ( str ) \u2013 If non-empty, use this Ollama embedding model (e.g., 'nomic-embed-text') for sentence embeddings. embeddings_ollama_host ( str ) \u2013 Ollama server host URL for embeddings (defaults to localhost). nlp_ollama_model ( str ) \u2013 If non-empty, use this Ollama model (e.g., 'gpt-oss') for tokenization, sentence splitting, and NER. nlp_ollama_host ( str ) \u2013 Ollama server host URL for NLP tasks (defaults to localhost). Examples: >>> cfg = RuntimeConfig(seed=123, embedding_dim=64, figures_multiplier=1.5, pos_tagger='naive') >>> cfg.seed, cfg.embedding_dim, cfg.pos_tagger (123, 64, 'naive') Source code in semiconj/config.py @dataclass class RuntimeConfig: \"\"\"Centralized runtime configuration. Attributes: seed: Global random seed for reproducibility. embedding_dim: Default embedding dimension for hashing/averaging embeddings. figures_multiplier: Multiplier applied to figures_score to tune its contribution. pos_tagger: Which POS tagger to use (\"naive\" or \"spacy\"). figures_ollama_model: If non-empty, use this Ollama model to compute figures_score. figures_ollama_host: Ollama server host URL for figures scoring. embeddings_ollama_model: If non-empty, use this Ollama embedding model (e.g., 'nomic-embed-text') for sentence embeddings. embeddings_ollama_host: Ollama server host URL for embeddings (defaults to localhost). nlp_ollama_model: If non-empty, use this Ollama model (e.g., 'gpt-oss') for tokenization, sentence splitting, and NER. nlp_ollama_host: Ollama server host URL for NLP tasks (defaults to localhost). Examples: >>> cfg = RuntimeConfig(seed=123, embedding_dim=64, figures_multiplier=1.5, pos_tagger='naive') >>> cfg.seed, cfg.embedding_dim, cfg.pos_tagger (123, 64, 'naive') \"\"\" seed: int = 42 embedding_dim: int = 128 figures_multiplier: float = 1.0 pos_tagger: str = \"\" figures_ollama_model: str = \"gpt-oss:latest\" figures_ollama_host: str = \"http://localhost:11434\" embeddings_ollama_model: str = \"nomic-embed-text:v1.5\" embeddings_ollama_host: str = \"http://localhost:11434\" nlp_ollama_model: str = \"gpt-oss:latest\" nlp_ollama_host: str = \"http://localhost:11434\"","title":"RuntimeConfig"},{"location":"api/config/#semiconj.config.get_runtime_config","text":"Return the current runtime configuration object. Returns: RuntimeConfig \u2013 The live RuntimeConfig instance used across the library. Examples: >>> cfg = get_runtime_config() >>> isinstance(cfg, RuntimeConfig) True Source code in semiconj/config.py def get_runtime_config() -> RuntimeConfig: \"\"\"Return the current runtime configuration object. Returns: The live RuntimeConfig instance used across the library. Examples: >>> cfg = get_runtime_config() >>> isinstance(cfg, RuntimeConfig) True \"\"\" return _runtime_config","title":"get_runtime_config"},{"location":"api/config/#semiconj.config.set_runtime_config","text":"Update the global runtime configuration with provided values. Parameters: seed ( Optional [ int ] , default: None ) \u2013 Global random seed to set. embedding_dim ( Optional [ int ] , default: None ) \u2013 Default embedding dimension for hashing/averaging embeddings. figures_multiplier ( Optional [ float ] , default: None ) \u2013 Multiplier applied to figures_score for tuning. pos_tagger ( Optional [ str ] , default: None ) \u2013 Either \"naive\" or \"spacy\" (strictly validated). figures_ollama_model ( Optional [ str ] , default: None ) \u2013 If provided, enable Ollama-based figures scoring using this model name. figures_ollama_host ( Optional [ str ] , default: None ) \u2013 Ollama HTTP host used for figures scoring (defaults to localhost). embeddings_ollama_model ( Optional [ str ] , default: None ) \u2013 If provided, enable Ollama-based sentence embeddings using this model name. embeddings_ollama_host ( Optional [ str ] , default: None ) \u2013 Ollama HTTP host used for embeddings (defaults to localhost). nlp_ollama_model ( Optional [ str ] , default: None ) \u2013 If provided, enable Ollama-based NLP (tokenization, sentence splitting, NER) using this model name. nlp_ollama_host ( Optional [ str ] , default: None ) \u2013 Ollama HTTP host used for NLP tasks (defaults to localhost). Raises: ValueError \u2013 If pos_tagger is provided and not one of {\"naive\", \"spacy\"}. Examples: >>> set_runtime_config(seed=123, embedding_dim=64, pos_tagger='naive') >>> get_runtime_config().seed 123 Source code in semiconj/config.py def set_runtime_config(seed: Optional[int] = None, embedding_dim: Optional[int] = None, figures_multiplier: Optional[float] = None, pos_tagger: Optional[str] = None, figures_ollama_model: Optional[str] = None, figures_ollama_host: Optional[str] = None, embeddings_ollama_model: Optional[str] = None, embeddings_ollama_host: Optional[str] = None, nlp_ollama_model: Optional[str] = None, nlp_ollama_host: Optional[str] = None) -> None: \"\"\"Update the global runtime configuration with provided values. Args: seed: Global random seed to set. embedding_dim: Default embedding dimension for hashing/averaging embeddings. figures_multiplier: Multiplier applied to figures_score for tuning. pos_tagger: Either \"naive\" or \"spacy\" (strictly validated). figures_ollama_model: If provided, enable Ollama-based figures scoring using this model name. figures_ollama_host: Ollama HTTP host used for figures scoring (defaults to localhost). embeddings_ollama_model: If provided, enable Ollama-based sentence embeddings using this model name. embeddings_ollama_host: Ollama HTTP host used for embeddings (defaults to localhost). nlp_ollama_model: If provided, enable Ollama-based NLP (tokenization, sentence splitting, NER) using this model name. nlp_ollama_host: Ollama HTTP host used for NLP tasks (defaults to localhost). Raises: ValueError: If ``pos_tagger`` is provided and not one of {\"naive\", \"spacy\"}. Examples: >>> set_runtime_config(seed=123, embedding_dim=64, pos_tagger='naive') # doctest: +SKIP >>> get_runtime_config().seed # doctest: +SKIP 123 \"\"\" if seed is not None: _runtime_config.seed = int(seed) if embedding_dim is not None: _runtime_config.embedding_dim = int(embedding_dim) if figures_multiplier is not None: _runtime_config.figures_multiplier = float(figures_multiplier) if pos_tagger is not None: pt = str(pos_tagger).lower() if pt not in {\"naive\", \"spacy\"}: raise ValueError(\"pos_tagger must be 'naive' or 'spacy'\") _runtime_config.pos_tagger = pt if figures_ollama_model is not None: _runtime_config.figures_ollama_model = str(figures_ollama_model) if figures_ollama_host is not None: _runtime_config.figures_ollama_host = str(figures_ollama_host) if embeddings_ollama_model is not None: _runtime_config.embeddings_ollama_model = str(embeddings_ollama_model) if embeddings_ollama_host is not None: _runtime_config.embeddings_ollama_host = str(embeddings_ollama_host) if nlp_ollama_model is not None: _runtime_config.nlp_ollama_model = str(nlp_ollama_model) if nlp_ollama_host is not None: _runtime_config.nlp_ollama_host = str(nlp_ollama_host)","title":"set_runtime_config"},{"location":"api/config/#semiconj.config.validate_config","text":"Validate configuration consistency. Parameters: weights ( Dict [ str , float ] ) \u2013 Mapping of S(M) component names to weights expected to sum to 1.0. contexts ( List [ Context ] ) \u2013 List of Context objects describing available interpretive contexts. Raises: ValueError \u2013 If weights do not sum to 1.0 (\u00b11e-6), if no contexts are provided, or if any context has an empty name. Notes This function enforces strict invariants used by the pipeline. Call it early to fail fast on misconfiguration. Examples: >>> good_w = {'a': 0.5, 'b': 0.5} >>> good_c = [Context('C_open', '...')] >>> validate_config(good_w, good_c) >>> bad_w = {'a': 0.7, 'b': 0.2} >>> try: ... validate_config(bad_w, good_c) ... except ValueError: ... print('bad weights') bad weights Source code in semiconj/config.py def validate_config(weights: Dict[str, float], contexts: List[Context]) -> None: \"\"\"Validate configuration consistency. Args: weights: Mapping of S(M) component names to weights expected to sum to 1.0. contexts: List of Context objects describing available interpretive contexts. Raises: ValueError: If weights do not sum to 1.0 (\u00b11e-6), if no contexts are provided, or if any context has an empty name. Notes: This function enforces strict invariants used by the pipeline. Call it early to fail fast on misconfiguration. Examples: >>> good_w = {'a': 0.5, 'b': 0.5} >>> good_c = [Context('C_open', '...')] >>> validate_config(good_w, good_c) >>> bad_w = {'a': 0.7, 'b': 0.2} >>> try: ... validate_config(bad_w, good_c) ... except ValueError: ... print('bad weights') bad weights \"\"\" total = sum(weights.values()) if abs(total - 1.0) > 1e-6: raise ValueError(f\"S component weights must sum to 1.0 (got {total:.6f}).\") if not contexts: raise ValueError(\"At least one context must be defined.\") if any(not c.name for c in contexts): raise ValueError(\"Each context must have a non-empty name.\")","title":"validate_config"},{"location":"api/corpus/","text":"Corpus \u00b6 Excerpt dataclass \u00b6 A single corpus record with text and metadata. Attributes: id ( str ) \u2013 Unique identifier of the record (stringified). text ( str ) \u2013 The raw text excerpt. meta ( Dict [ str , str ] ) \u2013 Arbitrary metadata as a mapping from column name to value. Examples: >>> Excerpt(id=\"001\", text=\"Hello world.\", meta={\"meta_domain\": \"news\"}) Excerpt(id='001', text='Hello world.', meta={'meta_domain': 'news'}) Source code in semiconj/corpus.py @dataclass class Excerpt: \"\"\"A single corpus record with text and metadata. Attributes: id: Unique identifier of the record (stringified). text: The raw text excerpt. meta: Arbitrary metadata as a mapping from column name to value. Examples: >>> Excerpt(id=\"001\", text=\"Hello world.\", meta={\"meta_domain\": \"news\"}) Excerpt(id='001', text='Hello world.', meta={'meta_domain': 'news'}) \"\"\" id: str text: str meta: Dict[str, str] excerpts_to_rows(excerpts) \u00b6 Convert excerpts into CSV-ready row dicts, preserving meta_* keys. For each Excerpt, this function emits a dict with at least id and text fields. Any metadata keys not starting with meta_ are prefixed accordingly. Parameters: excerpts ( List [ Excerpt ] ) \u2013 List of Excerpt objects. Returns: List [ Dict [ str , object ]] \u2013 A list of dictionaries suitable for CSV writing with write_csv . Examples: >>> ex = [Excerpt('1', 'Hello', {'domain': 'news'})] >>> rows = excerpts_to_rows(ex) >>> rows[0]['id'], rows[0]['text'], rows[0]['meta_domain'] ('1', 'Hello', 'news') Source code in semiconj/corpus.py def excerpts_to_rows(excerpts: List[Excerpt]) -> List[Dict[str, object]]: \"\"\"Convert excerpts into CSV-ready row dicts, preserving meta_* keys. For each Excerpt, this function emits a dict with at least ``id`` and ``text`` fields. Any metadata keys not starting with ``meta_`` are prefixed accordingly. Args: excerpts: List of Excerpt objects. Returns: A list of dictionaries suitable for CSV writing with ``write_csv``. Examples: >>> ex = [Excerpt('1', 'Hello', {'domain': 'news'})] >>> rows = excerpts_to_rows(ex) >>> rows[0]['id'], rows[0]['text'], rows[0]['meta_domain'] ('1', 'Hello', 'news') \"\"\" rows: List[Dict[str, object]] = [] for ex in excerpts: row: Dict[str, object] = {\"id\": ex.id, \"text\": ex.text} for k, v in ex.meta.items(): col = k if k.startswith(\"meta_\") else f\"meta_{k}\" row[col] = v rows.append(row) return rows read_corpus(csv_path) \u00b6 Read a CSV corpus into a list of Excerpt records. The CSV must contain at least the columns id and text . Any other columns are stored as metadata in the meta dict. Parameters: csv_path ( Path ) \u2013 Path to the input CSV file. Returns: List [ Excerpt ] \u2013 A list of Excerpt instances parsed from the CSV. Raises: ValueError \u2013 If required columns are missing. Examples: >>> from pathlib import Path >>> rows = read_corpus(Path('data/sample_corpus.csv')) >>> isinstance(rows, list) True Source code in semiconj/corpus.py def read_corpus(csv_path: Path) -> List[Excerpt]: \"\"\"Read a CSV corpus into a list of Excerpt records. The CSV must contain at least the columns ``id`` and ``text``. Any other columns are stored as metadata in the ``meta`` dict. Args: csv_path: Path to the input CSV file. Returns: A list of Excerpt instances parsed from the CSV. Raises: ValueError: If required columns are missing. Examples: >>> from pathlib import Path >>> rows = read_corpus(Path('data/sample_corpus.csv')) # doctest: +SKIP >>> isinstance(rows, list) # doctest: +SKIP True \"\"\" rows: List[Excerpt] = [] with open(csv_path, newline='', encoding='utf-8') as f: reader = csv.DictReader(f) if not reader.fieldnames or 'id' not in reader.fieldnames or 'text' not in reader.fieldnames: raise ValueError(\"CSV must contain 'id' and 'text' columns\") for r in reader: meta = {k: (v if v is not None else '') for k, v in r.items() if k not in {\"id\", \"text\"}} rows.append(Excerpt(id=str(r.get('id', '')).strip(), text=str(r.get('text', '') or '').strip(), meta=meta)) return rows validate_corpus(excerpts, min_words=10) \u00b6 Validate a corpus and return cleaned excerpts plus a summary report. Rules applied in order: - Drop rows with missing/empty id or text. - Drop duplicate ids (keep the first occurrence). - Drop extremely short texts (fewer than min_words tokens). Parameters: excerpts ( List [ Excerpt ] ) \u2013 List of Excerpt instances to validate. min_words ( int , default: 10 ) \u2013 Minimum whitespace-separated token count required to keep a text. Returns: List [ Excerpt ] \u2013 A pair (cleaned, report) where cleaned is the filtered list of excerpts Dict [ str , int ] \u2013 and report is a dict with counts: total , dropped_missing , dropped_dupe , Tuple [ List [ Excerpt ], Dict [ str , int ]] \u2013 dropped_short , kept . Examples: >>> ex = [ ... Excerpt('1', 'too short', {}), ... Excerpt('1', 'duplicate id', {}), ... Excerpt('2', 'this is long enough text for validation', {}), ... ] >>> cleaned, report = validate_corpus(ex, min_words=3) >>> [e.id for e in cleaned] ['2'] Source code in semiconj/corpus.py def validate_corpus(excerpts: List[Excerpt], min_words: int = 10) -> Tuple[List[Excerpt], Dict[str, int]]: \"\"\"Validate a corpus and return cleaned excerpts plus a summary report. Rules applied in order: - Drop rows with missing/empty id or text. - Drop duplicate ids (keep the first occurrence). - Drop extremely short texts (fewer than ``min_words`` tokens). Args: excerpts: List of Excerpt instances to validate. min_words: Minimum whitespace-separated token count required to keep a text. Returns: A pair ``(cleaned, report)`` where ``cleaned`` is the filtered list of excerpts and ``report`` is a dict with counts: ``total``, ``dropped_missing``, ``dropped_dupe``, ``dropped_short``, ``kept``. Examples: >>> ex = [ ... Excerpt('1', 'too short', {}), ... Excerpt('1', 'duplicate id', {}), ... Excerpt('2', 'this is long enough text for validation', {}), ... ] >>> cleaned, report = validate_corpus(ex, min_words=3) >>> [e.id for e in cleaned] ['2'] \"\"\" report = {\"total\": len(excerpts), \"dropped_missing\": 0, \"dropped_dupe\": 0, \"dropped_short\": 0, \"kept\": 0} seen: Set[str] = set() cleaned: List[Excerpt] = [] for ex in excerpts: if not ex.id or not ex.text: report[\"dropped_missing\"] += 1 continue if ex.id in seen: report[\"dropped_dupe\"] += 1 continue if len(ex.text.split()) < max(0, min_words): report[\"dropped_short\"] += 1 continue seen.add(ex.id) cleaned.append(ex) report[\"kept\"] = len(cleaned) logger.info( \"Validated corpus: total=%d kept=%d dropped_missing=%d dropped_dupe=%d dropped_short=%d\", report[\"total\"], report[\"kept\"], report[\"dropped_missing\"], report[\"dropped_dupe\"], report[\"dropped_short\"], ) return cleaned, report write_csv(path, rows) \u00b6 Write dictionaries to CSV, preserving all observed columns. This function computes the union of keys across all rows to avoid dropping sparse columns. Floats are formatted to 6 decimals to ensure consistency. Parameters: path ( Path ) \u2013 Destination CSV path. rows ( Iterable [ Dict [ str , object ]] ) \u2013 Iterable of mapping objects to serialize. Examples: >>> from pathlib import Path >>> rows = [{'id': '1', 'S': 0.5}, {'id': '2', 'S': 0.75, 'extra': 'x'}] >>> write_csv(Path('tmp.csv'), rows) # Creates tmp.csv with header: id,S,extra (order preserved by first occurrence) Source code in semiconj/corpus.py def write_csv(path: Path, rows: Iterable[Dict[str, object]]) -> None: \"\"\"Write dictionaries to CSV, preserving all observed columns. This function computes the union of keys across all rows to avoid dropping sparse columns. Floats are formatted to 6 decimals to ensure consistency. Args: path: Destination CSV path. rows: Iterable of mapping objects to serialize. Examples: >>> from pathlib import Path >>> rows = [{'id': '1', 'S': 0.5}, {'id': '2', 'S': 0.75, 'extra': 'x'}] >>> write_csv(Path('tmp.csv'), rows) # doctest: +SKIP # Creates tmp.csv with header: id,S,extra (order preserved by first occurrence) \"\"\" rows = list(rows) if not rows: return path.parent.mkdir(parents=True, exist_ok=True) # Determine union of fieldnames across rows to avoid dropping columns fieldnames: List[str] = [] seen_fields: Set[str] = set() for r in rows: for k in r.keys(): if k not in seen_fields: seen_fields.add(k) fieldnames.append(k) def _format(v: object) -> object: # Ensure numeric formatting consistency if isinstance(v, float): return f\"{v:.6f}\" return v with open(path, 'w', newline='', encoding='utf-8') as f: writer = csv.DictWriter( f, fieldnames=fieldnames, extrasaction='ignore', restval='', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n', ) writer.writeheader() for r in rows: writer.writerow({k: _format(r.get(k, '')) for k in fieldnames})","title":"Corpus"},{"location":"api/corpus/#corpus","text":"","title":"Corpus"},{"location":"api/corpus/#semiconj.corpus.Excerpt","text":"A single corpus record with text and metadata. Attributes: id ( str ) \u2013 Unique identifier of the record (stringified). text ( str ) \u2013 The raw text excerpt. meta ( Dict [ str , str ] ) \u2013 Arbitrary metadata as a mapping from column name to value. Examples: >>> Excerpt(id=\"001\", text=\"Hello world.\", meta={\"meta_domain\": \"news\"}) Excerpt(id='001', text='Hello world.', meta={'meta_domain': 'news'}) Source code in semiconj/corpus.py @dataclass class Excerpt: \"\"\"A single corpus record with text and metadata. Attributes: id: Unique identifier of the record (stringified). text: The raw text excerpt. meta: Arbitrary metadata as a mapping from column name to value. Examples: >>> Excerpt(id=\"001\", text=\"Hello world.\", meta={\"meta_domain\": \"news\"}) Excerpt(id='001', text='Hello world.', meta={'meta_domain': 'news'}) \"\"\" id: str text: str meta: Dict[str, str]","title":"Excerpt"},{"location":"api/corpus/#semiconj.corpus.excerpts_to_rows","text":"Convert excerpts into CSV-ready row dicts, preserving meta_* keys. For each Excerpt, this function emits a dict with at least id and text fields. Any metadata keys not starting with meta_ are prefixed accordingly. Parameters: excerpts ( List [ Excerpt ] ) \u2013 List of Excerpt objects. Returns: List [ Dict [ str , object ]] \u2013 A list of dictionaries suitable for CSV writing with write_csv . Examples: >>> ex = [Excerpt('1', 'Hello', {'domain': 'news'})] >>> rows = excerpts_to_rows(ex) >>> rows[0]['id'], rows[0]['text'], rows[0]['meta_domain'] ('1', 'Hello', 'news') Source code in semiconj/corpus.py def excerpts_to_rows(excerpts: List[Excerpt]) -> List[Dict[str, object]]: \"\"\"Convert excerpts into CSV-ready row dicts, preserving meta_* keys. For each Excerpt, this function emits a dict with at least ``id`` and ``text`` fields. Any metadata keys not starting with ``meta_`` are prefixed accordingly. Args: excerpts: List of Excerpt objects. Returns: A list of dictionaries suitable for CSV writing with ``write_csv``. Examples: >>> ex = [Excerpt('1', 'Hello', {'domain': 'news'})] >>> rows = excerpts_to_rows(ex) >>> rows[0]['id'], rows[0]['text'], rows[0]['meta_domain'] ('1', 'Hello', 'news') \"\"\" rows: List[Dict[str, object]] = [] for ex in excerpts: row: Dict[str, object] = {\"id\": ex.id, \"text\": ex.text} for k, v in ex.meta.items(): col = k if k.startswith(\"meta_\") else f\"meta_{k}\" row[col] = v rows.append(row) return rows","title":"excerpts_to_rows"},{"location":"api/corpus/#semiconj.corpus.read_corpus","text":"Read a CSV corpus into a list of Excerpt records. The CSV must contain at least the columns id and text . Any other columns are stored as metadata in the meta dict. Parameters: csv_path ( Path ) \u2013 Path to the input CSV file. Returns: List [ Excerpt ] \u2013 A list of Excerpt instances parsed from the CSV. Raises: ValueError \u2013 If required columns are missing. Examples: >>> from pathlib import Path >>> rows = read_corpus(Path('data/sample_corpus.csv')) >>> isinstance(rows, list) True Source code in semiconj/corpus.py def read_corpus(csv_path: Path) -> List[Excerpt]: \"\"\"Read a CSV corpus into a list of Excerpt records. The CSV must contain at least the columns ``id`` and ``text``. Any other columns are stored as metadata in the ``meta`` dict. Args: csv_path: Path to the input CSV file. Returns: A list of Excerpt instances parsed from the CSV. Raises: ValueError: If required columns are missing. Examples: >>> from pathlib import Path >>> rows = read_corpus(Path('data/sample_corpus.csv')) # doctest: +SKIP >>> isinstance(rows, list) # doctest: +SKIP True \"\"\" rows: List[Excerpt] = [] with open(csv_path, newline='', encoding='utf-8') as f: reader = csv.DictReader(f) if not reader.fieldnames or 'id' not in reader.fieldnames or 'text' not in reader.fieldnames: raise ValueError(\"CSV must contain 'id' and 'text' columns\") for r in reader: meta = {k: (v if v is not None else '') for k, v in r.items() if k not in {\"id\", \"text\"}} rows.append(Excerpt(id=str(r.get('id', '')).strip(), text=str(r.get('text', '') or '').strip(), meta=meta)) return rows","title":"read_corpus"},{"location":"api/corpus/#semiconj.corpus.validate_corpus","text":"Validate a corpus and return cleaned excerpts plus a summary report. Rules applied in order: - Drop rows with missing/empty id or text. - Drop duplicate ids (keep the first occurrence). - Drop extremely short texts (fewer than min_words tokens). Parameters: excerpts ( List [ Excerpt ] ) \u2013 List of Excerpt instances to validate. min_words ( int , default: 10 ) \u2013 Minimum whitespace-separated token count required to keep a text. Returns: List [ Excerpt ] \u2013 A pair (cleaned, report) where cleaned is the filtered list of excerpts Dict [ str , int ] \u2013 and report is a dict with counts: total , dropped_missing , dropped_dupe , Tuple [ List [ Excerpt ], Dict [ str , int ]] \u2013 dropped_short , kept . Examples: >>> ex = [ ... Excerpt('1', 'too short', {}), ... Excerpt('1', 'duplicate id', {}), ... Excerpt('2', 'this is long enough text for validation', {}), ... ] >>> cleaned, report = validate_corpus(ex, min_words=3) >>> [e.id for e in cleaned] ['2'] Source code in semiconj/corpus.py def validate_corpus(excerpts: List[Excerpt], min_words: int = 10) -> Tuple[List[Excerpt], Dict[str, int]]: \"\"\"Validate a corpus and return cleaned excerpts plus a summary report. Rules applied in order: - Drop rows with missing/empty id or text. - Drop duplicate ids (keep the first occurrence). - Drop extremely short texts (fewer than ``min_words`` tokens). Args: excerpts: List of Excerpt instances to validate. min_words: Minimum whitespace-separated token count required to keep a text. Returns: A pair ``(cleaned, report)`` where ``cleaned`` is the filtered list of excerpts and ``report`` is a dict with counts: ``total``, ``dropped_missing``, ``dropped_dupe``, ``dropped_short``, ``kept``. Examples: >>> ex = [ ... Excerpt('1', 'too short', {}), ... Excerpt('1', 'duplicate id', {}), ... Excerpt('2', 'this is long enough text for validation', {}), ... ] >>> cleaned, report = validate_corpus(ex, min_words=3) >>> [e.id for e in cleaned] ['2'] \"\"\" report = {\"total\": len(excerpts), \"dropped_missing\": 0, \"dropped_dupe\": 0, \"dropped_short\": 0, \"kept\": 0} seen: Set[str] = set() cleaned: List[Excerpt] = [] for ex in excerpts: if not ex.id or not ex.text: report[\"dropped_missing\"] += 1 continue if ex.id in seen: report[\"dropped_dupe\"] += 1 continue if len(ex.text.split()) < max(0, min_words): report[\"dropped_short\"] += 1 continue seen.add(ex.id) cleaned.append(ex) report[\"kept\"] = len(cleaned) logger.info( \"Validated corpus: total=%d kept=%d dropped_missing=%d dropped_dupe=%d dropped_short=%d\", report[\"total\"], report[\"kept\"], report[\"dropped_missing\"], report[\"dropped_dupe\"], report[\"dropped_short\"], ) return cleaned, report","title":"validate_corpus"},{"location":"api/corpus/#semiconj.corpus.write_csv","text":"Write dictionaries to CSV, preserving all observed columns. This function computes the union of keys across all rows to avoid dropping sparse columns. Floats are formatted to 6 decimals to ensure consistency. Parameters: path ( Path ) \u2013 Destination CSV path. rows ( Iterable [ Dict [ str , object ]] ) \u2013 Iterable of mapping objects to serialize. Examples: >>> from pathlib import Path >>> rows = [{'id': '1', 'S': 0.5}, {'id': '2', 'S': 0.75, 'extra': 'x'}] >>> write_csv(Path('tmp.csv'), rows) # Creates tmp.csv with header: id,S,extra (order preserved by first occurrence) Source code in semiconj/corpus.py def write_csv(path: Path, rows: Iterable[Dict[str, object]]) -> None: \"\"\"Write dictionaries to CSV, preserving all observed columns. This function computes the union of keys across all rows to avoid dropping sparse columns. Floats are formatted to 6 decimals to ensure consistency. Args: path: Destination CSV path. rows: Iterable of mapping objects to serialize. Examples: >>> from pathlib import Path >>> rows = [{'id': '1', 'S': 0.5}, {'id': '2', 'S': 0.75, 'extra': 'x'}] >>> write_csv(Path('tmp.csv'), rows) # doctest: +SKIP # Creates tmp.csv with header: id,S,extra (order preserved by first occurrence) \"\"\" rows = list(rows) if not rows: return path.parent.mkdir(parents=True, exist_ok=True) # Determine union of fieldnames across rows to avoid dropping columns fieldnames: List[str] = [] seen_fields: Set[str] = set() for r in rows: for k in r.keys(): if k not in seen_fields: seen_fields.add(k) fieldnames.append(k) def _format(v: object) -> object: # Ensure numeric formatting consistency if isinstance(v, float): return f\"{v:.6f}\" return v with open(path, 'w', newline='', encoding='utf-8') as f: writer = csv.DictWriter( f, fieldnames=fieldnames, extrasaction='ignore', restval='', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n', ) writer.writeheader() for r in rows: writer.writerow({k: _format(r.get(k, '')) for k in fieldnames})","title":"write_csv"},{"location":"api/decodability/","text":"Decodability \u00b6 aggregate_S(metrics, weights) \u00b6 Aggregate S(M) components with weights and clip to [0, 1]. The metrics dict should contain at least the components present in weights (typically: semantics , intertextuality , figures , lexicon , pos , codeswitch ). Each metric is individually clipped to [0,1] before weighting and summation; the final score is clipped to [0,1]. Parameters: metrics ( Dict [ str , float ] ) \u2013 Mapping from component name to score (expected in [0,1]). weights ( Dict [ str , float ] ) \u2013 Mapping from component name to non-negative weights that sum to 1. Returns: float \u2013 Aggregated S score in [0, 1]. Raises: KeyError \u2013 If any required component present in weights is missing from metrics . Examples: >>> metrics = { ... 'semantics': 0.7, 'intertextuality': 0.6, 'figures': 0.3, ... 'lexicon': 0.5, 'pos': 0.4, 'codeswitch': 0.1 ... } >>> weights = { ... 'semantics': 0.30, 'intertextuality': 0.25, 'figures': 0.15, ... 'lexicon': 0.15, 'pos': 0.10, 'codeswitch': 0.05 ... } >>> round(aggregate_S(metrics, weights), 6) 0.5... Source code in semiconj/decodability.py def aggregate_S(metrics: Dict[str, float], weights: Dict[str, float]) -> float: \"\"\"Aggregate S(M) components with weights and clip to [0, 1]. The metrics dict should contain at least the components present in ``weights`` (typically: ``semantics``, ``intertextuality``, ``figures``, ``lexicon``, ``pos``, ``codeswitch``). Each metric is individually clipped to [0,1] before weighting and summation; the final score is clipped to [0,1]. Args: metrics: Mapping from component name to score (expected in [0,1]). weights: Mapping from component name to non-negative weights that sum to 1. Returns: Aggregated S score in [0, 1]. Raises: KeyError: If any required component present in ``weights`` is missing from ``metrics``. Examples: >>> metrics = { ... 'semantics': 0.7, 'intertextuality': 0.6, 'figures': 0.3, ... 'lexicon': 0.5, 'pos': 0.4, 'codeswitch': 0.1 ... } >>> weights = { ... 'semantics': 0.30, 'intertextuality': 0.25, 'figures': 0.15, ... 'lexicon': 0.15, 'pos': 0.10, 'codeswitch': 0.05 ... } >>> round(aggregate_S(metrics, weights), 6) # doctest: +ELLIPSIS 0.5... \"\"\" missing = [k for k in weights.keys() if k not in metrics] if missing: raise KeyError(f\"Missing required S components: {missing}\") score = 0.0 for k, w in weights.items(): v = float(metrics[k]) score += w * max(0.0, min(1.0, v)) return max(0.0, min(1.0, score)) effective_decodability(D_intr, omega, rho) \u00b6 Compute effective decodability D given intrinsic decodability and community/context factors. This clips the product to the [0, 1] interval to ensure a bounded result. Parameters: D_intr ( float ) \u2013 Intrinsic decodability in [0, 1]. omega ( float ) \u2013 Community homogeneity/agreement in [0, 1]. rho ( float ) \u2013 Context rigidity/constraint in [0, 1]. Returns: float \u2013 A float in [0, 1] equal to clip(D_intr * omega * rho) . Examples: >>> effective_decodability(0.6, 0.8, 0.5) 0.24 >>> effective_decodability(2.0, 0.5, 0.5) # clipped to 1.0 then multiplied 0.25 Source code in semiconj/decodability.py def effective_decodability(D_intr: float, omega: float, rho: float) -> float: \"\"\"Compute effective decodability D given intrinsic decodability and community/context factors. This clips the product to the [0, 1] interval to ensure a bounded result. Args: D_intr: Intrinsic decodability in [0, 1]. omega: Community homogeneity/agreement in [0, 1]. rho: Context rigidity/constraint in [0, 1]. Returns: A float in [0, 1] equal to ``clip(D_intr * omega * rho)``. Examples: >>> effective_decodability(0.6, 0.8, 0.5) 0.24 >>> effective_decodability(2.0, 0.5, 0.5) # clipped to 1.0 then multiplied 0.25 \"\"\" return max(0.0, min(1.0, float(D_intr) * float(omega) * float(rho)))","title":"Decodability"},{"location":"api/decodability/#decodability","text":"","title":"Decodability"},{"location":"api/decodability/#semiconj.decodability.aggregate_S","text":"Aggregate S(M) components with weights and clip to [0, 1]. The metrics dict should contain at least the components present in weights (typically: semantics , intertextuality , figures , lexicon , pos , codeswitch ). Each metric is individually clipped to [0,1] before weighting and summation; the final score is clipped to [0,1]. Parameters: metrics ( Dict [ str , float ] ) \u2013 Mapping from component name to score (expected in [0,1]). weights ( Dict [ str , float ] ) \u2013 Mapping from component name to non-negative weights that sum to 1. Returns: float \u2013 Aggregated S score in [0, 1]. Raises: KeyError \u2013 If any required component present in weights is missing from metrics . Examples: >>> metrics = { ... 'semantics': 0.7, 'intertextuality': 0.6, 'figures': 0.3, ... 'lexicon': 0.5, 'pos': 0.4, 'codeswitch': 0.1 ... } >>> weights = { ... 'semantics': 0.30, 'intertextuality': 0.25, 'figures': 0.15, ... 'lexicon': 0.15, 'pos': 0.10, 'codeswitch': 0.05 ... } >>> round(aggregate_S(metrics, weights), 6) 0.5... Source code in semiconj/decodability.py def aggregate_S(metrics: Dict[str, float], weights: Dict[str, float]) -> float: \"\"\"Aggregate S(M) components with weights and clip to [0, 1]. The metrics dict should contain at least the components present in ``weights`` (typically: ``semantics``, ``intertextuality``, ``figures``, ``lexicon``, ``pos``, ``codeswitch``). Each metric is individually clipped to [0,1] before weighting and summation; the final score is clipped to [0,1]. Args: metrics: Mapping from component name to score (expected in [0,1]). weights: Mapping from component name to non-negative weights that sum to 1. Returns: Aggregated S score in [0, 1]. Raises: KeyError: If any required component present in ``weights`` is missing from ``metrics``. Examples: >>> metrics = { ... 'semantics': 0.7, 'intertextuality': 0.6, 'figures': 0.3, ... 'lexicon': 0.5, 'pos': 0.4, 'codeswitch': 0.1 ... } >>> weights = { ... 'semantics': 0.30, 'intertextuality': 0.25, 'figures': 0.15, ... 'lexicon': 0.15, 'pos': 0.10, 'codeswitch': 0.05 ... } >>> round(aggregate_S(metrics, weights), 6) # doctest: +ELLIPSIS 0.5... \"\"\" missing = [k for k in weights.keys() if k not in metrics] if missing: raise KeyError(f\"Missing required S components: {missing}\") score = 0.0 for k, w in weights.items(): v = float(metrics[k]) score += w * max(0.0, min(1.0, v)) return max(0.0, min(1.0, score))","title":"aggregate_S"},{"location":"api/decodability/#semiconj.decodability.effective_decodability","text":"Compute effective decodability D given intrinsic decodability and community/context factors. This clips the product to the [0, 1] interval to ensure a bounded result. Parameters: D_intr ( float ) \u2013 Intrinsic decodability in [0, 1]. omega ( float ) \u2013 Community homogeneity/agreement in [0, 1]. rho ( float ) \u2013 Context rigidity/constraint in [0, 1]. Returns: float \u2013 A float in [0, 1] equal to clip(D_intr * omega * rho) . Examples: >>> effective_decodability(0.6, 0.8, 0.5) 0.24 >>> effective_decodability(2.0, 0.5, 0.5) # clipped to 1.0 then multiplied 0.25 Source code in semiconj/decodability.py def effective_decodability(D_intr: float, omega: float, rho: float) -> float: \"\"\"Compute effective decodability D given intrinsic decodability and community/context factors. This clips the product to the [0, 1] interval to ensure a bounded result. Args: D_intr: Intrinsic decodability in [0, 1]. omega: Community homogeneity/agreement in [0, 1]. rho: Context rigidity/constraint in [0, 1]. Returns: A float in [0, 1] equal to ``clip(D_intr * omega * rho)``. Examples: >>> effective_decodability(0.6, 0.8, 0.5) 0.24 >>> effective_decodability(2.0, 0.5, 0.5) # clipped to 1.0 then multiplied 0.25 \"\"\" return max(0.0, min(1.0, float(D_intr) * float(omega) * float(rho)))","title":"effective_decodability"},{"location":"api/frontier/","text":"Frontier \u00b6 estimate_frontier(S, D, n_bins=6) \u00b6 Estimate a non-parametric frontier curve using rank bins and 95th percentiles. For each item i, compute x_i = S_i * D_i. Sort items by S_i and partition into n_bins rank-based bins. Within each bin, take the 95th percentile of x as the frontier value. The returned points are evenly spaced S-bin centers in [0,1]. Parameters: S ( List [ float ] ) \u2013 List of S(M) scores in [0,1]. Must have the same length as D. D ( List [ float ] ) \u2013 List of decodability scores in [0,1]. Must have the same length as S. n_bins ( int , default: 6 ) \u2013 Number of rank bins to produce along the S axis. Returns: List [ Tuple [ float , float ]] \u2013 List of (S_bin_center, k95) pairs where k95 is the 95th percentile of x=S\u00b7D List [ Tuple [ float , float ]] \u2013 within the corresponding bin. Raises: AssertionError \u2013 If len(S) != len(D) . Examples: >>> S = [0.1, 0.2, 0.8, 0.9] >>> D = [0.5, 0.6, 0.7, 0.4] >>> pts = estimate_frontier(S, D, n_bins=2) >>> len(pts) 2 >>> all(0.0 <= s <= 1.0 and 0.0 <= k <= 1.0 for s, k in pts) True Source code in semiconj/frontier.py def estimate_frontier(S: List[float], D: List[float], n_bins: int = 6) -> List[Tuple[float, float]]: \"\"\"Estimate a non-parametric frontier curve using rank bins and 95th percentiles. For each item i, compute x_i = S_i * D_i. Sort items by S_i and partition into ``n_bins`` rank-based bins. Within each bin, take the 95th percentile of x as the frontier value. The returned points are evenly spaced S-bin centers in [0,1]. Args: S: List of S(M) scores in [0,1]. Must have the same length as D. D: List of decodability scores in [0,1]. Must have the same length as S. n_bins: Number of rank bins to produce along the S axis. Returns: List of (S_bin_center, k95) pairs where k95 is the 95th percentile of x=S\u00b7D within the corresponding bin. Raises: AssertionError: If ``len(S) != len(D)``. Examples: >>> S = [0.1, 0.2, 0.8, 0.9] >>> D = [0.5, 0.6, 0.7, 0.4] >>> pts = estimate_frontier(S, D, n_bins=2) >>> len(pts) 2 >>> all(0.0 <= s <= 1.0 and 0.0 <= k <= 1.0 for s, k in pts) True \"\"\" assert len(S) == len(D) n = len(S) if n == 0: return [] pairs = sorted((S[i], S[i]*D[i]) for i in range(n)) bins: List[List[float]] = [[] for _ in range(n_bins)] edges = [i*(1.0/n_bins) for i in range(n_bins+1)] # Bin by rank of S for rank, (s, x) in enumerate(pairs): b = min(n_bins-1, int(rank / max(1, n//n_bins))) bins[b].append(x) def perc95(vals: List[float]) -> float: if not vals: return 0.0 vals = sorted(vals) k = max(0, int(0.95 * (len(vals)-1))) return vals[k] frontier = [] for b in range(n_bins): center = (b + 0.5) / n_bins frontier.append((center, perc95(bins[b]))) return frontier","title":"Frontier"},{"location":"api/frontier/#frontier","text":"","title":"Frontier"},{"location":"api/frontier/#semiconj.frontier.estimate_frontier","text":"Estimate a non-parametric frontier curve using rank bins and 95th percentiles. For each item i, compute x_i = S_i * D_i. Sort items by S_i and partition into n_bins rank-based bins. Within each bin, take the 95th percentile of x as the frontier value. The returned points are evenly spaced S-bin centers in [0,1]. Parameters: S ( List [ float ] ) \u2013 List of S(M) scores in [0,1]. Must have the same length as D. D ( List [ float ] ) \u2013 List of decodability scores in [0,1]. Must have the same length as S. n_bins ( int , default: 6 ) \u2013 Number of rank bins to produce along the S axis. Returns: List [ Tuple [ float , float ]] \u2013 List of (S_bin_center, k95) pairs where k95 is the 95th percentile of x=S\u00b7D List [ Tuple [ float , float ]] \u2013 within the corresponding bin. Raises: AssertionError \u2013 If len(S) != len(D) . Examples: >>> S = [0.1, 0.2, 0.8, 0.9] >>> D = [0.5, 0.6, 0.7, 0.4] >>> pts = estimate_frontier(S, D, n_bins=2) >>> len(pts) 2 >>> all(0.0 <= s <= 1.0 and 0.0 <= k <= 1.0 for s, k in pts) True Source code in semiconj/frontier.py def estimate_frontier(S: List[float], D: List[float], n_bins: int = 6) -> List[Tuple[float, float]]: \"\"\"Estimate a non-parametric frontier curve using rank bins and 95th percentiles. For each item i, compute x_i = S_i * D_i. Sort items by S_i and partition into ``n_bins`` rank-based bins. Within each bin, take the 95th percentile of x as the frontier value. The returned points are evenly spaced S-bin centers in [0,1]. Args: S: List of S(M) scores in [0,1]. Must have the same length as D. D: List of decodability scores in [0,1]. Must have the same length as S. n_bins: Number of rank bins to produce along the S axis. Returns: List of (S_bin_center, k95) pairs where k95 is the 95th percentile of x=S\u00b7D within the corresponding bin. Raises: AssertionError: If ``len(S) != len(D)``. Examples: >>> S = [0.1, 0.2, 0.8, 0.9] >>> D = [0.5, 0.6, 0.7, 0.4] >>> pts = estimate_frontier(S, D, n_bins=2) >>> len(pts) 2 >>> all(0.0 <= s <= 1.0 and 0.0 <= k <= 1.0 for s, k in pts) True \"\"\" assert len(S) == len(D) n = len(S) if n == 0: return [] pairs = sorted((S[i], S[i]*D[i]) for i in range(n)) bins: List[List[float]] = [[] for _ in range(n_bins)] edges = [i*(1.0/n_bins) for i in range(n_bins+1)] # Bin by rank of S for rank, (s, x) in enumerate(pairs): b = min(n_bins-1, int(rank / max(1, n//n_bins))) bins[b].append(x) def perc95(vals: List[float]) -> float: if not vals: return 0.0 vals = sorted(vals) k = max(0, int(0.95 * (len(vals)-1))) return vals[k] frontier = [] for b in range(n_bins): center = (b + 0.5) / n_bins frontier.append((center, perc95(bins[b]))) return frontier","title":"estimate_frontier"},{"location":"api/metrics/","text":"Metrics Package \u00b6 codeswitch_index(text) \u00b6 Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. Source code in semiconj/metrics/codeswitch.py def codeswitch_index(text: str) -> float: \"\"\"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. \"\"\" tokens = [t.lower() for t in re.findall(r\"[\\w']+\", text)] if not tokens: return 0.0 counts = {lang: 0 for lang in STOPWORDS} for t in tokens: for lang, sw in STOPWORDS.items(): if t in sw: counts[lang] += 1 dominant = max(counts, key=counts.get) dom_words = STOPWORDS[dominant] non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant)) return min(1.0, non_dom / max(1, len(tokens))) domain_coverage_score(text) \u00b6 Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. Source code in semiconj/metrics/intertextuality.py def domain_coverage_score(text: str) -> float: \"\"\"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. \"\"\" words = set(w.lower() for w in re.findall(r\"[\\w']+\", text)) hits = 0 for ws in DOMAINS.values(): if words & ws: hits += 1 return hits / max(1, len(DOMAINS)) figures_score(text) \u00b6 Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply cfg.figures_multiplier and clip to [0,1]. Source code in semiconj/metrics/figures.py def figures_score(text: str) -> float: \"\"\"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1]. \"\"\" from logging import getLogger logger = getLogger(__name__) cfg = get_runtime_config() def _heuristic(t: str) -> float: if not t.strip(): logger.debug(\"figures_score: empty text -> 0.0\") return 0.0 L = max(1, len(t.split())) m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS) i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS) raw = (m + i) / L return max(0.0, min(1.0, 5.0 * raw)) def _ollama(t: str) -> float: try: # Lazy imports to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore import json import re as _re if not getattr(cfg, \"figures_ollama_model\", \"\").strip(): raise RuntimeError(\"No figures_ollama_model configured\") client = get_shared_client(host=getattr(cfg, \"figures_ollama_host\", \"http://localhost:11434\")) system = ( \"You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1].\" ) rubric = ( \"Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text \" \"on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations.\" ) prompt = \"Text:\\n\" + t.strip() + \"\\n\\n\" + rubric + \"\\nRespond as JSON: {\\\"score\\\": <float between 0 and 1>}\" raw = client.generate( model=getattr(cfg, \"figures_ollama_model\"), prompt=prompt, system=system, temperature=0.2, seed=getattr(cfg, \"seed\", None), ) score_val = None if raw: start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) score_val = obj.get(\"score\", None) if score_val is not None: score_val = float(score_val) except Exception: score_val = None if score_val is None and raw: m = _re.search(r\"([01](?:\\\\.\\\\d+)?)\", raw) if m: try: score_val = float(m.group(1)) except Exception: score_val = None if score_val is None: raise ValueError(\"Could not parse 'score' from Ollama response\") return max(0.0, min(1.0, float(score_val))) except Exception as e: logger.warning(\"figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)\", e) raise # Try Ollama if configured; otherwise heuristic base_score: float if getattr(cfg, \"figures_ollama_model\", \"\").strip(): try: base_score = _ollama(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) except Exception: logger.warning(\"figures_score: Ollama scoring failed; falling back to heuristic\") pass # fall through to heuristic base_score = _heuristic(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) mtld(tokens, ttr_threshold=0.72, min_segment=10) \u00b6 Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. Source code in semiconj/metrics/complexity.py def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -> float: \"\"\"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. \"\"\" if not tokens: return 0.0 types = set() factor_count = 0 token_count = 0 ttr = 1.0 for tok in tokens: token_count += 1 types.add(tok) ttr = len(types) / token_count if ttr <= ttr_threshold and token_count >= min_segment: factor_count += 1 types.clear() token_count = 0 if token_count > 0: partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold) factor_count += partial return (len(tokens) / max(1e-9, factor_count)) ner_coverage(text) \u00b6 Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. Source code in semiconj/metrics/intertextuality.py def ner_coverage(text: str) -> float: \"\"\"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) ents = res.get(\"entities\", []) toks = res.get(\"tokens\", []) total = len(toks) if isinstance(toks, list) and toks else None if isinstance(ents, list) and ents: if not total: # If tokenizer not provided or empty, estimate using regex length total = len(re.findall(r\"[\\w']+\", text)) total = max(1, int(total or 0)) cov = len(ents) / total return max(0.0, min(1.0, cov)) except Exception: pass except Exception: pass tokens = re.findall(r\"[\\w']+\", text) if not tokens: return 0.0 ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {\"i\"}] return min(1.0, len(ent_like) / max(1, len(tokens))) ngram_entropy(tokens, n=1) \u00b6 Compute normalized entropy over token n-grams. Parameters: tokens ( List [ str ] ) \u2013 List of tokens. n ( int , default: 1 ) \u2013 N-gram size (default 1 for unigrams). Returns: float \u2013 Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 Source code in semiconj/metrics/entropy.py def ngram_entropy(tokens: List[str], n: int = 1) -> float: \"\"\"Compute normalized entropy over token n-grams. Args: tokens: List of tokens. n: N-gram size (default 1 for unigrams). Returns: Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 \"\"\" if len(tokens) < n: return 0.0 return entropy_from_counts(Counter(ngrams(tokens, n))) pos_entropy(tokens) \u00b6 Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. Source code in semiconj/metrics/complexity.py def pos_entropy(tokens: List[str]) -> float: \"\"\"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. \"\"\" tags = pos_tags(tokens) total = len(tags) or 1 counts = Counter(tags) H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(_UPOS_TAGS)), 2) return max(0.0, min(1.0, H / (max_H + 1e-9))) senses_per_lemma(tokens) \u00b6 Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. Source code in semiconj/metrics/complexity.py def senses_per_lemma(tokens: List[str]) -> float: \"\"\"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. \"\"\" try: from nltk.corpus import wordnet as wn # type: ignore except Exception as e: raise ImportError(\"NLTK is required for senses_per_lemma. Install nltk and wordnet data.\") from e lemmas = set(tokens) if not lemmas: return 0.0 total = 0 try: for w in lemmas: total += len(wn.synsets(w)) except Exception as e: # Typically LookupError when wordnet data is missing raise LookupError(\"NLTK WordNet data not found. Run: python -m nltk.downloader wordnet\") from e return total / max(1, len(lemmas)) sentence_embedding_dispersion(sentences) \u00b6 Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). Source code in semiconj/metrics/embeddings.py def sentence_embedding_dispersion(sentences: List[List[str]]) -> float: \"\"\"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). \"\"\" if not sentences: logger.debug(\"sentence_embedding_dispersion: empty sentences -> 0.0\") return 0.0 cfg = get_runtime_config() vecs: List[List[float]] use_ollama_model = getattr(cfg, \"embeddings_ollama_model\", \"\").strip() if use_ollama_model: try: # Any empty token list would complicate dimension handling; fallback in that case if any(len(s) == 0 for s in sentences): raise ValueError(\"empty sentence token list\") # Lazy import to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"embeddings_ollama_host\", \"http://localhost:11434\")) vecs = [client.embed(use_ollama_model, \" \".join(s)) for s in sentences] except Exception as e: logger.warning( \"sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)\", e, ) vecs = [_avg_vec(s) for s in sentences] else: vecs = [_avg_vec(s) for s in sentences] n = len(vecs) if n < 2: logger.debug(\"sentence_embedding_dispersion: <2 sentences -> 0.0\") return 0.0 dsum = 0.0 cnt = 0 for i in range(n): for j in range(i + 1, n): sim = _cosine(vecs[i], vecs[j]) dsum += (1.0 - sim) # distance cnt += 1 return dsum / max(1, cnt) yules_k(tokens) \u00b6 Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. Source code in semiconj/metrics/complexity.py def yules_k(tokens: List[str]) -> float: \"\"\"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. \"\"\" if not tokens: return 0.0 freqs = Counter(tokens) N = sum(freqs.values()) M2 = sum(n * n * v for n, v in Counter(freqs.values()).items()) K = 1e4 * (M2 - N) / (N * N) # invert to larger-better, normalized roughly to [0,1] return 1.0 / (1.0 + K) codeswitch \u00b6 codeswitch_index(text) \u00b6 Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. Source code in semiconj/metrics/codeswitch.py def codeswitch_index(text: str) -> float: \"\"\"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. \"\"\" tokens = [t.lower() for t in re.findall(r\"[\\w']+\", text)] if not tokens: return 0.0 counts = {lang: 0 for lang in STOPWORDS} for t in tokens: for lang, sw in STOPWORDS.items(): if t in sw: counts[lang] += 1 dominant = max(counts, key=counts.get) dom_words = STOPWORDS[dominant] non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant)) return min(1.0, non_dom / max(1, len(tokens))) complexity \u00b6 mtld(tokens, ttr_threshold=0.72, min_segment=10) \u00b6 Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. Source code in semiconj/metrics/complexity.py def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -> float: \"\"\"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. \"\"\" if not tokens: return 0.0 types = set() factor_count = 0 token_count = 0 ttr = 1.0 for tok in tokens: token_count += 1 types.add(tok) ttr = len(types) / token_count if ttr <= ttr_threshold and token_count >= min_segment: factor_count += 1 types.clear() token_count = 0 if token_count > 0: partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold) factor_count += partial return (len(tokens) / max(1e-9, factor_count)) naive_pos_tags(tokens) \u00b6 Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. Source code in semiconj/metrics/complexity.py def naive_pos_tags(tokens: List[str]) -> List[str]: \"\"\"Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. \"\"\" tags = [] for w in tokens: if w.endswith('ly'): tags.append('RB') elif w.endswith('ing') or w.endswith('ed'): tags.append('VB') elif w[0:1].isupper(): tags.append('NNP') elif w in {\"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\"}: tags.append('VB') elif w in {\"the\", \"a\", \"an\"}: tags.append('DT') elif w in {\"and\", \"or\", \"but\", \"so\", \"because\"}: tags.append('CC') else: tags.append('NN') return tags pos_entropy(tokens) \u00b6 Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. Source code in semiconj/metrics/complexity.py def pos_entropy(tokens: List[str]) -> float: \"\"\"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. \"\"\" tags = pos_tags(tokens) total = len(tags) or 1 counts = Counter(tags) H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(_UPOS_TAGS)), 2) return max(0.0, min(1.0, H / (max_H + 1e-9))) pos_tags(tokens) \u00b6 Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. Source code in semiconj/metrics/complexity.py def pos_tags(tokens: List[str]) -> List[str]: \"\"\"Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. \"\"\" from ..config import get_runtime_config cfg = get_runtime_config() # Try Ollama-based POS tagging first if configured model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model and tokens: try: from ..surrogates.ollama_client import OllamaClient # type: ignore import json allowed = sorted(list(_UPOS_TAGS)) client = OllamaClient(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) # Build a strict prompt to tag exactly the provided tokens system = ( \"You are a POS tagger. Respond in strict JSON only with a single key 'pos' \" \"containing Universal POS tags (UPOS) for each input token in order.\" ) # Use json.dumps to pass the exact token list to avoid tokenization drift tokens_json = json.dumps(tokens) rubric = ( \"Tag each token with one of the UPOS tags: \" + \", \".join(allowed) + \". \" + \"Return JSON exactly as: {\\\"pos\\\": [\\\"TAG1\\\", \\\"TAG2\\\", \\\"...\\\"]} with length equal to the number of input tokens.\" ) prompt = ( \"Tokens:\\n\" + tokens_json + \"\\n\\n\" + rubric + \"\\n\" \"Do not include explanations or additional keys.\" ) raw = client.generate(model=model, prompt=prompt, system=system, temperature=0.0, seed=getattr(cfg, \"seed\", None)) start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) pos_list = obj.get(\"pos\", []) if isinstance(pos_list, list) and len(pos_list) == len(tokens): tags: List[str] = [] for t in pos_list: tag = str(t).upper() tags.append(tag if tag in _UPOS_TAGS else \"X\") return tags except Exception: pass except Exception: # Fall through to spaCy/naive logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to naive POS tagger.') pass # Fallbacks if cfg.pos_tagger == \"spacy\": return _maybe_spacy_pos(tokens) return naive_pos_tags(tokens) senses_per_lemma(tokens) \u00b6 Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. Source code in semiconj/metrics/complexity.py def senses_per_lemma(tokens: List[str]) -> float: \"\"\"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. \"\"\" try: from nltk.corpus import wordnet as wn # type: ignore except Exception as e: raise ImportError(\"NLTK is required for senses_per_lemma. Install nltk and wordnet data.\") from e lemmas = set(tokens) if not lemmas: return 0.0 total = 0 try: for w in lemmas: total += len(wn.synsets(w)) except Exception as e: # Typically LookupError when wordnet data is missing raise LookupError(\"NLTK WordNet data not found. Run: python -m nltk.downloader wordnet\") from e return total / max(1, len(lemmas)) tokenize(text) \u00b6 Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. Source code in semiconj/metrics/complexity.py def tokenize(text: str) -> List[str]: \"\"\"Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) toks = res.get(\"tokens\", []) if isinstance(toks, list) and toks: # Normalize to lower-case to preserve previous behavior return [str(t).lower() for t in toks if isinstance(t, str)] except Exception: # fall back to regex below logging.getLogger(__name__).warning( f\"Ollama NLP model '{model}' not found. Falling back to regex-based tokenization.\" ) pass except Exception: # If config import fails for any reason, use regex pass return _WORD_RE.findall(text.lower()) yules_k(tokens) \u00b6 Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. Source code in semiconj/metrics/complexity.py def yules_k(tokens: List[str]) -> float: \"\"\"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. \"\"\" if not tokens: return 0.0 freqs = Counter(tokens) N = sum(freqs.values()) M2 = sum(n * n * v for n, v in Counter(freqs.values()).items()) K = 1e4 * (M2 - N) / (N * N) # invert to larger-better, normalized roughly to [0,1] return 1.0 / (1.0 + K) embeddings \u00b6 sentence_embedding_dispersion(sentences) \u00b6 Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). Source code in semiconj/metrics/embeddings.py def sentence_embedding_dispersion(sentences: List[List[str]]) -> float: \"\"\"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). \"\"\" if not sentences: logger.debug(\"sentence_embedding_dispersion: empty sentences -> 0.0\") return 0.0 cfg = get_runtime_config() vecs: List[List[float]] use_ollama_model = getattr(cfg, \"embeddings_ollama_model\", \"\").strip() if use_ollama_model: try: # Any empty token list would complicate dimension handling; fallback in that case if any(len(s) == 0 for s in sentences): raise ValueError(\"empty sentence token list\") # Lazy import to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"embeddings_ollama_host\", \"http://localhost:11434\")) vecs = [client.embed(use_ollama_model, \" \".join(s)) for s in sentences] except Exception as e: logger.warning( \"sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)\", e, ) vecs = [_avg_vec(s) for s in sentences] else: vecs = [_avg_vec(s) for s in sentences] n = len(vecs) if n < 2: logger.debug(\"sentence_embedding_dispersion: <2 sentences -> 0.0\") return 0.0 dsum = 0.0 cnt = 0 for i in range(n): for j in range(i + 1, n): sim = _cosine(vecs[i], vecs[j]) dsum += (1.0 - sim) # distance cnt += 1 return dsum / max(1, cnt) entropy \u00b6 entropy_from_counts(counts) \u00b6 Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Parameters: counts ( Counter ) \u2013 Counter mapping events to their frequencies. Returns: float \u2013 Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 Source code in semiconj/metrics/entropy.py def entropy_from_counts(counts: Counter) -> float: \"\"\"Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Args: counts: Counter mapping events to their frequencies. Returns: Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 \"\"\" total = sum(counts.values()) or 1 H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(counts)), 2) if max_H == 0: return 0.0 return H / max_H ngram_entropy(tokens, n=1) \u00b6 Compute normalized entropy over token n-grams. Parameters: tokens ( List [ str ] ) \u2013 List of tokens. n ( int , default: 1 ) \u2013 N-gram size (default 1 for unigrams). Returns: float \u2013 Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 Source code in semiconj/metrics/entropy.py def ngram_entropy(tokens: List[str], n: int = 1) -> float: \"\"\"Compute normalized entropy over token n-grams. Args: tokens: List of tokens. n: N-gram size (default 1 for unigrams). Returns: Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 \"\"\" if len(tokens) < n: return 0.0 return entropy_from_counts(Counter(ngrams(tokens, n))) ngrams(tokens, n) \u00b6 Yield consecutive n-grams from a list of tokens. Parameters: tokens ( List [ str ] ) \u2013 Sequence of token strings. n ( int ) \u2013 Size of the n-gram (n >= 1). Yields: Iterable [ Tuple [ str , ...]] \u2013 Tuples of length n representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] Source code in semiconj/metrics/entropy.py def ngrams(tokens: List[str], n: int) -> Iterable[Tuple[str, ...]]: \"\"\"Yield consecutive n-grams from a list of tokens. Args: tokens: Sequence of token strings. n: Size of the n-gram (n >= 1). Yields: Tuples of length ``n`` representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] \"\"\" for i in range(len(tokens) - n + 1): yield tuple(tokens[i:i+n]) figures \u00b6 figures_score(text) \u00b6 Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply cfg.figures_multiplier and clip to [0,1]. Source code in semiconj/metrics/figures.py def figures_score(text: str) -> float: \"\"\"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1]. \"\"\" from logging import getLogger logger = getLogger(__name__) cfg = get_runtime_config() def _heuristic(t: str) -> float: if not t.strip(): logger.debug(\"figures_score: empty text -> 0.0\") return 0.0 L = max(1, len(t.split())) m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS) i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS) raw = (m + i) / L return max(0.0, min(1.0, 5.0 * raw)) def _ollama(t: str) -> float: try: # Lazy imports to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore import json import re as _re if not getattr(cfg, \"figures_ollama_model\", \"\").strip(): raise RuntimeError(\"No figures_ollama_model configured\") client = get_shared_client(host=getattr(cfg, \"figures_ollama_host\", \"http://localhost:11434\")) system = ( \"You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1].\" ) rubric = ( \"Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text \" \"on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations.\" ) prompt = \"Text:\\n\" + t.strip() + \"\\n\\n\" + rubric + \"\\nRespond as JSON: {\\\"score\\\": <float between 0 and 1>}\" raw = client.generate( model=getattr(cfg, \"figures_ollama_model\"), prompt=prompt, system=system, temperature=0.2, seed=getattr(cfg, \"seed\", None), ) score_val = None if raw: start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) score_val = obj.get(\"score\", None) if score_val is not None: score_val = float(score_val) except Exception: score_val = None if score_val is None and raw: m = _re.search(r\"([01](?:\\\\.\\\\d+)?)\", raw) if m: try: score_val = float(m.group(1)) except Exception: score_val = None if score_val is None: raise ValueError(\"Could not parse 'score' from Ollama response\") return max(0.0, min(1.0, float(score_val))) except Exception as e: logger.warning(\"figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)\", e) raise # Try Ollama if configured; otherwise heuristic base_score: float if getattr(cfg, \"figures_ollama_model\", \"\").strip(): try: base_score = _ollama(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) except Exception: logger.warning(\"figures_score: Ollama scoring failed; falling back to heuristic\") pass # fall through to heuristic base_score = _heuristic(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) intertextuality \u00b6 domain_coverage_score(text) \u00b6 Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. Source code in semiconj/metrics/intertextuality.py def domain_coverage_score(text: str) -> float: \"\"\"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. \"\"\" words = set(w.lower() for w in re.findall(r\"[\\w']+\", text)) hits = 0 for ws in DOMAINS.values(): if words & ws: hits += 1 return hits / max(1, len(DOMAINS)) ner_coverage(text) \u00b6 Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. Source code in semiconj/metrics/intertextuality.py def ner_coverage(text: str) -> float: \"\"\"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) ents = res.get(\"entities\", []) toks = res.get(\"tokens\", []) total = len(toks) if isinstance(toks, list) and toks else None if isinstance(ents, list) and ents: if not total: # If tokenizer not provided or empty, estimate using regex length total = len(re.findall(r\"[\\w']+\", text)) total = max(1, int(total or 0)) cov = len(ents) / total return max(0.0, min(1.0, cov)) except Exception: pass except Exception: pass tokens = re.findall(r\"[\\w']+\", text) if not tokens: return 0.0 ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {\"i\"}] return min(1.0, len(ent_like) / max(1, len(tokens))) Submodules \u00b6 codeswitch \u00b6 codeswitch_index(text) \u00b6 Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. Source code in semiconj/metrics/codeswitch.py def codeswitch_index(text: str) -> float: \"\"\"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. \"\"\" tokens = [t.lower() for t in re.findall(r\"[\\w']+\", text)] if not tokens: return 0.0 counts = {lang: 0 for lang in STOPWORDS} for t in tokens: for lang, sw in STOPWORDS.items(): if t in sw: counts[lang] += 1 dominant = max(counts, key=counts.get) dom_words = STOPWORDS[dominant] non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant)) return min(1.0, non_dom / max(1, len(tokens))) complexity \u00b6 mtld(tokens, ttr_threshold=0.72, min_segment=10) \u00b6 Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. Source code in semiconj/metrics/complexity.py def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -> float: \"\"\"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. \"\"\" if not tokens: return 0.0 types = set() factor_count = 0 token_count = 0 ttr = 1.0 for tok in tokens: token_count += 1 types.add(tok) ttr = len(types) / token_count if ttr <= ttr_threshold and token_count >= min_segment: factor_count += 1 types.clear() token_count = 0 if token_count > 0: partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold) factor_count += partial return (len(tokens) / max(1e-9, factor_count)) naive_pos_tags(tokens) \u00b6 Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. Source code in semiconj/metrics/complexity.py def naive_pos_tags(tokens: List[str]) -> List[str]: \"\"\"Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. \"\"\" tags = [] for w in tokens: if w.endswith('ly'): tags.append('RB') elif w.endswith('ing') or w.endswith('ed'): tags.append('VB') elif w[0:1].isupper(): tags.append('NNP') elif w in {\"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\"}: tags.append('VB') elif w in {\"the\", \"a\", \"an\"}: tags.append('DT') elif w in {\"and\", \"or\", \"but\", \"so\", \"because\"}: tags.append('CC') else: tags.append('NN') return tags pos_entropy(tokens) \u00b6 Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. Source code in semiconj/metrics/complexity.py def pos_entropy(tokens: List[str]) -> float: \"\"\"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. \"\"\" tags = pos_tags(tokens) total = len(tags) or 1 counts = Counter(tags) H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(_UPOS_TAGS)), 2) return max(0.0, min(1.0, H / (max_H + 1e-9))) pos_tags(tokens) \u00b6 Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. Source code in semiconj/metrics/complexity.py def pos_tags(tokens: List[str]) -> List[str]: \"\"\"Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. \"\"\" from ..config import get_runtime_config cfg = get_runtime_config() # Try Ollama-based POS tagging first if configured model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model and tokens: try: from ..surrogates.ollama_client import OllamaClient # type: ignore import json allowed = sorted(list(_UPOS_TAGS)) client = OllamaClient(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) # Build a strict prompt to tag exactly the provided tokens system = ( \"You are a POS tagger. Respond in strict JSON only with a single key 'pos' \" \"containing Universal POS tags (UPOS) for each input token in order.\" ) # Use json.dumps to pass the exact token list to avoid tokenization drift tokens_json = json.dumps(tokens) rubric = ( \"Tag each token with one of the UPOS tags: \" + \", \".join(allowed) + \". \" + \"Return JSON exactly as: {\\\"pos\\\": [\\\"TAG1\\\", \\\"TAG2\\\", \\\"...\\\"]} with length equal to the number of input tokens.\" ) prompt = ( \"Tokens:\\n\" + tokens_json + \"\\n\\n\" + rubric + \"\\n\" \"Do not include explanations or additional keys.\" ) raw = client.generate(model=model, prompt=prompt, system=system, temperature=0.0, seed=getattr(cfg, \"seed\", None)) start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) pos_list = obj.get(\"pos\", []) if isinstance(pos_list, list) and len(pos_list) == len(tokens): tags: List[str] = [] for t in pos_list: tag = str(t).upper() tags.append(tag if tag in _UPOS_TAGS else \"X\") return tags except Exception: pass except Exception: # Fall through to spaCy/naive logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to naive POS tagger.') pass # Fallbacks if cfg.pos_tagger == \"spacy\": return _maybe_spacy_pos(tokens) return naive_pos_tags(tokens) senses_per_lemma(tokens) \u00b6 Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. Source code in semiconj/metrics/complexity.py def senses_per_lemma(tokens: List[str]) -> float: \"\"\"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. \"\"\" try: from nltk.corpus import wordnet as wn # type: ignore except Exception as e: raise ImportError(\"NLTK is required for senses_per_lemma. Install nltk and wordnet data.\") from e lemmas = set(tokens) if not lemmas: return 0.0 total = 0 try: for w in lemmas: total += len(wn.synsets(w)) except Exception as e: # Typically LookupError when wordnet data is missing raise LookupError(\"NLTK WordNet data not found. Run: python -m nltk.downloader wordnet\") from e return total / max(1, len(lemmas)) tokenize(text) \u00b6 Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. Source code in semiconj/metrics/complexity.py def tokenize(text: str) -> List[str]: \"\"\"Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) toks = res.get(\"tokens\", []) if isinstance(toks, list) and toks: # Normalize to lower-case to preserve previous behavior return [str(t).lower() for t in toks if isinstance(t, str)] except Exception: # fall back to regex below logging.getLogger(__name__).warning( f\"Ollama NLP model '{model}' not found. Falling back to regex-based tokenization.\" ) pass except Exception: # If config import fails for any reason, use regex pass return _WORD_RE.findall(text.lower()) yules_k(tokens) \u00b6 Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. Source code in semiconj/metrics/complexity.py def yules_k(tokens: List[str]) -> float: \"\"\"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. \"\"\" if not tokens: return 0.0 freqs = Counter(tokens) N = sum(freqs.values()) M2 = sum(n * n * v for n, v in Counter(freqs.values()).items()) K = 1e4 * (M2 - N) / (N * N) # invert to larger-better, normalized roughly to [0,1] return 1.0 / (1.0 + K) embeddings \u00b6 sentence_embedding_dispersion(sentences) \u00b6 Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). Source code in semiconj/metrics/embeddings.py def sentence_embedding_dispersion(sentences: List[List[str]]) -> float: \"\"\"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). \"\"\" if not sentences: logger.debug(\"sentence_embedding_dispersion: empty sentences -> 0.0\") return 0.0 cfg = get_runtime_config() vecs: List[List[float]] use_ollama_model = getattr(cfg, \"embeddings_ollama_model\", \"\").strip() if use_ollama_model: try: # Any empty token list would complicate dimension handling; fallback in that case if any(len(s) == 0 for s in sentences): raise ValueError(\"empty sentence token list\") # Lazy import to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"embeddings_ollama_host\", \"http://localhost:11434\")) vecs = [client.embed(use_ollama_model, \" \".join(s)) for s in sentences] except Exception as e: logger.warning( \"sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)\", e, ) vecs = [_avg_vec(s) for s in sentences] else: vecs = [_avg_vec(s) for s in sentences] n = len(vecs) if n < 2: logger.debug(\"sentence_embedding_dispersion: <2 sentences -> 0.0\") return 0.0 dsum = 0.0 cnt = 0 for i in range(n): for j in range(i + 1, n): sim = _cosine(vecs[i], vecs[j]) dsum += (1.0 - sim) # distance cnt += 1 return dsum / max(1, cnt) entropy \u00b6 entropy_from_counts(counts) \u00b6 Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Parameters: counts ( Counter ) \u2013 Counter mapping events to their frequencies. Returns: float \u2013 Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 Source code in semiconj/metrics/entropy.py def entropy_from_counts(counts: Counter) -> float: \"\"\"Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Args: counts: Counter mapping events to their frequencies. Returns: Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 \"\"\" total = sum(counts.values()) or 1 H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(counts)), 2) if max_H == 0: return 0.0 return H / max_H ngram_entropy(tokens, n=1) \u00b6 Compute normalized entropy over token n-grams. Parameters: tokens ( List [ str ] ) \u2013 List of tokens. n ( int , default: 1 ) \u2013 N-gram size (default 1 for unigrams). Returns: float \u2013 Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 Source code in semiconj/metrics/entropy.py def ngram_entropy(tokens: List[str], n: int = 1) -> float: \"\"\"Compute normalized entropy over token n-grams. Args: tokens: List of tokens. n: N-gram size (default 1 for unigrams). Returns: Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 \"\"\" if len(tokens) < n: return 0.0 return entropy_from_counts(Counter(ngrams(tokens, n))) ngrams(tokens, n) \u00b6 Yield consecutive n-grams from a list of tokens. Parameters: tokens ( List [ str ] ) \u2013 Sequence of token strings. n ( int ) \u2013 Size of the n-gram (n >= 1). Yields: Iterable [ Tuple [ str , ...]] \u2013 Tuples of length n representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] Source code in semiconj/metrics/entropy.py def ngrams(tokens: List[str], n: int) -> Iterable[Tuple[str, ...]]: \"\"\"Yield consecutive n-grams from a list of tokens. Args: tokens: Sequence of token strings. n: Size of the n-gram (n >= 1). Yields: Tuples of length ``n`` representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] \"\"\" for i in range(len(tokens) - n + 1): yield tuple(tokens[i:i+n]) figures \u00b6 figures_score(text) \u00b6 Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply cfg.figures_multiplier and clip to [0,1]. Source code in semiconj/metrics/figures.py def figures_score(text: str) -> float: \"\"\"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1]. \"\"\" from logging import getLogger logger = getLogger(__name__) cfg = get_runtime_config() def _heuristic(t: str) -> float: if not t.strip(): logger.debug(\"figures_score: empty text -> 0.0\") return 0.0 L = max(1, len(t.split())) m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS) i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS) raw = (m + i) / L return max(0.0, min(1.0, 5.0 * raw)) def _ollama(t: str) -> float: try: # Lazy imports to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore import json import re as _re if not getattr(cfg, \"figures_ollama_model\", \"\").strip(): raise RuntimeError(\"No figures_ollama_model configured\") client = get_shared_client(host=getattr(cfg, \"figures_ollama_host\", \"http://localhost:11434\")) system = ( \"You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1].\" ) rubric = ( \"Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text \" \"on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations.\" ) prompt = \"Text:\\n\" + t.strip() + \"\\n\\n\" + rubric + \"\\nRespond as JSON: {\\\"score\\\": <float between 0 and 1>}\" raw = client.generate( model=getattr(cfg, \"figures_ollama_model\"), prompt=prompt, system=system, temperature=0.2, seed=getattr(cfg, \"seed\", None), ) score_val = None if raw: start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) score_val = obj.get(\"score\", None) if score_val is not None: score_val = float(score_val) except Exception: score_val = None if score_val is None and raw: m = _re.search(r\"([01](?:\\\\.\\\\d+)?)\", raw) if m: try: score_val = float(m.group(1)) except Exception: score_val = None if score_val is None: raise ValueError(\"Could not parse 'score' from Ollama response\") return max(0.0, min(1.0, float(score_val))) except Exception as e: logger.warning(\"figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)\", e) raise # Try Ollama if configured; otherwise heuristic base_score: float if getattr(cfg, \"figures_ollama_model\", \"\").strip(): try: base_score = _ollama(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) except Exception: logger.warning(\"figures_score: Ollama scoring failed; falling back to heuristic\") pass # fall through to heuristic base_score = _heuristic(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) intertextuality \u00b6 domain_coverage_score(text) \u00b6 Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. Source code in semiconj/metrics/intertextuality.py def domain_coverage_score(text: str) -> float: \"\"\"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. \"\"\" words = set(w.lower() for w in re.findall(r\"[\\w']+\", text)) hits = 0 for ws in DOMAINS.values(): if words & ws: hits += 1 return hits / max(1, len(DOMAINS)) ner_coverage(text) \u00b6 Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. Source code in semiconj/metrics/intertextuality.py def ner_coverage(text: str) -> float: \"\"\"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) ents = res.get(\"entities\", []) toks = res.get(\"tokens\", []) total = len(toks) if isinstance(toks, list) and toks else None if isinstance(ents, list) and ents: if not total: # If tokenizer not provided or empty, estimate using regex length total = len(re.findall(r\"[\\w']+\", text)) total = max(1, int(total or 0)) cov = len(ents) / total return max(0.0, min(1.0, cov)) except Exception: pass except Exception: pass tokens = re.findall(r\"[\\w']+\", text) if not tokens: return 0.0 ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {\"i\"}] return min(1.0, len(ent_like) / max(1, len(tokens)))","title":"Metrics (pkg)"},{"location":"api/metrics/#metrics-package","text":"","title":"Metrics Package"},{"location":"api/metrics/#semiconj.metrics.codeswitch_index","text":"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. Source code in semiconj/metrics/codeswitch.py def codeswitch_index(text: str) -> float: \"\"\"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. \"\"\" tokens = [t.lower() for t in re.findall(r\"[\\w']+\", text)] if not tokens: return 0.0 counts = {lang: 0 for lang in STOPWORDS} for t in tokens: for lang, sw in STOPWORDS.items(): if t in sw: counts[lang] += 1 dominant = max(counts, key=counts.get) dom_words = STOPWORDS[dominant] non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant)) return min(1.0, non_dom / max(1, len(tokens)))","title":"codeswitch_index"},{"location":"api/metrics/#semiconj.metrics.domain_coverage_score","text":"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. Source code in semiconj/metrics/intertextuality.py def domain_coverage_score(text: str) -> float: \"\"\"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. \"\"\" words = set(w.lower() for w in re.findall(r\"[\\w']+\", text)) hits = 0 for ws in DOMAINS.values(): if words & ws: hits += 1 return hits / max(1, len(DOMAINS))","title":"domain_coverage_score"},{"location":"api/metrics/#semiconj.metrics.figures_score","text":"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply cfg.figures_multiplier and clip to [0,1]. Source code in semiconj/metrics/figures.py def figures_score(text: str) -> float: \"\"\"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1]. \"\"\" from logging import getLogger logger = getLogger(__name__) cfg = get_runtime_config() def _heuristic(t: str) -> float: if not t.strip(): logger.debug(\"figures_score: empty text -> 0.0\") return 0.0 L = max(1, len(t.split())) m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS) i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS) raw = (m + i) / L return max(0.0, min(1.0, 5.0 * raw)) def _ollama(t: str) -> float: try: # Lazy imports to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore import json import re as _re if not getattr(cfg, \"figures_ollama_model\", \"\").strip(): raise RuntimeError(\"No figures_ollama_model configured\") client = get_shared_client(host=getattr(cfg, \"figures_ollama_host\", \"http://localhost:11434\")) system = ( \"You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1].\" ) rubric = ( \"Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text \" \"on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations.\" ) prompt = \"Text:\\n\" + t.strip() + \"\\n\\n\" + rubric + \"\\nRespond as JSON: {\\\"score\\\": <float between 0 and 1>}\" raw = client.generate( model=getattr(cfg, \"figures_ollama_model\"), prompt=prompt, system=system, temperature=0.2, seed=getattr(cfg, \"seed\", None), ) score_val = None if raw: start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) score_val = obj.get(\"score\", None) if score_val is not None: score_val = float(score_val) except Exception: score_val = None if score_val is None and raw: m = _re.search(r\"([01](?:\\\\.\\\\d+)?)\", raw) if m: try: score_val = float(m.group(1)) except Exception: score_val = None if score_val is None: raise ValueError(\"Could not parse 'score' from Ollama response\") return max(0.0, min(1.0, float(score_val))) except Exception as e: logger.warning(\"figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)\", e) raise # Try Ollama if configured; otherwise heuristic base_score: float if getattr(cfg, \"figures_ollama_model\", \"\").strip(): try: base_score = _ollama(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) except Exception: logger.warning(\"figures_score: Ollama scoring failed; falling back to heuristic\") pass # fall through to heuristic base_score = _heuristic(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))","title":"figures_score"},{"location":"api/metrics/#semiconj.metrics.mtld","text":"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. Source code in semiconj/metrics/complexity.py def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -> float: \"\"\"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. \"\"\" if not tokens: return 0.0 types = set() factor_count = 0 token_count = 0 ttr = 1.0 for tok in tokens: token_count += 1 types.add(tok) ttr = len(types) / token_count if ttr <= ttr_threshold and token_count >= min_segment: factor_count += 1 types.clear() token_count = 0 if token_count > 0: partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold) factor_count += partial return (len(tokens) / max(1e-9, factor_count))","title":"mtld"},{"location":"api/metrics/#semiconj.metrics.ner_coverage","text":"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. Source code in semiconj/metrics/intertextuality.py def ner_coverage(text: str) -> float: \"\"\"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) ents = res.get(\"entities\", []) toks = res.get(\"tokens\", []) total = len(toks) if isinstance(toks, list) and toks else None if isinstance(ents, list) and ents: if not total: # If tokenizer not provided or empty, estimate using regex length total = len(re.findall(r\"[\\w']+\", text)) total = max(1, int(total or 0)) cov = len(ents) / total return max(0.0, min(1.0, cov)) except Exception: pass except Exception: pass tokens = re.findall(r\"[\\w']+\", text) if not tokens: return 0.0 ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {\"i\"}] return min(1.0, len(ent_like) / max(1, len(tokens)))","title":"ner_coverage"},{"location":"api/metrics/#semiconj.metrics.ngram_entropy","text":"Compute normalized entropy over token n-grams. Parameters: tokens ( List [ str ] ) \u2013 List of tokens. n ( int , default: 1 ) \u2013 N-gram size (default 1 for unigrams). Returns: float \u2013 Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 Source code in semiconj/metrics/entropy.py def ngram_entropy(tokens: List[str], n: int = 1) -> float: \"\"\"Compute normalized entropy over token n-grams. Args: tokens: List of tokens. n: N-gram size (default 1 for unigrams). Returns: Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 \"\"\" if len(tokens) < n: return 0.0 return entropy_from_counts(Counter(ngrams(tokens, n)))","title":"ngram_entropy"},{"location":"api/metrics/#semiconj.metrics.pos_entropy","text":"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. Source code in semiconj/metrics/complexity.py def pos_entropy(tokens: List[str]) -> float: \"\"\"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. \"\"\" tags = pos_tags(tokens) total = len(tags) or 1 counts = Counter(tags) H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(_UPOS_TAGS)), 2) return max(0.0, min(1.0, H / (max_H + 1e-9)))","title":"pos_entropy"},{"location":"api/metrics/#semiconj.metrics.senses_per_lemma","text":"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. Source code in semiconj/metrics/complexity.py def senses_per_lemma(tokens: List[str]) -> float: \"\"\"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. \"\"\" try: from nltk.corpus import wordnet as wn # type: ignore except Exception as e: raise ImportError(\"NLTK is required for senses_per_lemma. Install nltk and wordnet data.\") from e lemmas = set(tokens) if not lemmas: return 0.0 total = 0 try: for w in lemmas: total += len(wn.synsets(w)) except Exception as e: # Typically LookupError when wordnet data is missing raise LookupError(\"NLTK WordNet data not found. Run: python -m nltk.downloader wordnet\") from e return total / max(1, len(lemmas))","title":"senses_per_lemma"},{"location":"api/metrics/#semiconj.metrics.sentence_embedding_dispersion","text":"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). Source code in semiconj/metrics/embeddings.py def sentence_embedding_dispersion(sentences: List[List[str]]) -> float: \"\"\"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). \"\"\" if not sentences: logger.debug(\"sentence_embedding_dispersion: empty sentences -> 0.0\") return 0.0 cfg = get_runtime_config() vecs: List[List[float]] use_ollama_model = getattr(cfg, \"embeddings_ollama_model\", \"\").strip() if use_ollama_model: try: # Any empty token list would complicate dimension handling; fallback in that case if any(len(s) == 0 for s in sentences): raise ValueError(\"empty sentence token list\") # Lazy import to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"embeddings_ollama_host\", \"http://localhost:11434\")) vecs = [client.embed(use_ollama_model, \" \".join(s)) for s in sentences] except Exception as e: logger.warning( \"sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)\", e, ) vecs = [_avg_vec(s) for s in sentences] else: vecs = [_avg_vec(s) for s in sentences] n = len(vecs) if n < 2: logger.debug(\"sentence_embedding_dispersion: <2 sentences -> 0.0\") return 0.0 dsum = 0.0 cnt = 0 for i in range(n): for j in range(i + 1, n): sim = _cosine(vecs[i], vecs[j]) dsum += (1.0 - sim) # distance cnt += 1 return dsum / max(1, cnt)","title":"sentence_embedding_dispersion"},{"location":"api/metrics/#semiconj.metrics.yules_k","text":"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. Source code in semiconj/metrics/complexity.py def yules_k(tokens: List[str]) -> float: \"\"\"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. \"\"\" if not tokens: return 0.0 freqs = Counter(tokens) N = sum(freqs.values()) M2 = sum(n * n * v for n, v in Counter(freqs.values()).items()) K = 1e4 * (M2 - N) / (N * N) # invert to larger-better, normalized roughly to [0,1] return 1.0 / (1.0 + K)","title":"yules_k"},{"location":"api/metrics/#semiconj.metrics.codeswitch","text":"","title":"codeswitch"},{"location":"api/metrics/#semiconj.metrics.codeswitch.codeswitch_index","text":"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. Source code in semiconj/metrics/codeswitch.py def codeswitch_index(text: str) -> float: \"\"\"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. \"\"\" tokens = [t.lower() for t in re.findall(r\"[\\w']+\", text)] if not tokens: return 0.0 counts = {lang: 0 for lang in STOPWORDS} for t in tokens: for lang, sw in STOPWORDS.items(): if t in sw: counts[lang] += 1 dominant = max(counts, key=counts.get) dom_words = STOPWORDS[dominant] non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant)) return min(1.0, non_dom / max(1, len(tokens)))","title":"codeswitch_index"},{"location":"api/metrics/#semiconj.metrics.complexity","text":"","title":"complexity"},{"location":"api/metrics/#semiconj.metrics.complexity.mtld","text":"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. Source code in semiconj/metrics/complexity.py def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -> float: \"\"\"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. \"\"\" if not tokens: return 0.0 types = set() factor_count = 0 token_count = 0 ttr = 1.0 for tok in tokens: token_count += 1 types.add(tok) ttr = len(types) / token_count if ttr <= ttr_threshold and token_count >= min_segment: factor_count += 1 types.clear() token_count = 0 if token_count > 0: partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold) factor_count += partial return (len(tokens) / max(1e-9, factor_count))","title":"mtld"},{"location":"api/metrics/#semiconj.metrics.complexity.naive_pos_tags","text":"Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. Source code in semiconj/metrics/complexity.py def naive_pos_tags(tokens: List[str]) -> List[str]: \"\"\"Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. \"\"\" tags = [] for w in tokens: if w.endswith('ly'): tags.append('RB') elif w.endswith('ing') or w.endswith('ed'): tags.append('VB') elif w[0:1].isupper(): tags.append('NNP') elif w in {\"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\"}: tags.append('VB') elif w in {\"the\", \"a\", \"an\"}: tags.append('DT') elif w in {\"and\", \"or\", \"but\", \"so\", \"because\"}: tags.append('CC') else: tags.append('NN') return tags","title":"naive_pos_tags"},{"location":"api/metrics/#semiconj.metrics.complexity.pos_entropy","text":"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. Source code in semiconj/metrics/complexity.py def pos_entropy(tokens: List[str]) -> float: \"\"\"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. \"\"\" tags = pos_tags(tokens) total = len(tags) or 1 counts = Counter(tags) H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(_UPOS_TAGS)), 2) return max(0.0, min(1.0, H / (max_H + 1e-9)))","title":"pos_entropy"},{"location":"api/metrics/#semiconj.metrics.complexity.pos_tags","text":"Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. Source code in semiconj/metrics/complexity.py def pos_tags(tokens: List[str]) -> List[str]: \"\"\"Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. \"\"\" from ..config import get_runtime_config cfg = get_runtime_config() # Try Ollama-based POS tagging first if configured model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model and tokens: try: from ..surrogates.ollama_client import OllamaClient # type: ignore import json allowed = sorted(list(_UPOS_TAGS)) client = OllamaClient(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) # Build a strict prompt to tag exactly the provided tokens system = ( \"You are a POS tagger. Respond in strict JSON only with a single key 'pos' \" \"containing Universal POS tags (UPOS) for each input token in order.\" ) # Use json.dumps to pass the exact token list to avoid tokenization drift tokens_json = json.dumps(tokens) rubric = ( \"Tag each token with one of the UPOS tags: \" + \", \".join(allowed) + \". \" + \"Return JSON exactly as: {\\\"pos\\\": [\\\"TAG1\\\", \\\"TAG2\\\", \\\"...\\\"]} with length equal to the number of input tokens.\" ) prompt = ( \"Tokens:\\n\" + tokens_json + \"\\n\\n\" + rubric + \"\\n\" \"Do not include explanations or additional keys.\" ) raw = client.generate(model=model, prompt=prompt, system=system, temperature=0.0, seed=getattr(cfg, \"seed\", None)) start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) pos_list = obj.get(\"pos\", []) if isinstance(pos_list, list) and len(pos_list) == len(tokens): tags: List[str] = [] for t in pos_list: tag = str(t).upper() tags.append(tag if tag in _UPOS_TAGS else \"X\") return tags except Exception: pass except Exception: # Fall through to spaCy/naive logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to naive POS tagger.') pass # Fallbacks if cfg.pos_tagger == \"spacy\": return _maybe_spacy_pos(tokens) return naive_pos_tags(tokens)","title":"pos_tags"},{"location":"api/metrics/#semiconj.metrics.complexity.senses_per_lemma","text":"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. Source code in semiconj/metrics/complexity.py def senses_per_lemma(tokens: List[str]) -> float: \"\"\"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. \"\"\" try: from nltk.corpus import wordnet as wn # type: ignore except Exception as e: raise ImportError(\"NLTK is required for senses_per_lemma. Install nltk and wordnet data.\") from e lemmas = set(tokens) if not lemmas: return 0.0 total = 0 try: for w in lemmas: total += len(wn.synsets(w)) except Exception as e: # Typically LookupError when wordnet data is missing raise LookupError(\"NLTK WordNet data not found. Run: python -m nltk.downloader wordnet\") from e return total / max(1, len(lemmas))","title":"senses_per_lemma"},{"location":"api/metrics/#semiconj.metrics.complexity.tokenize","text":"Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. Source code in semiconj/metrics/complexity.py def tokenize(text: str) -> List[str]: \"\"\"Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) toks = res.get(\"tokens\", []) if isinstance(toks, list) and toks: # Normalize to lower-case to preserve previous behavior return [str(t).lower() for t in toks if isinstance(t, str)] except Exception: # fall back to regex below logging.getLogger(__name__).warning( f\"Ollama NLP model '{model}' not found. Falling back to regex-based tokenization.\" ) pass except Exception: # If config import fails for any reason, use regex pass return _WORD_RE.findall(text.lower())","title":"tokenize"},{"location":"api/metrics/#semiconj.metrics.complexity.yules_k","text":"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. Source code in semiconj/metrics/complexity.py def yules_k(tokens: List[str]) -> float: \"\"\"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. \"\"\" if not tokens: return 0.0 freqs = Counter(tokens) N = sum(freqs.values()) M2 = sum(n * n * v for n, v in Counter(freqs.values()).items()) K = 1e4 * (M2 - N) / (N * N) # invert to larger-better, normalized roughly to [0,1] return 1.0 / (1.0 + K)","title":"yules_k"},{"location":"api/metrics/#semiconj.metrics.embeddings","text":"","title":"embeddings"},{"location":"api/metrics/#semiconj.metrics.embeddings.sentence_embedding_dispersion","text":"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). Source code in semiconj/metrics/embeddings.py def sentence_embedding_dispersion(sentences: List[List[str]]) -> float: \"\"\"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). \"\"\" if not sentences: logger.debug(\"sentence_embedding_dispersion: empty sentences -> 0.0\") return 0.0 cfg = get_runtime_config() vecs: List[List[float]] use_ollama_model = getattr(cfg, \"embeddings_ollama_model\", \"\").strip() if use_ollama_model: try: # Any empty token list would complicate dimension handling; fallback in that case if any(len(s) == 0 for s in sentences): raise ValueError(\"empty sentence token list\") # Lazy import to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"embeddings_ollama_host\", \"http://localhost:11434\")) vecs = [client.embed(use_ollama_model, \" \".join(s)) for s in sentences] except Exception as e: logger.warning( \"sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)\", e, ) vecs = [_avg_vec(s) for s in sentences] else: vecs = [_avg_vec(s) for s in sentences] n = len(vecs) if n < 2: logger.debug(\"sentence_embedding_dispersion: <2 sentences -> 0.0\") return 0.0 dsum = 0.0 cnt = 0 for i in range(n): for j in range(i + 1, n): sim = _cosine(vecs[i], vecs[j]) dsum += (1.0 - sim) # distance cnt += 1 return dsum / max(1, cnt)","title":"sentence_embedding_dispersion"},{"location":"api/metrics/#semiconj.metrics.entropy","text":"","title":"entropy"},{"location":"api/metrics/#semiconj.metrics.entropy.entropy_from_counts","text":"Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Parameters: counts ( Counter ) \u2013 Counter mapping events to their frequencies. Returns: float \u2013 Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 Source code in semiconj/metrics/entropy.py def entropy_from_counts(counts: Counter) -> float: \"\"\"Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Args: counts: Counter mapping events to their frequencies. Returns: Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 \"\"\" total = sum(counts.values()) or 1 H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(counts)), 2) if max_H == 0: return 0.0 return H / max_H","title":"entropy_from_counts"},{"location":"api/metrics/#semiconj.metrics.entropy.ngram_entropy","text":"Compute normalized entropy over token n-grams. Parameters: tokens ( List [ str ] ) \u2013 List of tokens. n ( int , default: 1 ) \u2013 N-gram size (default 1 for unigrams). Returns: float \u2013 Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 Source code in semiconj/metrics/entropy.py def ngram_entropy(tokens: List[str], n: int = 1) -> float: \"\"\"Compute normalized entropy over token n-grams. Args: tokens: List of tokens. n: N-gram size (default 1 for unigrams). Returns: Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 \"\"\" if len(tokens) < n: return 0.0 return entropy_from_counts(Counter(ngrams(tokens, n)))","title":"ngram_entropy"},{"location":"api/metrics/#semiconj.metrics.entropy.ngrams","text":"Yield consecutive n-grams from a list of tokens. Parameters: tokens ( List [ str ] ) \u2013 Sequence of token strings. n ( int ) \u2013 Size of the n-gram (n >= 1). Yields: Iterable [ Tuple [ str , ...]] \u2013 Tuples of length n representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] Source code in semiconj/metrics/entropy.py def ngrams(tokens: List[str], n: int) -> Iterable[Tuple[str, ...]]: \"\"\"Yield consecutive n-grams from a list of tokens. Args: tokens: Sequence of token strings. n: Size of the n-gram (n >= 1). Yields: Tuples of length ``n`` representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] \"\"\" for i in range(len(tokens) - n + 1): yield tuple(tokens[i:i+n])","title":"ngrams"},{"location":"api/metrics/#semiconj.metrics.figures","text":"","title":"figures"},{"location":"api/metrics/#semiconj.metrics.figures.figures_score","text":"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply cfg.figures_multiplier and clip to [0,1]. Source code in semiconj/metrics/figures.py def figures_score(text: str) -> float: \"\"\"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1]. \"\"\" from logging import getLogger logger = getLogger(__name__) cfg = get_runtime_config() def _heuristic(t: str) -> float: if not t.strip(): logger.debug(\"figures_score: empty text -> 0.0\") return 0.0 L = max(1, len(t.split())) m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS) i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS) raw = (m + i) / L return max(0.0, min(1.0, 5.0 * raw)) def _ollama(t: str) -> float: try: # Lazy imports to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore import json import re as _re if not getattr(cfg, \"figures_ollama_model\", \"\").strip(): raise RuntimeError(\"No figures_ollama_model configured\") client = get_shared_client(host=getattr(cfg, \"figures_ollama_host\", \"http://localhost:11434\")) system = ( \"You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1].\" ) rubric = ( \"Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text \" \"on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations.\" ) prompt = \"Text:\\n\" + t.strip() + \"\\n\\n\" + rubric + \"\\nRespond as JSON: {\\\"score\\\": <float between 0 and 1>}\" raw = client.generate( model=getattr(cfg, \"figures_ollama_model\"), prompt=prompt, system=system, temperature=0.2, seed=getattr(cfg, \"seed\", None), ) score_val = None if raw: start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) score_val = obj.get(\"score\", None) if score_val is not None: score_val = float(score_val) except Exception: score_val = None if score_val is None and raw: m = _re.search(r\"([01](?:\\\\.\\\\d+)?)\", raw) if m: try: score_val = float(m.group(1)) except Exception: score_val = None if score_val is None: raise ValueError(\"Could not parse 'score' from Ollama response\") return max(0.0, min(1.0, float(score_val))) except Exception as e: logger.warning(\"figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)\", e) raise # Try Ollama if configured; otherwise heuristic base_score: float if getattr(cfg, \"figures_ollama_model\", \"\").strip(): try: base_score = _ollama(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) except Exception: logger.warning(\"figures_score: Ollama scoring failed; falling back to heuristic\") pass # fall through to heuristic base_score = _heuristic(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))","title":"figures_score"},{"location":"api/metrics/#semiconj.metrics.intertextuality","text":"","title":"intertextuality"},{"location":"api/metrics/#semiconj.metrics.intertextuality.domain_coverage_score","text":"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. Source code in semiconj/metrics/intertextuality.py def domain_coverage_score(text: str) -> float: \"\"\"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. \"\"\" words = set(w.lower() for w in re.findall(r\"[\\w']+\", text)) hits = 0 for ws in DOMAINS.values(): if words & ws: hits += 1 return hits / max(1, len(DOMAINS))","title":"domain_coverage_score"},{"location":"api/metrics/#semiconj.metrics.intertextuality.ner_coverage","text":"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. Source code in semiconj/metrics/intertextuality.py def ner_coverage(text: str) -> float: \"\"\"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) ents = res.get(\"entities\", []) toks = res.get(\"tokens\", []) total = len(toks) if isinstance(toks, list) and toks else None if isinstance(ents, list) and ents: if not total: # If tokenizer not provided or empty, estimate using regex length total = len(re.findall(r\"[\\w']+\", text)) total = max(1, int(total or 0)) cov = len(ents) / total return max(0.0, min(1.0, cov)) except Exception: pass except Exception: pass tokens = re.findall(r\"[\\w']+\", text) if not tokens: return 0.0 ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {\"i\"}] return min(1.0, len(ent_like) / max(1, len(tokens)))","title":"ner_coverage"},{"location":"api/metrics/#submodules","text":"","title":"Submodules"},{"location":"api/metrics/#codeswitch","text":"","title":"codeswitch"},{"location":"api/metrics/#semiconj.metrics.codeswitch.codeswitch_index","text":"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. Source code in semiconj/metrics/codeswitch.py def codeswitch_index(text: str) -> float: \"\"\"Fraction of tokens that appear to be from a non-dominant language. Heuristic using stopword overlaps across a few languages. \"\"\" tokens = [t.lower() for t in re.findall(r\"[\\w']+\", text)] if not tokens: return 0.0 counts = {lang: 0 for lang in STOPWORDS} for t in tokens: for lang, sw in STOPWORDS.items(): if t in sw: counts[lang] += 1 dominant = max(counts, key=counts.get) dom_words = STOPWORDS[dominant] non_dom = sum(1 for t in tokens if any((t in STOPWORDS[l]) for l in STOPWORDS if l != dominant)) return min(1.0, non_dom / max(1, len(tokens)))","title":"codeswitch_index"},{"location":"api/metrics/#complexity","text":"","title":"complexity"},{"location":"api/metrics/#semiconj.metrics.complexity.mtld","text":"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. Source code in semiconj/metrics/complexity.py def mtld(tokens: List[str], ttr_threshold: float = 0.72, min_segment: int = 10) -> float: \"\"\"Approximate MTLD (Measure of Textual Lexical Diversity). Reference: McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Returns a positive value, roughly stable across lengths; higher implies more diversity. Note: This function returns a raw MTLD-like score, not normalized to [0,1]. \"\"\" if not tokens: return 0.0 types = set() factor_count = 0 token_count = 0 ttr = 1.0 for tok in tokens: token_count += 1 types.add(tok) ttr = len(types) / token_count if ttr <= ttr_threshold and token_count >= min_segment: factor_count += 1 types.clear() token_count = 0 if token_count > 0: partial = (1 - (len(types) / max(1, token_count))) / max(1e-9, 1 - ttr_threshold) factor_count += partial return (len(tokens) / max(1e-9, factor_count))","title":"mtld"},{"location":"api/metrics/#semiconj.metrics.complexity.naive_pos_tags","text":"Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. Source code in semiconj/metrics/complexity.py def naive_pos_tags(tokens: List[str]) -> List[str]: \"\"\"Very rough POS tags if no tagger available. This is a heuristic; replace with spaCy/UD tagger if available. \"\"\" tags = [] for w in tokens: if w.endswith('ly'): tags.append('RB') elif w.endswith('ing') or w.endswith('ed'): tags.append('VB') elif w[0:1].isupper(): tags.append('NNP') elif w in {\"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\"}: tags.append('VB') elif w in {\"the\", \"a\", \"an\"}: tags.append('DT') elif w in {\"and\", \"or\", \"but\", \"so\", \"because\"}: tags.append('CC') else: tags.append('NN') return tags","title":"naive_pos_tags"},{"location":"api/metrics/#semiconj.metrics.complexity.pos_entropy","text":"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. Source code in semiconj/metrics/complexity.py def pos_entropy(tokens: List[str]) -> float: \"\"\"Shannon entropy of POS tag distribution normalized to [0,1]. Normalization uses log2(|UPOS|) as the denominator to make scores comparable across texts, regardless of how many tag types are observed in a short sample. \"\"\" tags = pos_tags(tokens) total = len(tags) or 1 counts = Counter(tags) H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(_UPOS_TAGS)), 2) return max(0.0, min(1.0, H / (max_H + 1e-9)))","title":"pos_entropy"},{"location":"api/metrics/#semiconj.metrics.complexity.pos_tags","text":"Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. Source code in semiconj/metrics/complexity.py def pos_tags(tokens: List[str]) -> List[str]: \"\"\"Return POS tags for the provided tokens. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), request UPOS tags via the Ollama API. The model is prompted to return strict JSON {\"pos\": [UPOS...]} aligned with the given tokens. - Otherwise, if cfg.pos_tagger == 'spacy', use spaCy (requires en_core_web_sm). - Otherwise, fall back to the naive heuristic tagger. \"\"\" from ..config import get_runtime_config cfg = get_runtime_config() # Try Ollama-based POS tagging first if configured model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model and tokens: try: from ..surrogates.ollama_client import OllamaClient # type: ignore import json allowed = sorted(list(_UPOS_TAGS)) client = OllamaClient(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) # Build a strict prompt to tag exactly the provided tokens system = ( \"You are a POS tagger. Respond in strict JSON only with a single key 'pos' \" \"containing Universal POS tags (UPOS) for each input token in order.\" ) # Use json.dumps to pass the exact token list to avoid tokenization drift tokens_json = json.dumps(tokens) rubric = ( \"Tag each token with one of the UPOS tags: \" + \", \".join(allowed) + \". \" + \"Return JSON exactly as: {\\\"pos\\\": [\\\"TAG1\\\", \\\"TAG2\\\", \\\"...\\\"]} with length equal to the number of input tokens.\" ) prompt = ( \"Tokens:\\n\" + tokens_json + \"\\n\\n\" + rubric + \"\\n\" \"Do not include explanations or additional keys.\" ) raw = client.generate(model=model, prompt=prompt, system=system, temperature=0.0, seed=getattr(cfg, \"seed\", None)) start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) pos_list = obj.get(\"pos\", []) if isinstance(pos_list, list) and len(pos_list) == len(tokens): tags: List[str] = [] for t in pos_list: tag = str(t).upper() tags.append(tag if tag in _UPOS_TAGS else \"X\") return tags except Exception: pass except Exception: # Fall through to spaCy/naive logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to naive POS tagger.') pass # Fallbacks if cfg.pos_tagger == \"spacy\": return _maybe_spacy_pos(tokens) return naive_pos_tags(tokens)","title":"pos_tags"},{"location":"api/metrics/#semiconj.metrics.complexity.senses_per_lemma","text":"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. Source code in semiconj/metrics/complexity.py def senses_per_lemma(tokens: List[str]) -> float: \"\"\"Mean WordNet senses per lemma (raw; not normalized). Strict behavior: requires NLTK WordNet corpus to be installed and available. Raises ImportError/LookupError if NLTK or its WordNet data is missing. \"\"\" try: from nltk.corpus import wordnet as wn # type: ignore except Exception as e: raise ImportError(\"NLTK is required for senses_per_lemma. Install nltk and wordnet data.\") from e lemmas = set(tokens) if not lemmas: return 0.0 total = 0 try: for w in lemmas: total += len(wn.synsets(w)) except Exception as e: # Typically LookupError when wordnet data is missing raise LookupError(\"NLTK WordNet data not found. Run: python -m nltk.downloader wordnet\") from e return total / max(1, len(lemmas))","title":"senses_per_lemma"},{"location":"api/metrics/#semiconj.metrics.complexity.tokenize","text":"Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. Source code in semiconj/metrics/complexity.py def tokenize(text: str) -> List[str]: \"\"\"Tokenize text. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use it to obtain tokens. - Otherwise, fall back to a simple regex-based tokenizer. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) toks = res.get(\"tokens\", []) if isinstance(toks, list) and toks: # Normalize to lower-case to preserve previous behavior return [str(t).lower() for t in toks if isinstance(t, str)] except Exception: # fall back to regex below logging.getLogger(__name__).warning( f\"Ollama NLP model '{model}' not found. Falling back to regex-based tokenization.\" ) pass except Exception: # If config import fails for any reason, use regex pass return _WORD_RE.findall(text.lower())","title":"tokenize"},{"location":"api/metrics/#semiconj.metrics.complexity.yules_k","text":"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. Source code in semiconj/metrics/complexity.py def yules_k(tokens: List[str]) -> float: \"\"\"Yule's K lexical diversity measure. Reference: Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Lower K implies higher lexical diversity. We return 1/(1+K) to map to (0,1], keeping a larger-is-better scale and bounding at 1 when K\u21920. \"\"\" if not tokens: return 0.0 freqs = Counter(tokens) N = sum(freqs.values()) M2 = sum(n * n * v for n, v in Counter(freqs.values()).items()) K = 1e4 * (M2 - N) / (N * N) # invert to larger-better, normalized roughly to [0,1] return 1.0 / (1.0 + K)","title":"yules_k"},{"location":"api/metrics/#embeddings","text":"","title":"embeddings"},{"location":"api/metrics/#semiconj.metrics.embeddings.sentence_embedding_dispersion","text":"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). Source code in semiconj/metrics/embeddings.py def sentence_embedding_dispersion(sentences: List[List[str]]) -> float: \"\"\"Compute mean pairwise cosine distance among sentence embeddings. Behavior: - If an Ollama embedding model is configured (e.g., 'nomic-embed-text'), use it to embed each sentence string. - Otherwise, or on any failure, fall back to hash-averaged word vectors for offline robustness. - Returns a value in [0,1]; higher means more dispersion (semantic variety). \"\"\" if not sentences: logger.debug(\"sentence_embedding_dispersion: empty sentences -> 0.0\") return 0.0 cfg = get_runtime_config() vecs: List[List[float]] use_ollama_model = getattr(cfg, \"embeddings_ollama_model\", \"\").strip() if use_ollama_model: try: # Any empty token list would complicate dimension handling; fallback in that case if any(len(s) == 0 for s in sentences): raise ValueError(\"empty sentence token list\") # Lazy import to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"embeddings_ollama_host\", \"http://localhost:11434\")) vecs = [client.embed(use_ollama_model, \" \".join(s)) for s in sentences] except Exception as e: logger.warning( \"sentence_embedding_dispersion: Ollama embeddings failed or unavailable; falling back to hash-avg (%s)\", e, ) vecs = [_avg_vec(s) for s in sentences] else: vecs = [_avg_vec(s) for s in sentences] n = len(vecs) if n < 2: logger.debug(\"sentence_embedding_dispersion: <2 sentences -> 0.0\") return 0.0 dsum = 0.0 cnt = 0 for i in range(n): for j in range(i + 1, n): sim = _cosine(vecs[i], vecs[j]) dsum += (1.0 - sim) # distance cnt += 1 return dsum / max(1, cnt)","title":"sentence_embedding_dispersion"},{"location":"api/metrics/#entropy","text":"","title":"entropy"},{"location":"api/metrics/#semiconj.metrics.entropy.entropy_from_counts","text":"Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Parameters: counts ( Counter ) \u2013 Counter mapping events to their frequencies. Returns: float \u2013 Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 Source code in semiconj/metrics/entropy.py def entropy_from_counts(counts: Counter) -> float: \"\"\"Compute normalized Shannon entropy from a Counter of event counts. The entropy is computed in bits and normalized by log2(K), where K is the number of unique events, yielding a value in [0,1]. Args: counts: Counter mapping events to their frequencies. Returns: Normalized entropy in [0, 1]. Returns 0.0 for degenerate distributions. Examples: >>> from collections import Counter >>> round(entropy_from_counts(Counter({'a': 1, 'b': 1})), 6) 1.0 >>> round(entropy_from_counts(Counter({'a': 2, 'b': 0})), 6) 0.0 \"\"\" total = sum(counts.values()) or 1 H = 0.0 for c in counts.values(): p = c / total H -= p * math.log(p + 1e-12, 2) max_H = math.log(max(1, len(counts)), 2) if max_H == 0: return 0.0 return H / max_H","title":"entropy_from_counts"},{"location":"api/metrics/#semiconj.metrics.entropy.ngram_entropy","text":"Compute normalized entropy over token n-grams. Parameters: tokens ( List [ str ] ) \u2013 List of tokens. n ( int , default: 1 ) \u2013 N-gram size (default 1 for unigrams). Returns: float \u2013 Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 Source code in semiconj/metrics/entropy.py def ngram_entropy(tokens: List[str], n: int = 1) -> float: \"\"\"Compute normalized entropy over token n-grams. Args: tokens: List of tokens. n: N-gram size (default 1 for unigrams). Returns: Normalized Shannon entropy of the n-gram distribution in [0,1]. Examples: >>> round(ngram_entropy([\"a\", \"b\", \"a\", \"b\"], n=1), 6) 1.0 >>> round(ngram_entropy([\"a\", \"a\", \"a\"], n=1), 6) 0.0 \"\"\" if len(tokens) < n: return 0.0 return entropy_from_counts(Counter(ngrams(tokens, n)))","title":"ngram_entropy"},{"location":"api/metrics/#semiconj.metrics.entropy.ngrams","text":"Yield consecutive n-grams from a list of tokens. Parameters: tokens ( List [ str ] ) \u2013 Sequence of token strings. n ( int ) \u2013 Size of the n-gram (n >= 1). Yields: Iterable [ Tuple [ str , ...]] \u2013 Tuples of length n representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] Source code in semiconj/metrics/entropy.py def ngrams(tokens: List[str], n: int) -> Iterable[Tuple[str, ...]]: \"\"\"Yield consecutive n-grams from a list of tokens. Args: tokens: Sequence of token strings. n: Size of the n-gram (n >= 1). Yields: Tuples of length ``n`` representing each n-gram. Examples: >>> list(ngrams([\"a\", \"b\", \"c\"], 2)) [('a', 'b'), ('b', 'c')] \"\"\" for i in range(len(tokens) - n + 1): yield tuple(tokens[i:i+n])","title":"ngrams"},{"location":"api/metrics/#figures","text":"","title":"figures"},{"location":"api/metrics/#semiconj.metrics.figures.figures_score","text":"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply cfg.figures_multiplier and clip to [0,1]. Source code in semiconj/metrics/figures.py def figures_score(text: str) -> float: \"\"\"Score figurative language intensity in [0,1]. Behavior: - If a figures Ollama model is configured via runtime config, use it to score the text. - Otherwise, fall back to the original heuristic (metaphor/irony cue density). - In all cases, apply ``cfg.figures_multiplier`` and clip to [0,1]. \"\"\" from logging import getLogger logger = getLogger(__name__) cfg = get_runtime_config() def _heuristic(t: str) -> float: if not t.strip(): logger.debug(\"figures_score: empty text -> 0.0\") return 0.0 L = max(1, len(t.split())) m = sum(len(re.findall(p, t.lower())) for p in _METAPHOR_HINTS) i = sum(len(re.findall(p, t.lower())) for p in _IRONY_HINTS) raw = (m + i) / L return max(0.0, min(1.0, 5.0 * raw)) def _ollama(t: str) -> float: try: # Lazy imports to avoid hard dependency when unused from ..surrogates.ollama_client import get_shared_client # type: ignore import json import re as _re if not getattr(cfg, \"figures_ollama_model\", \"\").strip(): raise RuntimeError(\"No figures_ollama_model configured\") client = get_shared_client(host=getattr(cfg, \"figures_ollama_host\", \"http://localhost:11434\")) system = ( \"You are an expert linguist. Respond in strict JSON only with a single key 'score' in [0,1].\" ) rubric = ( \"Rate the intensity of figurative language (metaphor, simile, irony, symbolism) in the provided text \" \"on a continuous scale from 0 to 1, where 0 means none and 1 means heavy. Do not include explanations.\" ) prompt = \"Text:\\n\" + t.strip() + \"\\n\\n\" + rubric + \"\\nRespond as JSON: {\\\"score\\\": <float between 0 and 1>}\" raw = client.generate( model=getattr(cfg, \"figures_ollama_model\"), prompt=prompt, system=system, temperature=0.2, seed=getattr(cfg, \"seed\", None), ) score_val = None if raw: start = raw.find('{') end = raw.rfind('}') if start != -1 and end != -1 and end > start: try: obj = json.loads(raw[start:end+1]) score_val = obj.get(\"score\", None) if score_val is not None: score_val = float(score_val) except Exception: score_val = None if score_val is None and raw: m = _re.search(r\"([01](?:\\\\.\\\\d+)?)\", raw) if m: try: score_val = float(m.group(1)) except Exception: score_val = None if score_val is None: raise ValueError(\"Could not parse 'score' from Ollama response\") return max(0.0, min(1.0, float(score_val))) except Exception as e: logger.warning(\"figures_score: Ollama scoring failed or unavailable; falling back to heuristic (%s)\", e) raise # Try Ollama if configured; otherwise heuristic base_score: float if getattr(cfg, \"figures_ollama_model\", \"\").strip(): try: base_score = _ollama(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score)) except Exception: logger.warning(\"figures_score: Ollama scoring failed; falling back to heuristic\") pass # fall through to heuristic base_score = _heuristic(text) return max(0.0, min(1.0, float(cfg.figures_multiplier) * base_score))","title":"figures_score"},{"location":"api/metrics/#intertextuality","text":"","title":"intertextuality"},{"location":"api/metrics/#semiconj.metrics.intertextuality.domain_coverage_score","text":"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. Source code in semiconj/metrics/intertextuality.py def domain_coverage_score(text: str) -> float: \"\"\"Score in [0,1] for how many domain keyword sets are hit. Multi-domain coverage increases intertextuality. \"\"\" words = set(w.lower() for w in re.findall(r\"[\\w']+\", text)) hits = 0 for ws in DOMAINS.values(): if words & ws: hits += 1 return hits / max(1, len(DOMAINS))","title":"domain_coverage_score"},{"location":"api/metrics/#semiconj.metrics.intertextuality.ner_coverage","text":"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. Source code in semiconj/metrics/intertextuality.py def ner_coverage(text: str) -> float: \"\"\"Named-entity coverage in [0,1]. Behavior: - If an Ollama NLP model is configured (e.g., 'gpt-oss'), use its extracted entities. - Otherwise, fall back to a capitalization-based heuristic proxy. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) ents = res.get(\"entities\", []) toks = res.get(\"tokens\", []) total = len(toks) if isinstance(toks, list) and toks else None if isinstance(ents, list) and ents: if not total: # If tokenizer not provided or empty, estimate using regex length total = len(re.findall(r\"[\\w']+\", text)) total = max(1, int(total or 0)) cov = len(ents) / total return max(0.0, min(1.0, cov)) except Exception: pass except Exception: pass tokens = re.findall(r\"[\\w']+\", text) if not tokens: return 0.0 ent_like = [t for t in tokens if t[0:1].isupper() and t.lower() not in {\"i\"}] return min(1.0, len(ent_like) / max(1, len(tokens)))","title":"ner_coverage"},{"location":"api/reporting/","text":"Reporting \u00b6 maybe_plot_frontier(path, frontier_points) \u00b6 Plot the estimated frontier points and save the figure. Parameters: path ( Path ) \u2013 Output PNG path. frontier_points ( List [ tuple ] ) \u2013 Sequence of (S_bin_center, k95) pairs as returned by semiconj.frontier.estimate_frontier . Raises: ImportError \u2013 If matplotlib is not installed. ValueError \u2013 If frontier_points is empty. Examples: >>> from pathlib import Path >>> pts = [(0.1, 0.05), (0.5, 0.2), (0.9, 0.3)] >>> maybe_plot_frontier(Path('frontier.png'), pts) # Saves a simple line plot to frontier.png Source code in semiconj/reporting.py def maybe_plot_frontier(path: Path, frontier_points: List[tuple]) -> None: \"\"\"Plot the estimated frontier points and save the figure. Args: path: Output PNG path. frontier_points: Sequence of (S_bin_center, k95) pairs as returned by ``semiconj.frontier.estimate_frontier``. Raises: ImportError: If matplotlib is not installed. ValueError: If ``frontier_points`` is empty. Examples: >>> from pathlib import Path >>> pts = [(0.1, 0.05), (0.5, 0.2), (0.9, 0.3)] >>> maybe_plot_frontier(Path('frontier.png'), pts) # doctest: +SKIP # Saves a simple line plot to frontier.png \"\"\" try: import matplotlib.pyplot as plt # type: ignore except Exception as e: raise ImportError(\"matplotlib is required to plot the frontier. Install matplotlib.\") from e if not frontier_points: raise ValueError(\"Frontier points are required to plot the frontier (no fallback plotting).\") xs = [s for s, _ in frontier_points] ys = [y for _, y in frontier_points] plt.figure(figsize=(5,3)) plt.plot(xs, ys, marker='o') plt.xlabel('S (binned)') plt.ylabel('95th percentile of S\u00b7D') path.parent.mkdir(parents=True, exist_ok=True) plt.tight_layout() plt.savefig(path) save_metrics(path, rows) \u00b6 Save a sequence of metric rows to CSV. This is a thin wrapper over semiconj.corpus.write_csv that preserves the union of keys across rows and formats floats to 6 decimals. Parameters: path ( Path ) \u2013 Output CSV path. rows ( Iterable [ Dict [ str , object ]] ) \u2013 Iterable of dictionaries representing records to write. Examples: >>> from pathlib import Path >>> data = [{'id': 'a1', 'S': 0.5}, {'id': 'a2', 'S': 0.7}] >>> save_metrics(Path('out.csv'), data) # File 'out.csv' will contain a header and two rows. Source code in semiconj/reporting.py def save_metrics(path: Path, rows: Iterable[Dict[str, object]]) -> None: \"\"\"Save a sequence of metric rows to CSV. This is a thin wrapper over ``semiconj.corpus.write_csv`` that preserves the union of keys across rows and formats floats to 6 decimals. Args: path: Output CSV path. rows: Iterable of dictionaries representing records to write. Examples: >>> from pathlib import Path >>> data = [{'id': 'a1', 'S': 0.5}, {'id': 'a2', 'S': 0.7}] >>> save_metrics(Path('out.csv'), data) # doctest: +SKIP # File 'out.csv' will contain a header and two rows. \"\"\" rows_list = list(rows) write_csv(path, rows_list) logger.info(\"Saved metrics to %s (%d rows)\", path, len(rows_list))","title":"Reporting"},{"location":"api/reporting/#reporting","text":"","title":"Reporting"},{"location":"api/reporting/#semiconj.reporting.maybe_plot_frontier","text":"Plot the estimated frontier points and save the figure. Parameters: path ( Path ) \u2013 Output PNG path. frontier_points ( List [ tuple ] ) \u2013 Sequence of (S_bin_center, k95) pairs as returned by semiconj.frontier.estimate_frontier . Raises: ImportError \u2013 If matplotlib is not installed. ValueError \u2013 If frontier_points is empty. Examples: >>> from pathlib import Path >>> pts = [(0.1, 0.05), (0.5, 0.2), (0.9, 0.3)] >>> maybe_plot_frontier(Path('frontier.png'), pts) # Saves a simple line plot to frontier.png Source code in semiconj/reporting.py def maybe_plot_frontier(path: Path, frontier_points: List[tuple]) -> None: \"\"\"Plot the estimated frontier points and save the figure. Args: path: Output PNG path. frontier_points: Sequence of (S_bin_center, k95) pairs as returned by ``semiconj.frontier.estimate_frontier``. Raises: ImportError: If matplotlib is not installed. ValueError: If ``frontier_points`` is empty. Examples: >>> from pathlib import Path >>> pts = [(0.1, 0.05), (0.5, 0.2), (0.9, 0.3)] >>> maybe_plot_frontier(Path('frontier.png'), pts) # doctest: +SKIP # Saves a simple line plot to frontier.png \"\"\" try: import matplotlib.pyplot as plt # type: ignore except Exception as e: raise ImportError(\"matplotlib is required to plot the frontier. Install matplotlib.\") from e if not frontier_points: raise ValueError(\"Frontier points are required to plot the frontier (no fallback plotting).\") xs = [s for s, _ in frontier_points] ys = [y for _, y in frontier_points] plt.figure(figsize=(5,3)) plt.plot(xs, ys, marker='o') plt.xlabel('S (binned)') plt.ylabel('95th percentile of S\u00b7D') path.parent.mkdir(parents=True, exist_ok=True) plt.tight_layout() plt.savefig(path)","title":"maybe_plot_frontier"},{"location":"api/reporting/#semiconj.reporting.save_metrics","text":"Save a sequence of metric rows to CSV. This is a thin wrapper over semiconj.corpus.write_csv that preserves the union of keys across rows and formats floats to 6 decimals. Parameters: path ( Path ) \u2013 Output CSV path. rows ( Iterable [ Dict [ str , object ]] ) \u2013 Iterable of dictionaries representing records to write. Examples: >>> from pathlib import Path >>> data = [{'id': 'a1', 'S': 0.5}, {'id': 'a2', 'S': 0.7}] >>> save_metrics(Path('out.csv'), data) # File 'out.csv' will contain a header and two rows. Source code in semiconj/reporting.py def save_metrics(path: Path, rows: Iterable[Dict[str, object]]) -> None: \"\"\"Save a sequence of metric rows to CSV. This is a thin wrapper over ``semiconj.corpus.write_csv`` that preserves the union of keys across rows and formats floats to 6 decimals. Args: path: Output CSV path. rows: Iterable of dictionaries representing records to write. Examples: >>> from pathlib import Path >>> data = [{'id': 'a1', 'S': 0.5}, {'id': 'a2', 'S': 0.7}] >>> save_metrics(Path('out.csv'), data) # doctest: +SKIP # File 'out.csv' will contain a header and two rows. \"\"\" rows_list = list(rows) write_csv(path, rows_list) logger.info(\"Saved metrics to %s (%d rows)\", path, len(rows_list))","title":"save_metrics"},{"location":"api/semiotic/","text":"Semiotic Package \u00b6 parser \u00b6 compute_d_intr(text) \u00b6 Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. Source code in semiconj/semiotic/parser.py def compute_d_intr(text: str) -> Dict[str, float]: \"\"\"Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. \"\"\" triads = extract_triads(text) sents = split_sentences(text) structural_precision = 0.0 if sents: structural_precision = sum(1 for s in sents if s.strip()) structural_precision = min(1.0, len(triads) / max(1, len(sents))) interps = [t.interpretant for t in triads] # convergence: mean pairwise similarity among interpretants if len(interps) < 2: convergence = 0.0 else: vecs = interpretant_vectors(interps) sims = [] for i in range(len(vecs)): for j in range(i+1, len(vecs)): sims.append(_cosine(vecs[i], vecs[j])) convergence = max(0.0, sum(sims) / max(1, len(sims))) coherence = global_coherence(text) entropy = interpretive_entropy(interps) # Aggregate: geometric-like combination with entropy penalty base = (structural_precision + convergence + coherence) / 3.0 d_intr = max(0.0, min(1.0, base * (1.0 - 0.5 * entropy))) return { \"structural_precision\": structural_precision, \"convergence\": convergence, \"coherence\": coherence, \"entropy\": entropy, \"D_intr\": d_intr, } extract_triads(text) \u00b6 Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' Source code in semiconj/semiotic/parser.py def extract_triads(text: str) -> List[Triad]: \"\"\"Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' \"\"\" sents = split_sentences(text) triads: List[Triad] = [] for s in sents: w = WORD_RE.findall(s) if not w: continue # sign sign_candidates = [t for t in w if (t[0:1].isupper() and len(t) > 1)] sign = sign_candidates[0] if sign_candidates else (w[0] if w else \"\") # object obj_candidates = [t for t in w[1:] if (t[0:1].isupper() and len(t) > 1)] obj = obj_candidates[0] if obj_candidates else (w[1] if len(w) > 1 else sign) # interpretant if re.search(r\"\\b(is|means|represents|defines)\\b\", s, flags=re.IGNORECASE): interpretant = s.strip() else: interpretant = s.strip() if sign and obj and interpretant: triads.append(Triad(sign=str(sign), obj=str(obj), interpretant=interpretant)) return triads split_sentences(text) \u00b6 Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. Source code in semiconj/semiotic/parser.py def split_sentences(text: str) -> List[str]: \"\"\"Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) sents = res.get(\"sentences\", []) if isinstance(sents, list) and sents: return [str(s).strip() for s in sents if str(s).strip()] except Exception: logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to regex-based sentence splitter.', exc_info=True) pass except Exception: pass parts = SENT_RE.split(text.strip()) return [p for p in parts if p] words(s) \u00b6 Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. Source code in semiconj/semiotic/parser.py def words(s: str) -> List[str]: \"\"\"Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. \"\"\" try: from ..metrics.complexity import tokenize # type: ignore return tokenize(s) except Exception: return WORD_RE.findall(s.lower()) Submodules \u00b6 parser \u00b6 compute_d_intr(text) \u00b6 Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. Source code in semiconj/semiotic/parser.py def compute_d_intr(text: str) -> Dict[str, float]: \"\"\"Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. \"\"\" triads = extract_triads(text) sents = split_sentences(text) structural_precision = 0.0 if sents: structural_precision = sum(1 for s in sents if s.strip()) structural_precision = min(1.0, len(triads) / max(1, len(sents))) interps = [t.interpretant for t in triads] # convergence: mean pairwise similarity among interpretants if len(interps) < 2: convergence = 0.0 else: vecs = interpretant_vectors(interps) sims = [] for i in range(len(vecs)): for j in range(i+1, len(vecs)): sims.append(_cosine(vecs[i], vecs[j])) convergence = max(0.0, sum(sims) / max(1, len(sims))) coherence = global_coherence(text) entropy = interpretive_entropy(interps) # Aggregate: geometric-like combination with entropy penalty base = (structural_precision + convergence + coherence) / 3.0 d_intr = max(0.0, min(1.0, base * (1.0 - 0.5 * entropy))) return { \"structural_precision\": structural_precision, \"convergence\": convergence, \"coherence\": coherence, \"entropy\": entropy, \"D_intr\": d_intr, } extract_triads(text) \u00b6 Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' Source code in semiconj/semiotic/parser.py def extract_triads(text: str) -> List[Triad]: \"\"\"Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' \"\"\" sents = split_sentences(text) triads: List[Triad] = [] for s in sents: w = WORD_RE.findall(s) if not w: continue # sign sign_candidates = [t for t in w if (t[0:1].isupper() and len(t) > 1)] sign = sign_candidates[0] if sign_candidates else (w[0] if w else \"\") # object obj_candidates = [t for t in w[1:] if (t[0:1].isupper() and len(t) > 1)] obj = obj_candidates[0] if obj_candidates else (w[1] if len(w) > 1 else sign) # interpretant if re.search(r\"\\b(is|means|represents|defines)\\b\", s, flags=re.IGNORECASE): interpretant = s.strip() else: interpretant = s.strip() if sign and obj and interpretant: triads.append(Triad(sign=str(sign), obj=str(obj), interpretant=interpretant)) return triads split_sentences(text) \u00b6 Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. Source code in semiconj/semiotic/parser.py def split_sentences(text: str) -> List[str]: \"\"\"Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) sents = res.get(\"sentences\", []) if isinstance(sents, list) and sents: return [str(s).strip() for s in sents if str(s).strip()] except Exception: logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to regex-based sentence splitter.', exc_info=True) pass except Exception: pass parts = SENT_RE.split(text.strip()) return [p for p in parts if p] words(s) \u00b6 Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. Source code in semiconj/semiotic/parser.py def words(s: str) -> List[str]: \"\"\"Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. \"\"\" try: from ..metrics.complexity import tokenize # type: ignore return tokenize(s) except Exception: return WORD_RE.findall(s.lower())","title":"Semiotic (pkg)"},{"location":"api/semiotic/#semiotic-package","text":"","title":"Semiotic Package"},{"location":"api/semiotic/#semiconj.semiotic.parser","text":"","title":"parser"},{"location":"api/semiotic/#semiconj.semiotic.parser.compute_d_intr","text":"Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. Source code in semiconj/semiotic/parser.py def compute_d_intr(text: str) -> Dict[str, float]: \"\"\"Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. \"\"\" triads = extract_triads(text) sents = split_sentences(text) structural_precision = 0.0 if sents: structural_precision = sum(1 for s in sents if s.strip()) structural_precision = min(1.0, len(triads) / max(1, len(sents))) interps = [t.interpretant for t in triads] # convergence: mean pairwise similarity among interpretants if len(interps) < 2: convergence = 0.0 else: vecs = interpretant_vectors(interps) sims = [] for i in range(len(vecs)): for j in range(i+1, len(vecs)): sims.append(_cosine(vecs[i], vecs[j])) convergence = max(0.0, sum(sims) / max(1, len(sims))) coherence = global_coherence(text) entropy = interpretive_entropy(interps) # Aggregate: geometric-like combination with entropy penalty base = (structural_precision + convergence + coherence) / 3.0 d_intr = max(0.0, min(1.0, base * (1.0 - 0.5 * entropy))) return { \"structural_precision\": structural_precision, \"convergence\": convergence, \"coherence\": coherence, \"entropy\": entropy, \"D_intr\": d_intr, }","title":"compute_d_intr"},{"location":"api/semiotic/#semiconj.semiotic.parser.extract_triads","text":"Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' Source code in semiconj/semiotic/parser.py def extract_triads(text: str) -> List[Triad]: \"\"\"Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' \"\"\" sents = split_sentences(text) triads: List[Triad] = [] for s in sents: w = WORD_RE.findall(s) if not w: continue # sign sign_candidates = [t for t in w if (t[0:1].isupper() and len(t) > 1)] sign = sign_candidates[0] if sign_candidates else (w[0] if w else \"\") # object obj_candidates = [t for t in w[1:] if (t[0:1].isupper() and len(t) > 1)] obj = obj_candidates[0] if obj_candidates else (w[1] if len(w) > 1 else sign) # interpretant if re.search(r\"\\b(is|means|represents|defines)\\b\", s, flags=re.IGNORECASE): interpretant = s.strip() else: interpretant = s.strip() if sign and obj and interpretant: triads.append(Triad(sign=str(sign), obj=str(obj), interpretant=interpretant)) return triads","title":"extract_triads"},{"location":"api/semiotic/#semiconj.semiotic.parser.split_sentences","text":"Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. Source code in semiconj/semiotic/parser.py def split_sentences(text: str) -> List[str]: \"\"\"Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) sents = res.get(\"sentences\", []) if isinstance(sents, list) and sents: return [str(s).strip() for s in sents if str(s).strip()] except Exception: logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to regex-based sentence splitter.', exc_info=True) pass except Exception: pass parts = SENT_RE.split(text.strip()) return [p for p in parts if p]","title":"split_sentences"},{"location":"api/semiotic/#semiconj.semiotic.parser.words","text":"Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. Source code in semiconj/semiotic/parser.py def words(s: str) -> List[str]: \"\"\"Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. \"\"\" try: from ..metrics.complexity import tokenize # type: ignore return tokenize(s) except Exception: return WORD_RE.findall(s.lower())","title":"words"},{"location":"api/semiotic/#submodules","text":"","title":"Submodules"},{"location":"api/semiotic/#parser","text":"","title":"parser"},{"location":"api/semiotic/#semiconj.semiotic.parser.compute_d_intr","text":"Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. Source code in semiconj/semiotic/parser.py def compute_d_intr(text: str) -> Dict[str, float]: \"\"\"Compute intrinsic decodability components and aggregate. Returns dict with: structural_precision, convergence, coherence, entropy, D_intr. \"\"\" triads = extract_triads(text) sents = split_sentences(text) structural_precision = 0.0 if sents: structural_precision = sum(1 for s in sents if s.strip()) structural_precision = min(1.0, len(triads) / max(1, len(sents))) interps = [t.interpretant for t in triads] # convergence: mean pairwise similarity among interpretants if len(interps) < 2: convergence = 0.0 else: vecs = interpretant_vectors(interps) sims = [] for i in range(len(vecs)): for j in range(i+1, len(vecs)): sims.append(_cosine(vecs[i], vecs[j])) convergence = max(0.0, sum(sims) / max(1, len(sims))) coherence = global_coherence(text) entropy = interpretive_entropy(interps) # Aggregate: geometric-like combination with entropy penalty base = (structural_precision + convergence + coherence) / 3.0 d_intr = max(0.0, min(1.0, base * (1.0 - 0.5 * entropy))) return { \"structural_precision\": structural_precision, \"convergence\": convergence, \"coherence\": coherence, \"entropy\": entropy, \"D_intr\": d_intr, }","title":"compute_d_intr"},{"location":"api/semiotic/#semiconj.semiotic.parser.extract_triads","text":"Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' Source code in semiconj/semiotic/parser.py def extract_triads(text: str) -> List[Triad]: \"\"\"Rule-based triad extraction heuristics. - sign: prominent noun-like token (capitalized word or noun-ish pattern) - object: next capitalized token or repeated head noun - interpretant: definitional or summary sentence containing 'is/means/represents/defines' \"\"\" sents = split_sentences(text) triads: List[Triad] = [] for s in sents: w = WORD_RE.findall(s) if not w: continue # sign sign_candidates = [t for t in w if (t[0:1].isupper() and len(t) > 1)] sign = sign_candidates[0] if sign_candidates else (w[0] if w else \"\") # object obj_candidates = [t for t in w[1:] if (t[0:1].isupper() and len(t) > 1)] obj = obj_candidates[0] if obj_candidates else (w[1] if len(w) > 1 else sign) # interpretant if re.search(r\"\\b(is|means|represents|defines)\\b\", s, flags=re.IGNORECASE): interpretant = s.strip() else: interpretant = s.strip() if sign and obj and interpretant: triads.append(Triad(sign=str(sign), obj=str(obj), interpretant=interpretant)) return triads","title":"extract_triads"},{"location":"api/semiotic/#semiconj.semiotic.parser.split_sentences","text":"Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. Source code in semiconj/semiotic/parser.py def split_sentences(text: str) -> List[str]: \"\"\"Split text into sentences. Behavior: - If an Ollama NLP model is configured, use it to obtain sentence splits. - Otherwise, fall back to a simple regex-based splitter. \"\"\" try: from ..config import get_runtime_config cfg = get_runtime_config() model = getattr(cfg, \"nlp_ollama_model\", \"\").strip() if model: try: from ..surrogates.ollama_client import get_shared_client # type: ignore client = get_shared_client(host=getattr(cfg, \"nlp_ollama_host\", \"http://localhost:11434\")) res = client.nlp(model=model, text=text) sents = res.get(\"sentences\", []) if isinstance(sents, list) and sents: return [str(s).strip() for s in sents if str(s).strip()] except Exception: logging.getLogger(__name__).warning('Failed to load Ollama NLP model. Falling back to regex-based sentence splitter.', exc_info=True) pass except Exception: pass parts = SENT_RE.split(text.strip()) return [p for p in parts if p]","title":"split_sentences"},{"location":"api/semiotic/#semiconj.semiotic.parser.words","text":"Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. Source code in semiconj/semiotic/parser.py def words(s: str) -> List[str]: \"\"\"Tokenize a sentence string into words, delegating to central tokenizer. Uses metrics.complexity.tokenize to honor Ollama-based tokenization when enabled. \"\"\" try: from ..metrics.complexity import tokenize # type: ignore return tokenize(s) except Exception: return WORD_RE.findall(s.lower())","title":"words"},{"location":"api/surrogates/","text":"Surrogates Package \u00b6 community \u00b6 fleiss_kappa(assignments) \u00b6 Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. Source code in semiconj/surrogates/community.py def fleiss_kappa(assignments: List[List[str]]) -> float: \"\"\"Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. \"\"\" if not assignments: return 0.0 cats = sorted(set(c for row in assignments for c in row)) cat_idx = {c: i for i, c in enumerate(cats)} n_cat = len(cats) n = len(assignments) m = len(assignments[0]) if assignments[0] else 0 if m == 0: return 0.0 # count matrix n x k nij = [[0] * n_cat for _ in range(n)] for i, row in enumerate(assignments): for lab in row: nij[i][cat_idx[lab]] += 1 pj = [sum(nij[i][j] for i in range(n)) / (n * m) for j in range(n_cat)] Pi = [ (sum(nij[i][j] * nij[i][j] for j in range(n_cat)) - m) / (m * (m - 1) + 1e-9) for i in range(n) ] Pbar = sum(Pi) / n Pe = sum(p * p for p in pj) if Pe >= 1.0: return 0.0 return (Pbar - Pe) / (1.0 - Pe + 1e-9) label_text(text) \u00b6 Heuristic domain labeler using keyword matching. Parameters: text ( str ) \u2013 Input text to classify. Returns: str \u2013 One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' Source code in semiconj/surrogates/community.py def label_text(text: str) -> str: \"\"\"Heuristic domain labeler using keyword matching. Args: text: Input text to classify. Returns: One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' \"\"\" t = text.lower() # lightweight keyword-based labeler keywords = { \"technology\": [\"algorithm\", \"software\", \"network\", \"data\", \"model\", \"system\"], \"biology\": [\"cell\", \"species\", \"genome\", \"protein\", \"organism\"], \"art\": [\"painting\", \"poetry\", \"novel\", \"canvas\", \"aesthetic\"], \"politics\": [\"election\", \"policy\", \"state\", \"government\", \"vote\"], \"finance\": [\"market\", \"risk\", \"equity\", \"capital\", \"investment\"], \"sports\": [\"match\", \"score\", \"team\", \"league\", \"coach\"], } for lab, ks in keywords.items(): if any(k in t for k in ks): return lab return \"general\" omega_and_rho(texts, contexts, k=12, strategy='ollama', models=None, ollama_host='http://localhost:11434') \u00b6 Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). Source code in semiconj/surrogates/community.py def omega_and_rho(texts: List[str], contexts: List[str], k: int = 12, strategy: str = \"ollama\", models: List[str] = None, ollama_host: str = \"http://localhost:11434\") -> Tuple[Dict[str, float], Dict[str, float]]: \"\"\"Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). \"\"\" if strategy == \"ollama\": if not models: raise ValueError(\"In 'ollama' mode, a non-empty list of models must be provided.\") comm = build_ollama_community(models=models, k=k, host=ollama_host) else: comm = build_community(k) # Collect labels and summary vectors per_context_labels: Dict[str, List[List[str]]] = {c: [] for c in contexts} per_context_vecs: Dict[str, List[List[List[float]]]] = {c: [] for c in contexts} for c in _tqdm(contexts, desc=\"\u03a9/\u03c1 contexts\", total=len(contexts)): for t in _tqdm(texts, desc=f\"{c} items\", total=len(texts), leave=False): labs = [] vecs = [] for h in comm: lab, summ = h.produce(t, c) labs.append(lab) vecs.append(vec(summ)) per_context_labels[c].append(labs) per_context_vecs[c].append(vecs) # \u03a9(H): agreement + convergence mapped to [0,1] omega: Dict[str, float] = {} for c in contexts: kappa = fleiss_kappa(per_context_labels[c]) # summary convergence: mean pairwise cosine among raters, averaged over items sims = [] for item_vecs in per_context_vecs[c]: m = len(item_vecs) if m < 2: continue item_sims = [] for i in range(m): for j in range(i+1, m): item_sims.append(cosine(item_vecs[i], item_vecs[j])) if item_sims: sims.append(sum(item_sims) / len(item_sims)) conv = sum(sims) / len(sims) if sims else 0.0 # map to [0,1] omega[c] = max(0.0, min(1.0, 0.5 * (kappa + conv))) # \u03c1(C): 1 - var_H(C) / var_H(C_open) # Measure variance of summary vectors across H, averaged across items def variance_of_vectors(vecs: List[List[float]]) -> float: if not vecs: return 0.0 d = len(vecs[0]) # component-wise variance average means = [sum(v[i] for v in vecs) / len(vecs) for i in range(d)] var = sum(sum((v[i] - means[i]) ** 2 for v in vecs) / len(vecs) for i in range(d)) / d return var var_by_context = {} for c in contexts: item_vars = [variance_of_vectors(vv) for vv in per_context_vecs[c]] var_by_context[c] = sum(item_vars) / len(item_vars) if item_vars else 0.0 if \"C_open\" not in var_by_context: raise ValueError(\"Context 'C_open' is required to compute rho baseline.\") base = var_by_context[\"C_open\"] if base <= 0.0: # Degenerate case: no variance across H in the open context; define rho as 0 for all contexts rho = {c: 0.0 for c in contexts} return omega, rho rho: Dict[str, float] = {} for c in contexts: rho[c] = max(0.0, min(1.0, 1.0 - (var_by_context[c] / base))) return omega, rho summarize(text, n_sentences=1, seed=0) \u00b6 Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by seed . Parameters: text ( str ) \u2013 Input text to summarize. n_sentences ( int , default: 1 ) \u2013 Number of sentences to include in the summary (>=1). seed ( int , default: 0 ) \u2013 Random seed for tie-breaking. Returns: str \u2013 A string containing the selected sentence(s), joined by '. '. If the text str \u2013 has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' Source code in semiconj/surrogates/community.py def summarize(text: str, n_sentences: int = 1, seed: int = 0) -> str: \"\"\"Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by ``seed``. Args: text: Input text to summarize. n_sentences: Number of sentences to include in the summary (>=1). seed: Random seed for tie-breaking. Returns: A string containing the selected sentence(s), joined by '. '. If the text has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' \"\"\" # Simple extractive summary: pick highest-average TF word sentences, randomized tie-breakers sents = [s for s in text.split('.') if s.strip()] if not sents: return text.strip() toks = [tokenize(s) for s in sents] tf = Counter([w for ts in toks for w in ts]) scores = [] rnd = random.Random(seed) for i, ts in enumerate(toks): score = sum(tf[w] for w in set(ts)) / max(1, len(set(ts))) score += 1e-3 * rnd.random() scores.append((score, i)) scores.sort(reverse=True) idxs = sorted(i for _, i in scores[:n_sentences]) return '. '.join(sents[i].strip() for i in idxs) ollama_client \u00b6 OllamaClient dataclass \u00b6 Source code in semiconj/surrogates/ollama_client.py @dataclass class OllamaClient: host: str = \"http://localhost:11434\" # Internal reusable clients/sessions for efficiency _py_client: Optional[object] = None # ollama.Client when available _session: Optional[object] = None # requests.Session when requests available def _get_py_client(self): if ollama is None: return None if self._py_client is None: try: self._py_client = ollama.Client(host=self.host) except Exception: self._py_client = None return self._py_client def _get_session(self): if requests is None: return None if self._session is None: try: import requests as _rq # type: ignore self._session = _rq.Session() except Exception: self._session = None return self._session def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities} # ---------------------- Prompting and parsing helpers ---------------------- def parse_label_summary(self, raw: str, allowed_labels: List[str]) -> Tuple[str, str]: txt = raw.strip() start = txt.find('{') end = txt.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama response is not strict JSON with 'label' and 'summary' fields.\") try: obj = json.loads(txt[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama response.\") from e label = str(obj.get(\"label\", \"\")).strip() summary = str(obj.get(\"summary\", \"\")).strip() if not label or not summary: raise ValueError(\"Ollama JSON must contain non-empty 'label' and 'summary'.\") if label not in allowed_labels: raise ValueError(f\"Label '{label}' not in allowed set: {allowed_labels}\") return label, summary embed(model, text) \u00b6 Return a single embedding vector for the text via ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) generate(model, prompt, system=None, temperature=0.7, seed=None) \u00b6 Generate non-streamed output using the Ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) nlp(model, text) \u00b6 Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). Source code in semiconj/surrogates/ollama_client.py def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities} Submodules \u00b6 community \u00b6 fleiss_kappa(assignments) \u00b6 Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. Source code in semiconj/surrogates/community.py def fleiss_kappa(assignments: List[List[str]]) -> float: \"\"\"Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. \"\"\" if not assignments: return 0.0 cats = sorted(set(c for row in assignments for c in row)) cat_idx = {c: i for i, c in enumerate(cats)} n_cat = len(cats) n = len(assignments) m = len(assignments[0]) if assignments[0] else 0 if m == 0: return 0.0 # count matrix n x k nij = [[0] * n_cat for _ in range(n)] for i, row in enumerate(assignments): for lab in row: nij[i][cat_idx[lab]] += 1 pj = [sum(nij[i][j] for i in range(n)) / (n * m) for j in range(n_cat)] Pi = [ (sum(nij[i][j] * nij[i][j] for j in range(n_cat)) - m) / (m * (m - 1) + 1e-9) for i in range(n) ] Pbar = sum(Pi) / n Pe = sum(p * p for p in pj) if Pe >= 1.0: return 0.0 return (Pbar - Pe) / (1.0 - Pe + 1e-9) label_text(text) \u00b6 Heuristic domain labeler using keyword matching. Parameters: text ( str ) \u2013 Input text to classify. Returns: str \u2013 One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' Source code in semiconj/surrogates/community.py def label_text(text: str) -> str: \"\"\"Heuristic domain labeler using keyword matching. Args: text: Input text to classify. Returns: One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' \"\"\" t = text.lower() # lightweight keyword-based labeler keywords = { \"technology\": [\"algorithm\", \"software\", \"network\", \"data\", \"model\", \"system\"], \"biology\": [\"cell\", \"species\", \"genome\", \"protein\", \"organism\"], \"art\": [\"painting\", \"poetry\", \"novel\", \"canvas\", \"aesthetic\"], \"politics\": [\"election\", \"policy\", \"state\", \"government\", \"vote\"], \"finance\": [\"market\", \"risk\", \"equity\", \"capital\", \"investment\"], \"sports\": [\"match\", \"score\", \"team\", \"league\", \"coach\"], } for lab, ks in keywords.items(): if any(k in t for k in ks): return lab return \"general\" omega_and_rho(texts, contexts, k=12, strategy='ollama', models=None, ollama_host='http://localhost:11434') \u00b6 Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). Source code in semiconj/surrogates/community.py def omega_and_rho(texts: List[str], contexts: List[str], k: int = 12, strategy: str = \"ollama\", models: List[str] = None, ollama_host: str = \"http://localhost:11434\") -> Tuple[Dict[str, float], Dict[str, float]]: \"\"\"Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). \"\"\" if strategy == \"ollama\": if not models: raise ValueError(\"In 'ollama' mode, a non-empty list of models must be provided.\") comm = build_ollama_community(models=models, k=k, host=ollama_host) else: comm = build_community(k) # Collect labels and summary vectors per_context_labels: Dict[str, List[List[str]]] = {c: [] for c in contexts} per_context_vecs: Dict[str, List[List[List[float]]]] = {c: [] for c in contexts} for c in _tqdm(contexts, desc=\"\u03a9/\u03c1 contexts\", total=len(contexts)): for t in _tqdm(texts, desc=f\"{c} items\", total=len(texts), leave=False): labs = [] vecs = [] for h in comm: lab, summ = h.produce(t, c) labs.append(lab) vecs.append(vec(summ)) per_context_labels[c].append(labs) per_context_vecs[c].append(vecs) # \u03a9(H): agreement + convergence mapped to [0,1] omega: Dict[str, float] = {} for c in contexts: kappa = fleiss_kappa(per_context_labels[c]) # summary convergence: mean pairwise cosine among raters, averaged over items sims = [] for item_vecs in per_context_vecs[c]: m = len(item_vecs) if m < 2: continue item_sims = [] for i in range(m): for j in range(i+1, m): item_sims.append(cosine(item_vecs[i], item_vecs[j])) if item_sims: sims.append(sum(item_sims) / len(item_sims)) conv = sum(sims) / len(sims) if sims else 0.0 # map to [0,1] omega[c] = max(0.0, min(1.0, 0.5 * (kappa + conv))) # \u03c1(C): 1 - var_H(C) / var_H(C_open) # Measure variance of summary vectors across H, averaged across items def variance_of_vectors(vecs: List[List[float]]) -> float: if not vecs: return 0.0 d = len(vecs[0]) # component-wise variance average means = [sum(v[i] for v in vecs) / len(vecs) for i in range(d)] var = sum(sum((v[i] - means[i]) ** 2 for v in vecs) / len(vecs) for i in range(d)) / d return var var_by_context = {} for c in contexts: item_vars = [variance_of_vectors(vv) for vv in per_context_vecs[c]] var_by_context[c] = sum(item_vars) / len(item_vars) if item_vars else 0.0 if \"C_open\" not in var_by_context: raise ValueError(\"Context 'C_open' is required to compute rho baseline.\") base = var_by_context[\"C_open\"] if base <= 0.0: # Degenerate case: no variance across H in the open context; define rho as 0 for all contexts rho = {c: 0.0 for c in contexts} return omega, rho rho: Dict[str, float] = {} for c in contexts: rho[c] = max(0.0, min(1.0, 1.0 - (var_by_context[c] / base))) return omega, rho summarize(text, n_sentences=1, seed=0) \u00b6 Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by seed . Parameters: text ( str ) \u2013 Input text to summarize. n_sentences ( int , default: 1 ) \u2013 Number of sentences to include in the summary (>=1). seed ( int , default: 0 ) \u2013 Random seed for tie-breaking. Returns: str \u2013 A string containing the selected sentence(s), joined by '. '. If the text str \u2013 has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' Source code in semiconj/surrogates/community.py def summarize(text: str, n_sentences: int = 1, seed: int = 0) -> str: \"\"\"Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by ``seed``. Args: text: Input text to summarize. n_sentences: Number of sentences to include in the summary (>=1). seed: Random seed for tie-breaking. Returns: A string containing the selected sentence(s), joined by '. '. If the text has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' \"\"\" # Simple extractive summary: pick highest-average TF word sentences, randomized tie-breakers sents = [s for s in text.split('.') if s.strip()] if not sents: return text.strip() toks = [tokenize(s) for s in sents] tf = Counter([w for ts in toks for w in ts]) scores = [] rnd = random.Random(seed) for i, ts in enumerate(toks): score = sum(tf[w] for w in set(ts)) / max(1, len(set(ts))) score += 1e-3 * rnd.random() scores.append((score, i)) scores.sort(reverse=True) idxs = sorted(i for _, i in scores[:n_sentences]) return '. '.join(sents[i].strip() for i in idxs) ollama_client \u00b6 OllamaClient dataclass \u00b6 Source code in semiconj/surrogates/ollama_client.py @dataclass class OllamaClient: host: str = \"http://localhost:11434\" # Internal reusable clients/sessions for efficiency _py_client: Optional[object] = None # ollama.Client when available _session: Optional[object] = None # requests.Session when requests available def _get_py_client(self): if ollama is None: return None if self._py_client is None: try: self._py_client = ollama.Client(host=self.host) except Exception: self._py_client = None return self._py_client def _get_session(self): if requests is None: return None if self._session is None: try: import requests as _rq # type: ignore self._session = _rq.Session() except Exception: self._session = None return self._session def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities} # ---------------------- Prompting and parsing helpers ---------------------- def parse_label_summary(self, raw: str, allowed_labels: List[str]) -> Tuple[str, str]: txt = raw.strip() start = txt.find('{') end = txt.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama response is not strict JSON with 'label' and 'summary' fields.\") try: obj = json.loads(txt[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama response.\") from e label = str(obj.get(\"label\", \"\")).strip() summary = str(obj.get(\"summary\", \"\")).strip() if not label or not summary: raise ValueError(\"Ollama JSON must contain non-empty 'label' and 'summary'.\") if label not in allowed_labels: raise ValueError(f\"Label '{label}' not in allowed set: {allowed_labels}\") return label, summary embed(model, text) \u00b6 Return a single embedding vector for the text via ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) generate(model, prompt, system=None, temperature=0.7, seed=None) \u00b6 Generate non-streamed output using the Ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) nlp(model, text) \u00b6 Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). Source code in semiconj/surrogates/ollama_client.py def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities}","title":"Surrogates (pkg)"},{"location":"api/surrogates/#surrogates-package","text":"","title":"Surrogates Package"},{"location":"api/surrogates/#semiconj.surrogates.community","text":"","title":"community"},{"location":"api/surrogates/#semiconj.surrogates.community.fleiss_kappa","text":"Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. Source code in semiconj/surrogates/community.py def fleiss_kappa(assignments: List[List[str]]) -> float: \"\"\"Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. \"\"\" if not assignments: return 0.0 cats = sorted(set(c for row in assignments for c in row)) cat_idx = {c: i for i, c in enumerate(cats)} n_cat = len(cats) n = len(assignments) m = len(assignments[0]) if assignments[0] else 0 if m == 0: return 0.0 # count matrix n x k nij = [[0] * n_cat for _ in range(n)] for i, row in enumerate(assignments): for lab in row: nij[i][cat_idx[lab]] += 1 pj = [sum(nij[i][j] for i in range(n)) / (n * m) for j in range(n_cat)] Pi = [ (sum(nij[i][j] * nij[i][j] for j in range(n_cat)) - m) / (m * (m - 1) + 1e-9) for i in range(n) ] Pbar = sum(Pi) / n Pe = sum(p * p for p in pj) if Pe >= 1.0: return 0.0 return (Pbar - Pe) / (1.0 - Pe + 1e-9)","title":"fleiss_kappa"},{"location":"api/surrogates/#semiconj.surrogates.community.label_text","text":"Heuristic domain labeler using keyword matching. Parameters: text ( str ) \u2013 Input text to classify. Returns: str \u2013 One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' Source code in semiconj/surrogates/community.py def label_text(text: str) -> str: \"\"\"Heuristic domain labeler using keyword matching. Args: text: Input text to classify. Returns: One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' \"\"\" t = text.lower() # lightweight keyword-based labeler keywords = { \"technology\": [\"algorithm\", \"software\", \"network\", \"data\", \"model\", \"system\"], \"biology\": [\"cell\", \"species\", \"genome\", \"protein\", \"organism\"], \"art\": [\"painting\", \"poetry\", \"novel\", \"canvas\", \"aesthetic\"], \"politics\": [\"election\", \"policy\", \"state\", \"government\", \"vote\"], \"finance\": [\"market\", \"risk\", \"equity\", \"capital\", \"investment\"], \"sports\": [\"match\", \"score\", \"team\", \"league\", \"coach\"], } for lab, ks in keywords.items(): if any(k in t for k in ks): return lab return \"general\"","title":"label_text"},{"location":"api/surrogates/#semiconj.surrogates.community.omega_and_rho","text":"Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). Source code in semiconj/surrogates/community.py def omega_and_rho(texts: List[str], contexts: List[str], k: int = 12, strategy: str = \"ollama\", models: List[str] = None, ollama_host: str = \"http://localhost:11434\") -> Tuple[Dict[str, float], Dict[str, float]]: \"\"\"Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). \"\"\" if strategy == \"ollama\": if not models: raise ValueError(\"In 'ollama' mode, a non-empty list of models must be provided.\") comm = build_ollama_community(models=models, k=k, host=ollama_host) else: comm = build_community(k) # Collect labels and summary vectors per_context_labels: Dict[str, List[List[str]]] = {c: [] for c in contexts} per_context_vecs: Dict[str, List[List[List[float]]]] = {c: [] for c in contexts} for c in _tqdm(contexts, desc=\"\u03a9/\u03c1 contexts\", total=len(contexts)): for t in _tqdm(texts, desc=f\"{c} items\", total=len(texts), leave=False): labs = [] vecs = [] for h in comm: lab, summ = h.produce(t, c) labs.append(lab) vecs.append(vec(summ)) per_context_labels[c].append(labs) per_context_vecs[c].append(vecs) # \u03a9(H): agreement + convergence mapped to [0,1] omega: Dict[str, float] = {} for c in contexts: kappa = fleiss_kappa(per_context_labels[c]) # summary convergence: mean pairwise cosine among raters, averaged over items sims = [] for item_vecs in per_context_vecs[c]: m = len(item_vecs) if m < 2: continue item_sims = [] for i in range(m): for j in range(i+1, m): item_sims.append(cosine(item_vecs[i], item_vecs[j])) if item_sims: sims.append(sum(item_sims) / len(item_sims)) conv = sum(sims) / len(sims) if sims else 0.0 # map to [0,1] omega[c] = max(0.0, min(1.0, 0.5 * (kappa + conv))) # \u03c1(C): 1 - var_H(C) / var_H(C_open) # Measure variance of summary vectors across H, averaged across items def variance_of_vectors(vecs: List[List[float]]) -> float: if not vecs: return 0.0 d = len(vecs[0]) # component-wise variance average means = [sum(v[i] for v in vecs) / len(vecs) for i in range(d)] var = sum(sum((v[i] - means[i]) ** 2 for v in vecs) / len(vecs) for i in range(d)) / d return var var_by_context = {} for c in contexts: item_vars = [variance_of_vectors(vv) for vv in per_context_vecs[c]] var_by_context[c] = sum(item_vars) / len(item_vars) if item_vars else 0.0 if \"C_open\" not in var_by_context: raise ValueError(\"Context 'C_open' is required to compute rho baseline.\") base = var_by_context[\"C_open\"] if base <= 0.0: # Degenerate case: no variance across H in the open context; define rho as 0 for all contexts rho = {c: 0.0 for c in contexts} return omega, rho rho: Dict[str, float] = {} for c in contexts: rho[c] = max(0.0, min(1.0, 1.0 - (var_by_context[c] / base))) return omega, rho","title":"omega_and_rho"},{"location":"api/surrogates/#semiconj.surrogates.community.summarize","text":"Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by seed . Parameters: text ( str ) \u2013 Input text to summarize. n_sentences ( int , default: 1 ) \u2013 Number of sentences to include in the summary (>=1). seed ( int , default: 0 ) \u2013 Random seed for tie-breaking. Returns: str \u2013 A string containing the selected sentence(s), joined by '. '. If the text str \u2013 has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' Source code in semiconj/surrogates/community.py def summarize(text: str, n_sentences: int = 1, seed: int = 0) -> str: \"\"\"Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by ``seed``. Args: text: Input text to summarize. n_sentences: Number of sentences to include in the summary (>=1). seed: Random seed for tie-breaking. Returns: A string containing the selected sentence(s), joined by '. '. If the text has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' \"\"\" # Simple extractive summary: pick highest-average TF word sentences, randomized tie-breakers sents = [s for s in text.split('.') if s.strip()] if not sents: return text.strip() toks = [tokenize(s) for s in sents] tf = Counter([w for ts in toks for w in ts]) scores = [] rnd = random.Random(seed) for i, ts in enumerate(toks): score = sum(tf[w] for w in set(ts)) / max(1, len(set(ts))) score += 1e-3 * rnd.random() scores.append((score, i)) scores.sort(reverse=True) idxs = sorted(i for _, i in scores[:n_sentences]) return '. '.join(sents[i].strip() for i in idxs)","title":"summarize"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client","text":"","title":"ollama_client"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient","text":"Source code in semiconj/surrogates/ollama_client.py @dataclass class OllamaClient: host: str = \"http://localhost:11434\" # Internal reusable clients/sessions for efficiency _py_client: Optional[object] = None # ollama.Client when available _session: Optional[object] = None # requests.Session when requests available def _get_py_client(self): if ollama is None: return None if self._py_client is None: try: self._py_client = ollama.Client(host=self.host) except Exception: self._py_client = None return self._py_client def _get_session(self): if requests is None: return None if self._session is None: try: import requests as _rq # type: ignore self._session = _rq.Session() except Exception: self._session = None return self._session def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities} # ---------------------- Prompting and parsing helpers ---------------------- def parse_label_summary(self, raw: str, allowed_labels: List[str]) -> Tuple[str, str]: txt = raw.strip() start = txt.find('{') end = txt.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama response is not strict JSON with 'label' and 'summary' fields.\") try: obj = json.loads(txt[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama response.\") from e label = str(obj.get(\"label\", \"\")).strip() summary = str(obj.get(\"summary\", \"\")).strip() if not label or not summary: raise ValueError(\"Ollama JSON must contain non-empty 'label' and 'summary'.\") if label not in allowed_labels: raise ValueError(f\"Label '{label}' not in allowed set: {allowed_labels}\") return label, summary","title":"OllamaClient"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient.embed","text":"Return a single embedding vector for the text via ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" )","title":"embed"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient.generate","text":"Generate non-streamed output using the Ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" )","title":"generate"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient.nlp","text":"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). Source code in semiconj/surrogates/ollama_client.py def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities}","title":"nlp"},{"location":"api/surrogates/#submodules","text":"","title":"Submodules"},{"location":"api/surrogates/#community","text":"","title":"community"},{"location":"api/surrogates/#semiconj.surrogates.community.fleiss_kappa","text":"Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. Source code in semiconj/surrogates/community.py def fleiss_kappa(assignments: List[List[str]]) -> float: \"\"\"Compute Fleiss' kappa for categorical labels. assignments: list of items, each is list of labels from raters. \"\"\" if not assignments: return 0.0 cats = sorted(set(c for row in assignments for c in row)) cat_idx = {c: i for i, c in enumerate(cats)} n_cat = len(cats) n = len(assignments) m = len(assignments[0]) if assignments[0] else 0 if m == 0: return 0.0 # count matrix n x k nij = [[0] * n_cat for _ in range(n)] for i, row in enumerate(assignments): for lab in row: nij[i][cat_idx[lab]] += 1 pj = [sum(nij[i][j] for i in range(n)) / (n * m) for j in range(n_cat)] Pi = [ (sum(nij[i][j] * nij[i][j] for j in range(n_cat)) - m) / (m * (m - 1) + 1e-9) for i in range(n) ] Pbar = sum(Pi) / n Pe = sum(p * p for p in pj) if Pe >= 1.0: return 0.0 return (Pbar - Pe) / (1.0 - Pe + 1e-9)","title":"fleiss_kappa"},{"location":"api/surrogates/#semiconj.surrogates.community.label_text","text":"Heuristic domain labeler using keyword matching. Parameters: text ( str ) \u2013 Input text to classify. Returns: str \u2013 One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' Source code in semiconj/surrogates/community.py def label_text(text: str) -> str: \"\"\"Heuristic domain labeler using keyword matching. Args: text: Input text to classify. Returns: One of LABELS: 'technology', 'biology', 'art', 'politics', 'finance', 'sports', or 'general'. Examples: >>> label_text('The algorithm optimizes network throughput.') 'technology' >>> label_text('A canvas and painting techniques are discussed.') 'art' \"\"\" t = text.lower() # lightweight keyword-based labeler keywords = { \"technology\": [\"algorithm\", \"software\", \"network\", \"data\", \"model\", \"system\"], \"biology\": [\"cell\", \"species\", \"genome\", \"protein\", \"organism\"], \"art\": [\"painting\", \"poetry\", \"novel\", \"canvas\", \"aesthetic\"], \"politics\": [\"election\", \"policy\", \"state\", \"government\", \"vote\"], \"finance\": [\"market\", \"risk\", \"equity\", \"capital\", \"investment\"], \"sports\": [\"match\", \"score\", \"team\", \"league\", \"coach\"], } for lab, ks in keywords.items(): if any(k in t for k in ks): return lab return \"general\"","title":"label_text"},{"location":"api/surrogates/#semiconj.surrogates.community.omega_and_rho","text":"Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). Source code in semiconj/surrogates/community.py def omega_and_rho(texts: List[str], contexts: List[str], k: int = 12, strategy: str = \"ollama\", models: List[str] = None, ollama_host: str = \"http://localhost:11434\") -> Tuple[Dict[str, float], Dict[str, float]]: \"\"\"Compute \u03a9(H) and \u03c1(C) across contexts for given texts. Returns (Omega_by_context, Rho_by_context). \"\"\" if strategy == \"ollama\": if not models: raise ValueError(\"In 'ollama' mode, a non-empty list of models must be provided.\") comm = build_ollama_community(models=models, k=k, host=ollama_host) else: comm = build_community(k) # Collect labels and summary vectors per_context_labels: Dict[str, List[List[str]]] = {c: [] for c in contexts} per_context_vecs: Dict[str, List[List[List[float]]]] = {c: [] for c in contexts} for c in _tqdm(contexts, desc=\"\u03a9/\u03c1 contexts\", total=len(contexts)): for t in _tqdm(texts, desc=f\"{c} items\", total=len(texts), leave=False): labs = [] vecs = [] for h in comm: lab, summ = h.produce(t, c) labs.append(lab) vecs.append(vec(summ)) per_context_labels[c].append(labs) per_context_vecs[c].append(vecs) # \u03a9(H): agreement + convergence mapped to [0,1] omega: Dict[str, float] = {} for c in contexts: kappa = fleiss_kappa(per_context_labels[c]) # summary convergence: mean pairwise cosine among raters, averaged over items sims = [] for item_vecs in per_context_vecs[c]: m = len(item_vecs) if m < 2: continue item_sims = [] for i in range(m): for j in range(i+1, m): item_sims.append(cosine(item_vecs[i], item_vecs[j])) if item_sims: sims.append(sum(item_sims) / len(item_sims)) conv = sum(sims) / len(sims) if sims else 0.0 # map to [0,1] omega[c] = max(0.0, min(1.0, 0.5 * (kappa + conv))) # \u03c1(C): 1 - var_H(C) / var_H(C_open) # Measure variance of summary vectors across H, averaged across items def variance_of_vectors(vecs: List[List[float]]) -> float: if not vecs: return 0.0 d = len(vecs[0]) # component-wise variance average means = [sum(v[i] for v in vecs) / len(vecs) for i in range(d)] var = sum(sum((v[i] - means[i]) ** 2 for v in vecs) / len(vecs) for i in range(d)) / d return var var_by_context = {} for c in contexts: item_vars = [variance_of_vectors(vv) for vv in per_context_vecs[c]] var_by_context[c] = sum(item_vars) / len(item_vars) if item_vars else 0.0 if \"C_open\" not in var_by_context: raise ValueError(\"Context 'C_open' is required to compute rho baseline.\") base = var_by_context[\"C_open\"] if base <= 0.0: # Degenerate case: no variance across H in the open context; define rho as 0 for all contexts rho = {c: 0.0 for c in contexts} return omega, rho rho: Dict[str, float] = {} for c in contexts: rho[c] = max(0.0, min(1.0, 1.0 - (var_by_context[c] / base))) return omega, rho","title":"omega_and_rho"},{"location":"api/surrogates/#semiconj.surrogates.community.summarize","text":"Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by seed . Parameters: text ( str ) \u2013 Input text to summarize. n_sentences ( int , default: 1 ) \u2013 Number of sentences to include in the summary (>=1). seed ( int , default: 0 ) \u2013 Random seed for tie-breaking. Returns: str \u2013 A string containing the selected sentence(s), joined by '. '. If the text str \u2013 has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' Source code in semiconj/surrogates/community.py def summarize(text: str, n_sentences: int = 1, seed: int = 0) -> str: \"\"\"Extractive summary by selecting top-scoring sentences. Sentences are scored by the average term frequency of their unique tokens, with tiny random tie-breakers controlled by ``seed``. Args: text: Input text to summarize. n_sentences: Number of sentences to include in the summary (>=1). seed: Random seed for tie-breaking. Returns: A string containing the selected sentence(s), joined by '. '. If the text has no sentence-like delimiters, returns the stripped input. Examples: >>> summarize('A. B. C.', n_sentences=2, seed=0) 'A. B' \"\"\" # Simple extractive summary: pick highest-average TF word sentences, randomized tie-breakers sents = [s for s in text.split('.') if s.strip()] if not sents: return text.strip() toks = [tokenize(s) for s in sents] tf = Counter([w for ts in toks for w in ts]) scores = [] rnd = random.Random(seed) for i, ts in enumerate(toks): score = sum(tf[w] for w in set(ts)) / max(1, len(set(ts))) score += 1e-3 * rnd.random() scores.append((score, i)) scores.sort(reverse=True) idxs = sorted(i for _, i in scores[:n_sentences]) return '. '.join(sents[i].strip() for i in idxs)","title":"summarize"},{"location":"api/surrogates/#ollama_client","text":"","title":"ollama_client"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient","text":"Source code in semiconj/surrogates/ollama_client.py @dataclass class OllamaClient: host: str = \"http://localhost:11434\" # Internal reusable clients/sessions for efficiency _py_client: Optional[object] = None # ollama.Client when available _session: Optional[object] = None # requests.Session when requests available def _get_py_client(self): if ollama is None: return None if self._py_client is None: try: self._py_client = ollama.Client(host=self.host) except Exception: self._py_client = None return self._py_client def _get_session(self): if requests is None: return None if self._session is None: try: import requests as _rq # type: ignore self._session = _rq.Session() except Exception: self._session = None return self._session def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" ) def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities} # ---------------------- Prompting and parsing helpers ---------------------- def parse_label_summary(self, raw: str, allowed_labels: List[str]) -> Tuple[str, str]: txt = raw.strip() start = txt.find('{') end = txt.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama response is not strict JSON with 'label' and 'summary' fields.\") try: obj = json.loads(txt[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama response.\") from e label = str(obj.get(\"label\", \"\")).strip() summary = str(obj.get(\"summary\", \"\")).strip() if not label or not summary: raise ValueError(\"Ollama JSON must contain non-empty 'label' and 'summary'.\") if label not in allowed_labels: raise ValueError(f\"Label '{label}' not in allowed set: {allowed_labels}\") return label, summary","title":"OllamaClient"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient.embed","text":"Return a single embedding vector for the text via ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def embed(self, model: str, text: str) -> List[float]: \"\"\"Return a single embedding vector for the text via ollama Python client if available, else HTTP. \"\"\" last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) embeddings (attempt {attempt}) model={model} len(text)={len(text)}\") data = py_client.embeddings(model=model, input=text) emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema from ollama(py)\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) embeddings failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/embeddings\" payload = {\"model\": model, \"input\": text} session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model} len(text)={len(text)}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() emb = data.get(\"embedding\") if not isinstance(emb, list) or not all(isinstance(x, (int, float)) for x in emb): raise ValueError(\"Unexpected embeddings response schema\") return [float(x) for x in emb] except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama embeddings HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama embeddings call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" )","title":"embed"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient.generate","text":"Generate non-streamed output using the Ollama Python client if available, else HTTP. Source code in semiconj/surrogates/ollama_client.py def generate(self, model: str, prompt: str, system: Optional[str] = None, temperature: float = 0.7, seed: Optional[int] = None) -> str: \"\"\"Generate non-streamed output using the Ollama Python client if available, else HTTP. \"\"\" options = {\"temperature\": float(temperature)} if seed is not None: options[\"seed\"] = int(seed) last_err: Optional[Exception] = None # Preferred path: official ollama Python client py_client = self._get_py_client() if py_client is not None: for attempt in range(1, 4): try: logger.debug(f\"Ollama(py) generate (attempt {attempt}) model={model}\") data = py_client.generate(model=model, prompt=prompt, system=system, options=options) return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama(py) call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) # Fallback: HTTP API via requests payload = { \"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": options, } if system: payload[\"system\"] = system if requests is None: raise RuntimeError( \"Ollama Python client and 'requests' are unavailable. Install 'ollama' or 'requests' to call the API.\" ) url = self.host.rstrip('/') + \"/api/generate\" session = self._get_session() for attempt in range(1, 4): try: logger.debug(f\"Ollama HTTP POST {url} (attempt {attempt}) model={model}\") if session is not None: resp = session.post(url, json=payload, timeout=60) # type: ignore else: resp = requests.post(url, json=payload, timeout=60) # type: ignore resp.raise_for_status() data = resp.json() return str(data.get(\"response\", \"\")).strip() except Exception as e: last_err = e wait = min(5.0, 0.5 * (2 ** (attempt - 1))) logger.warning(f\"Ollama HTTP call failed (attempt {attempt}): {e}. Retrying in {wait:.1f}s...\") time.sleep(wait) raise RuntimeError( \"Ollama call failed after retries. Ensure the server is running and the model is pulled. \" f\"Last error: {last_err}\" )","title":"generate"},{"location":"api/surrogates/#semiconj.surrogates.ollama_client.OllamaClient.nlp","text":"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). Source code in semiconj/surrogates/ollama_client.py def nlp(self, model: str, text: str) -> dict: \"\"\"Perform basic NLP with an Ollama instruct model (e.g., gpt-oss). Returns a dict with keys: tokens (list[str]), sentences (list[str]), entities (list[{text,type}]). Strict JSON parsing with minimal recovery (extract first {...}). \"\"\" system = ( \"You are an NLP annotator. Respond in strict JSON only with keys: \" \"tokens (array of strings), sentences (array of strings), entities (array of objects with 'text' and 'type').\" ) prompt = ( \"Text to annotate:\\n\" + text.strip() + \"\\n\\n\" \"Produce JSON exactly in this schema: {\\\"tokens\\\":[...],\\\"sentences\\\":[...],\\\"entities\\\":[{\\\"text\\\":\\\"...\\\",\\\"type\\\":\\\"...\\\"}]}\" ) raw = self.generate(model=model, prompt=prompt, system=system, temperature=0.0) # Extract JSON object start = raw.find('{') end = raw.rfind('}') if start == -1 or end == -1 or end <= start: raise ValueError(\"Ollama NLP response is not strict JSON.\") try: obj = json.loads(raw[start:end+1]) except Exception as e: raise ValueError(\"Failed to parse JSON from Ollama NLP response.\") from e tokens = obj.get(\"tokens\", []) sentences = obj.get(\"sentences\", []) entities = obj.get(\"entities\", []) # Basic validation if not isinstance(tokens, list) or not all(isinstance(t, str) for t in tokens): tokens = [] if not isinstance(sentences, list) or not all(isinstance(s, str) for s in sentences): sentences = [] if not isinstance(entities, list): entities = [] # Normalize entity items norm_entities = [] for e in entities: try: txt = str(e.get(\"text\", \"\")) typ = str(e.get(\"type\", \"\")) if txt: norm_entities.append({\"text\": txt, \"type\": typ}) except Exception: continue return {\"tokens\": tokens, \"sentences\": sentences, \"entities\": norm_entities}","title":"nlp"}]}